# -*- coding: utf-8 -*-
#===========================================
# ARC Symbolic Ultra
#===========================================
from __future__ import annotations

# --- Standard / typing imports (deduped) ---
import os, re, glob, json, math, time, random, threading, logging, hashlib, csv, traceback, sys
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Tuple, Any, Optional, Sequence, Callable, Iterable
from collections import defaultdict, Counter, deque
from datetime import datetime
from uuid import uuid4
from concurrent.futures import ThreadPoolExecutor, as_completed
import functools
from functools import wraps
import socket, gzip, zipfile, shutil
import time
import uuid


import numpy as np

# --- Safe/late imports (auto self-heal) ---
def safe_import(name: str):
    try:
        return __import__(name)
    except Exception:
        # small delay & retry to survive transient Kaggle import hiccups
        try:
            time.sleep(0.25)
            return __import__(name)
        except Exception as e:
            meta_log("import.recover", name=name, ok=False, error=str(e)) if 'meta_log' in globals() else None
            return None

pd = safe_import("pandas")
PIL_mod = safe_import("PIL")
if PIL_mod:
    try:
        from PIL import Image, ImageDraw
    except Exception:
        Image = None; ImageDraw = None
else:
    Image = None; ImageDraw = None

plt_mod = safe_import("matplotlib")
if plt_mod:
    try:
        import matplotlib.pyplot as plt
    except Exception:
        plt = None
else:
    plt = None

# ----------------------------
# Global exception hook (Kairos feedback)
# ----------------------------
def _global_ex_hook(exctype, value, tb):
    try:
        meta_log("fatal.error", type=str(exctype), msg=str(value))
        k = globals().get("kairos", None)
        if k and hasattr(k, "step"):
            k.step(int(time.time() % 100))
    except Exception:
        pass
    # still print the exception
    sys.__excepthook__(exctype, value, tb)

sys.excepthook = _global_ex_hook

#  minimal creativity feature extractor

def creativity_features(grid: np.ndarray) -> Dict[str, float]:
    """Return tiny, stable features; caller may pass to meta.evaluate_creativity()."""
    try:
        h, w = grid.shape[:2]
        total = max(1, h * w)
        uniq, counts = np.unique(grid, return_counts=True)
        diversity = float(len(uniq)) / min(total, 256)
        dominance = float(np.max(counts)) / total if counts.size else 1.0
        # edge density: simple XOR of neighbors
        edges = 0
        try:
            edges = int(np.sum((grid[:, 1:] != grid[:, :-1])) + np.sum((grid[1:, :] != grid[:-1, :])))
        except Exception:
            edges = 0
        edge_density = float(edges) / max(1, (h * (w - 1) + (h - 1) * w))
        return {
            "diversity": float(min(1.0, diversity)),
            "dominance": float(dominance),
            "edge_density": float(edge_density),
        }
    except Exception:
        return {"diversity": 0.0, "dominance": 1.0, "edge_density": 0.0}

# ---------- Global grid telemetry (canonical + back-compat aliases) ----------
CURRENT_GRID: Optional[np.ndarray] = None
PREV_GRID: Optional[np.ndarray] = None
current_grid = None   # legacy alias
prev_grid = None      # legacy alias

def set_grid_telemetry(prev: Optional[np.ndarray], cur: Optional[np.ndarray]) -> None:
    """Set global grid telemetry in a consistent, sanitized way (uses _sanitize_grid later in file)."""
    global PREV_GRID, CURRENT_GRID, prev_grid, current_grid
    try:
        PREV_GRID = _sanitize_grid(prev) if (prev is not None) else None
    except Exception:
        PREV_GRID = None
    try:
        CURRENT_GRID = _sanitize_grid(cur) if (cur is not None) else None
    except Exception:
        CURRENT_GRID = None
    prev_grid = PREV_GRID
    current_grid = CURRENT_GRID

# Optional vectorized alias used by some helpers
_np = np

def _clamp(x, lo=0.0, hi=1.0):
    """Defensive clamp that also supports list/tuple inputs."""
    if lo > hi:
        lo, hi = hi, lo
    try:
        if isinstance(x, (list, tuple)):
            return type(x)(_clamp(v, lo, hi) for v in x)
    except Exception:
        pass
    try:
        if x is None:
            return lo
        try:
            if math.isnan(x):  # type: ignore[attr-defined]
                return lo
        except Exception:
            pass
    except Exception:
        pass
    # Final scalar clamp with defensive comparisons
    try:
        if x < lo:
            return lo
        if x > hi:
            return hi
        return x
    except Exception:
        return lo

# Public alias some call sites might expect
clamp = _clamp

# Optional but robust: expose in builtins so any module-level global lookup finds it
try:
    import builtins as _builtins
    _builtins._clamp = _clamp
    _builtins.clamp = _clamp
except Exception:
    pass

# ----- SSOT dials (centralized, no-throw) -----
def _ssot_dials() -> Dict[str, float]:
    """
    Single Source of Truth for environment-derived numeric dials.
    NOTE: Keel-related dials are for **Memory-only** consumption per policy.
    Non-Memory modules must not read/act on Keel directly.
    """
    d: Dict[str, float] = {}
    # Kairos dials (if present)
    try:
        k = globals().get("kairos", None)
        if k is not None:
            d["kairos.depth"] = float(getattr(k, "depth", 1.0))
            d["kairos.flux"]  = float(getattr(k, "last_entropy_flux", 0.0))
            # Preserve original behavior: state is a string (downstream code may expect text)
            d["kairos.state"] = str(getattr(k, "symbolic_state", "Ω₀"))  # type: ignore[assignment]
    except Exception:
        pass

    def _envf(key: str, dflt: float) -> float:
        try:
            return float(os.getenv(key, str(dflt)))
        except Exception:
            return dflt

    d["TEL_DOWNSAMPLE_N"]  = _envf("META_TEL_DOWNSAMPLE_N", 1.0)
    d["KAIROS_AMP_BASE"]   = _envf("KAIROS_AMP_BASE", 0.15)
    d["KAIROS_AMP_SCALE"]  = _envf("KAIROS_AMP_SCALE", 0.35)
    d["KAIROS_AMP_MIN"]    = _envf("KAIROS_AMP_MIN", 0.10)
    d["KAIROS_AMP_MAX"]    = _envf("KAIROS_AMP_MAX", 0.50)
    d["KAIROS_OMEGA1_MAX"] = _envf("KAIROS_OMEGA1_MAX", 5.0)
    d["KAIROS_OMEGA2_MAX"] = _envf("KAIROS_OMEGA2_MAX", 20.0)
    d["KAIROS_OMEGA3_MAX"] = _envf("KAIROS_OMEGA3_MAX", 50.0)

    # Memory-only policy: present for Memory adapter reads; others must ignore.
    d["KEEL_THRESHOLD_B"]  = _envf("KEEL_COMPRESS_THRESHOLD", 262144.0)
    return d

# -------------------------------------------
# Env-tunable constants (contract/CSV-backed)
# -------------------------------------------
META_FIREWALL_DH        = float(os.getenv("META_FIREWALL_DH", "0.35"))
META_FIREWALL_EPI       = float(os.getenv("META_FIREWALL_EPI", "0.35"))
META_FIREWALL_BIND      = float(os.getenv("META_FIREWALL_BIND", "0.35"))
META_TEL_DOWNSAMPLE_N   = int(os.getenv("META_TEL_DOWNSAMPLE_N", "1"))   # 1 = log every event
META_EMIT_STRICT        = int(os.getenv("META_EMIT_STRICT", "1"))         # 1 = raise on emit failure

# -------------------------------------------
# SSOT accessor for dials (Meta + Kairos should use this)
# -------------------------------------------
def get_meta_dials() -> Dict[str, Any]:
    return {
        "firewall_dh":     float(os.getenv("META_FIREWALL_DH",  str(META_FIREWALL_DH))),
        "firewall_epi":    float(os.getenv("META_FIREWALL_EPI", str(META_FIREWALL_EPI))),
        "firewall_bind":   float(os.getenv("META_FIREWALL_BIND",str(META_FIREWALL_BIND))),
        "tel_downsample_n":max(1, int(os.getenv("META_TEL_DOWNSAMPLE_N", str(META_TEL_DOWNSAMPLE_N)))),
        "emit_strict":     int(os.getenv("META_EMIT_STRICT",    str(META_EMIT_STRICT))),
    }

# ----------------------------------------------------------
# Global/Monolith shims we may rely on (best-effort)
# ----------------------------------------------------------
# Invariants/physics
compute_invariants = globals().get("compute_invariants", None)
_SCORE_PAIR = globals().get("score_pair", None)

# Telemetry/Explain
_meta_log = globals().get("meta_log", None)
_EXPLAIN = globals().get("EXPLAIN", None)

class EvaluationTelemetry:
    """Minimal unified emitter; downsampling/strictness governed by META_* dials."""
    def __init__(self):
        try:
            self._dials = get_meta_dials()  # type: ignore[name-defined]
        except Exception:
            self._dials = {"tel_downsample_n": 1, "emit_strict": 0}

        self._ctr = 0

    def emit(self, stage: str, **rec):
        try:
            self._ctr += 1
            n = max(1, int(self._dials.get("tel_downsample_n", 1)))
            if (self._ctr % n) != 0:
                return
            rec.setdefault("stage", stage)
            rec.setdefault("ts", time.time())
            try:
                meta_log("telemetry.emit", **rec)  # noqa: F821
            except Exception:
                pass
        except Exception as e:
            if int(self._dials.get("emit_strict", 0)) == 1:
                raise e

# External calibrator (optional)
HYBRID_GLOBAL_CAL = globals().get("HYBRID_GLOBAL_CAL", None)

# Env dials
_DEF = lambda k, v: os.getenv(k, str(v))
META_TEL_DOWNSAMPLE_N = int(_DEF("META_TEL_DOWNSAMPLE_N", 1))
KB_RECALL_CACHE_CAP   = int(_DEF("KB_RECALL_CACHE_CAP", 512))
KB_TELEMETRY_CAP      = int(_DEF("KB_TELEMETRY_CAP", 20000))
KB_CONF_FLOOR         = float(_DEF("KB_CONF_FLOOR", 0.05))
KB_CONF_CEIL          = float(_DEF("KB_CONF_CEIL", 0.999))
KB_FAIL_DECAY         = float(_DEF("KB_FAIL_DECAY", 0.995))
KB_RECENCY_ALPHA      = float(_DEF("KB_RECENCY_ALPHA", 0.80))
KB_RECENCY_TAU        = float(_DEF("KB_RECENCY_TAU", 600.0))
KB_SCHEMA_VERSION     = "kb/2"  # bumped
KB_DIAG_EXPORTS       = int(_DEF("KB_DIAG_EXPORTS", 0))

# ----------------------------------------------------------------
# Shared Config (env overrides)
# ----------------------------------------------------------------
CSV_RULES_INPUT  = os.getenv("CSV_RULES_INPUT", "/kaggle/input/training-rules/training_rules (3).csv")
# FIX: treat this as a real path with default, not an env var name
CSV_RULES_OUTPUT = os.getenv("CSV_RULES_OUTPUT", "exports/rules/human_rules.csv")
_HARD_DENY_KEYS  = ("task_id", "taskId", "task-id", "phase", "phase_tag", "dataset_split", "split")

KEEL_COMPRESS_THRESHOLD = 1_000_000
KEEL_CONTAINER_DEFAULT  = "deployment/keel"

# Dataset roots to globals for loaders (lookup if present, else env/defaults)
DATA_ROOT_2025  = globals().get("data_root_2025")  or os.getenv("ARC2025_ROOT") or globals().get("ARC2025_ROOT_DEFAULT")
DATA_ROOT_ARCDB = globals().get("data_root_arcdb") or os.getenv("ARCDB_ROOT")   or globals().get("ARCDB_ROOT_DEFAULT")
globals()["DATA_ROOT_2025"]  = DATA_ROOT_2025
globals()["DATA_ROOT_ARCDB"] = DATA_ROOT_ARCDB
def _meta(topic: str, **kw):
    try:
        meta_log(topic, **kw)  # noqa: F821
    except Exception:
        pass
_meta("dataset.roots", arc2025=DATA_ROOT_2025, arcdb=DATA_ROOT_ARCDB)

# Hardened fallbacks for older code paths
try:
    _HARD_DENY_KEYS
except NameError:
    _HARD_DENY_KEYS = {
        "api_key","apikey","secret","token","password","auth","credentials",
        "key","private_key","task_id","taskId","task-id","phase","phase_tag",
        "dataset_split","split"
    }

try:
    CSV_RULES_INPUT
except NameError:
    CSV_RULES_INPUT = "/kaggle/input/training-rules/training_rules (3).csv"

try:
    CSV_RULES_OUTPUT
except NameError:
    CSV_RULES_OUTPUT = "exports/rules/human_rules.csv"

DENY = {
    "api_key","apikey","secret","token","password","auth","credentials",
    "key","private_key","task_id","taskId","task-id","phase","phase_tag",
    "dataset_split","split"
}

def get_compute_invariants():    
    return globals().get("compute_invariants", None)


# ===========================================================
# Entry model (guarded; reuses existing helpers)
# ===========================================================
@dataclass
class HoloEntry:
    h_in: str
    h_out: str
    h_pair: str

    inp_shape: Tuple[int, int]
    out_shape: Tuple[int, int]

    confidence: float
    echoes: int
    last_ts: float

    glyph_in: Optional[str] = None
    glyph_out: Optional[str] = None
    epi_in: Optional[float] = None
    epi_out: Optional[float] = None
    binder_in: Optional[float] = None
    binder_out: Optional[float] = None
    entropy_in: Optional[float] = None  # store entropy SLOPE
    entropy_out: Optional[float] = None # store entropy SLOPE

    origin: str = "none"
    retention: float = 0.0
    seen: int = 1
    note: str = ""

    @staticmethod
    def from_pair(
        inp: np.ndarray,
        out: np.ndarray,
        *,
        meta: Optional[Dict[str, Any]] = None,
        base_conf: float = 0.5,
        echoes: int = 1,
        note: str = ""
    ) -> "HoloEntry":
        """
        Build HoloEntry from (inp, out) grid pair using shared helpers:
        _as_u8, grid_signature, _pair_sig, _phys_fields, _safe_shape,
        _load_prov_bonus, _norm_origin, _now
        """
        inp_u8 = _as_u8(inp)
        out_u8 = _as_u8(out)

        sig_in = grid_signature(inp_u8)
        sig_out = grid_signature(out_u8)
        h_in = sig_in["sha1"]
        h_out = sig_out["sha1"]
        h_pair = _pair_sig(inp_u8, out_u8)

        phys_in = _phys_fields(inp_u8)
        phys_out = _phys_fields(out_u8)

        return HoloEntry(
            h_in=h_in,
            h_out=h_out,
            h_pair=h_pair,
            inp_shape=_safe_shape(inp_u8),
            out_shape=_safe_shape(out_u8),
            confidence=float(base_conf + _load_prov_bonus().get(_norm_origin(meta), 0.0)),
            echoes=int(echoes),
            last_ts=_now(),
            glyph_in=phys_in.get("glyph"),
            glyph_out=phys_out.get("glyph"),
            epi_in=phys_in.get("epi"),
            epi_out=phys_out.get("epi"),
            binder_in=phys_in.get("binder"),
            binder_out=phys_out.get("binder"),
            entropy_in=phys_in.get("entropy_slope"),
            entropy_out=phys_out.get("entropy_slope"),
            origin=_norm_origin(meta),
            retention=0.0,
            seen=1,
            note=str(note or ""),
        )

# ==========================================================
# Provenance bonuses (file-backed, memoized)
# ==========================================================
def _prov_bonus_map() -> Dict[str, float]:
    return {
        "curiosity":     0.05,
        "sandbox":       0.05,
        "kb_xform":      0.03,
        "episodic":      0.03,
        "generator":     0.00,
        "direct_replay": 0.04,
        "human":         0.05,
        "xform_proxy":   0.02,
        "inference":     0.00,
        "none":          0.00,
        "solver":        0.04,
        "meta":          0.02,
    }

_PROV_BONUS_CACHE: Dict[str, Any] = {"mtime": None, "data": None}

def _load_prov_bonus() -> Dict[str, float]:
    path = os.path.join("deployment", "prov_bonus.json")
    try:
        if os.path.isfile(path):
            mtime = os.path.getmtime(path)
            if _PROV_BONUS_CACHE["mtime"] != mtime:
                with open(path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                out = {str(k): float(v) for k, v in data.items()} if isinstance(data, dict) else _prov_bonus_map()
                _PROV_BONUS_CACHE.update({"mtime": mtime, "data": out})
                try:
                    meta_log("holo.prov_bonus_loaded", mtime=float(mtime), entries=len(out))
                except Exception:
                    pass
            return dict(_PROV_BONUS_CACHE["data"] or {})
    except Exception:
        pass
    if _PROV_BONUS_CACHE["data"] is None:
        _PROV_BONUS_CACHE["data"] = _prov_bonus_map()
    return dict(_PROV_BONUS_CACHE["data"])

# Calibrated score with recency, optional global calibrator hook
def _calibrated_score(conf: float, ts: float, now: Optional[float] = None) -> float:
    try:
        now = time.time() if now is None else float(now)
        age = max(0.0, now - float(ts))
        rec = float(np.exp(-age / max(1e-6, KB_RECENCY_TAU))) if np is not None else 1.0
        conf = float(min(max(conf, KB_CONF_FLOOR), KB_CONF_CEIL))
        s = float(KB_RECENCY_ALPHA * conf + (1.0 - KB_RECENCY_ALPHA) * rec)
        if callable(HYBRID_GLOBAL_CAL):
            try:
                s = float(HYBRID_GLOBAL_CAL(s))
            except Exception:
                pass
        return s
    except Exception:
        return float(min(max(conf, KB_CONF_FLOOR), KB_CONF_CEIL))

# Atomic write + optional keel sidecar (from hardened v1)
def _atomic_write(path: str, data: bytes) -> None:
    tmp = f"{path}.tmp"
    with open(tmp, "wb") as f:
        f.write(data)
    os.replace(tmp, path)

def _maybe_keel(path: str, kind: str, *, is_internal: bool = True):    
    if not is_internal:
        return None, None

    keel_side = None
    ratio = None
    try:
        if "compress_file_keel_only" in globals():
            keel_side = compress_file_keel_only(path)  # type: ignore[name-defined]
            if keel_side and os.path.exists(keel_side):
                try:
                    s_src = os.path.getsize(path)
                    s_k = os.path.getsize(keel_side)
                    ratio = float(s_src) / max(1, float(s_k))
                except Exception:
                    ratio = None
    except Exception:
        keel_side = None
        ratio = None
    try:
        if callable(_meta_log):
            _meta_log("kb.exported", kind=kind, path=path, keel_sidecar=keel_side, ratio=ratio)
    except Exception:
        pass
    try:
        if _EXPLAIN is not None and hasattr(_EXPLAIN, "log"):
            _EXPLAIN.log("kb.exported", {"kind": kind, "path": path, "keel_sidecar": keel_side, "ratio": ratio})
    except Exception:
        pass
    return keel_side, ratio

def _compress_with_keel(path: str) -> Optional[str]:
    """
    Pack an external artifact using Keel if available (sidecar policy).
    Returns keel sidecar path if created; otherwise returns None.
    """
    try:
        if "_maybe_keel" in globals():
            keel_out, _ = _maybe_keel(path, kind="csv", is_internal=False)  # noqa: F821
            return keel_out
        # Fallback: write a tiny .keel marker (non-intrusive)
        side = path + ".keel"
        try:
            with open(side, "wb") as f:
                f.write(b"KEEL:external-sidecar")
            return side
        except Exception:
            return None
    except Exception:
        return None


# =========================================================
# Keel Decompression Utilities (auto-decompress on exposure)
# =========================================================
from pathlib import Path

def _keel_decode_bytes(blob: bytes) -> Optional[np.ndarray]:    
    # Prefer a directly exposed keel_decode if present
    kd = globals().get("keel_decode")
    if callable(kd):
        try:
            arr = kd(blob)
            return np.asarray(arr, dtype=np.uint8)
        except Exception:
            pass
    # Or try a module if the project bundles it (rename as needed)
    try:
        mod = __import__("keel_v341_monolith")
        if hasattr(mod, "keel_decode") and callable(mod.keel_decode):
            return np.asarray(mod.keel_decode(blob), dtype=np.uint8)
    except Exception:
        pass
    return None

def keel_file_to_png(k3x_path: str, out_png: Optional[str] = None) -> Optional[str]:    
    try:
        p = Path(k3x_path)
        if not p.is_file():
            return None
        blob = p.read_bytes()
        img_u8 = _keel_decode_bytes(blob)
        if img_u8 is None:
            return None
        # PIL may be None in some environments; guard it
        if "Image" not in globals() or Image is None:
            return None
        outp = out_png or (str(p.with_suffix("")) + ".png")
        Image.fromarray(img_u8).save(outp, "PNG", optimize=True)
        try:
            meta_log("keel.autodecompress_ok", src=str(p), out=outp)  # noqa: F821
        except Exception:
            pass
        return outp
    except Exception as e:
        try: meta_log("keel.autodecompress_fail", src=str(k3x_path), err=str(e))  # noqa: F821
        except Exception: pass
        return None

def prepare_for_download(path: str) -> str:    
    try:
        p = Path(path)
        sfx = p.suffix.lower()
        if sfx == ".k3x":
            outp = keel_file_to_png(str(p))
            if outp:
                return outp
        # .keel is an internal sidecar (policy). We do not expose it directly.
        if sfx == ".keel":
            try: meta_log("keel.policy_skip_expose", src=str(p))  # noqa: F821
            except Exception: pass
            return str(p)
    except Exception:
        pass
    return path


# =========================================================
# Recall-or-add (public)
# =========================================================
def recall_or_add(self, inp: np.ndarray, gold: np.ndarray, meta: dict) -> Tuple[np.ndarray, dict, float]:    
    hits = self.get(inp)
    if hits:
        # Assume first hit is the best; preserve original contract
        return hits[0]
    self.add(inp, gold, meta)
    return gold, meta, 0.0

# ----------------------
# Tiny global-safe helpers
# ----------------------
def _g(name: str):
    return globals().get(name, None)

def _log_info(msg: str):
    try:
        logger.info(msg)  # noqa: F821
    except Exception:
        pass

def _log_exc(msg: str):
    try:
        logger.exception(msg)  # noqa: F821
    except Exception:
        pass

def _ensure_dir(p: str):
    if not p:
        return
    try:
        os.makedirs(p, exist_ok=True)
    except Exception:
        pass

def _write_json(path: str, obj: Any):
    _ensure_dir(os.path.dirname(path))
    tmp = path + ".tmp"
    try:
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(obj, f, indent=2)
        os.replace(tmp, path)
    except Exception:
        # last-ditch
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(obj, f, indent=2)
        except Exception:
            pass

def _sha256(path: str) -> Optional[str]:
    try:
        h = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(1 << 20), b""):
                h.update(chunk)
        return h.hexdigest()
    except Exception:
        return None

def _safe_call(fn_name: str, *a, **kw):
    fn = _g(fn_name)
    if callable(fn):
        try:
            return fn(*a, **kw)
        except Exception as e:
            _log_exc(f"[{fn_name}] {e}")
            return None
    return None

def _now_ts() -> float:
    return time.time()

def _now_iso() -> str:
    import datetime as _dt
    return _dt.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

# Alias required by callers in this header block
_now = _now_ts

def _safe_shape(arr: np.ndarray) -> Tuple[int, int]:    
    try:
        a = np.asarray(arr)
        if a.ndim >= 2:
            return int(a.shape[0]), int(a.shape[1])
    except Exception:
        pass
    return (0, 0)

# ---- entropy (Shannon, palette) ----
def _entropy(arr) -> float:
    a = np.asarray(arr, dtype=int)
    _, cnts = np.unique(a, return_counts=True)
    p = cnts.astype(float)
    s = p.sum()
    if s <= 0:
        return 0.0
    p /= s
    p = p[(p > 0) & np.isfinite(p)]
    if p.size == 0:
        return 0.0
    return float(-(p * np.log(p + 1e-12)).sum())

# ---- binder-like index ----
def _binder_like(arr) -> float:
    a = np.asarray(arr, dtype=int)
    _, cnts = np.unique(a, return_counts=True)
    if cnts.size == 0:
        return 0.0
    p = cnts.astype(float) / max(1.0, cnts.sum())
    x = p - p.mean()
    m2 = float(np.sum(x**2))
    m4 = float(np.sum(x**4))
    if m2 <= 1e-12:
        return 0.0
    val = 1.0 - (m4 / (3.0 * (m2**2) + 1e-12))
    if not np.isfinite(val):
        return 0.0
    return float(max(0.0, min(1.0, val)))

# ---- shape similarity ----
def _ensure_int_ndarray(a):
    return np.asarray(a, dtype=int)

def pad_to_same_shape(a, b, pad_val=0):
    a = _ensure_int_ndarray(a); b = _ensure_int_ndarray(b)
    R = max(a.shape[0], b.shape[0]); C = max(a.shape[1], b.shape[1])
    A = np.full((R, C), pad_val, dtype=int)
    B = np.full((R, C), pad_val, dtype=int)
    A[:a.shape[0], :a.shape[1]] = a
    B[:b.shape[0], :b.shape[1]] = b
    return A, B

def _safe0(*args, **kwargs) -> float:
    return 0.0

def _shape_similarity(a, b) -> float:
    a = _ensure_int_ndarray(a); b = _ensure_int_ndarray(b)
    Ra, Ca = a.shape
    Rb, Cb = b.shape
    if Ra <= 0 or Ca <= 0 or Rb <= 0 or Cb <= 0:
        return 0.0
    r = min(Ra, Rb) / max(Ra, Rb)
    c = min(Ca, Cb) / max(Ca, Cb)
    return float(max(0.0, min(1.0, r * c)))

# ---- EPI ----
def _epi(a, b) -> float:
    A, B = pad_to_same_shape(a, b, pad_val=-1)
    mask = (A != -1) | (B != -1)
    if not np.any(mask):
        return 1.0
    return float(np.mean((A == B)[mask]))

# ---- physics plausibility ----
def _physics_plausibility(prev, cur) -> float:
    P = _ensure_int_ndarray(prev); C = _ensure_int_ndarray(cur)
    mP = float(np.sum(P != -1)); mC = float(np.sum(C != -1))
    mass_score = (1.0 if (mP <= 0 and mC <= 0)
                  else (1.0 - min(1.0, abs(mC - mP) / max(1.0, max(mP, mC)))))
    def _centroid(X):
        ys, xs = np.where(X != -1)
        if ys.size == 0:
            return (0.0, 0.0)
        return (float(np.mean(ys)), float(np.mean(xs)))
    cPy, cPx = _centroid(P); cCy, cCx = _centroid(C)
    R, Cc = max(P.shape[0], C.shape[0]), max(P.shape[1], C.shape[1])
    norm = float(max(1.0, (R**2 + Cc**2) ** 0.5))
    d = ((cCy - cPy)**2 + (cCx - cPx)**2) ** 0.5 / norm
    cent_score = 1.0 - max(0.0, min(1.0, d))
    return float(max(0.0, min(1.0, 0.6 * mass_score + 0.4 * cent_score)))

# ---- invariant composite ----
def _inv_composite(prev, cur) -> float:
    try:
        b = _binder_like(cur)
    except Exception:
        b = 0.0
    try:
        s = _epi(prev, cur)
    except Exception:
        s = 0.0
    return float(max(0.0, min(1.0, 0.5 * b + 0.5 * s)))

# ==========================================================
# Global Recursive Harmonic Collapse Matrix (RHCM) Kernel
# ==========================================================
phi = (1 + np.sqrt(5)) / 2

def fibonacci(n: int) -> int:
    fibs = [1, 1]
    for _ in range(2, n + 1):
        fibs.append(fibs[-1] + fibs[-2])
    return fibs[n] if n < len(fibs) else fibs[-1]

def generate_symbolic_entropy(n: int, seed: int = 42) -> np.ndarray:
    rng = np.random.default_rng(seed)
    base = rng.random((n, n))
    return np.sin(np.pi * base) + np.cos(np.pi * np.flip(base, axis=1))

def generate_RHCM(n: int, recursion_depth: float = 1.618, entropy_matrix: Optional[np.ndarray] = None) -> np.ndarray:
    F_n = max(1, fibonacci(n))
    Ψ = entropy_matrix if entropy_matrix is not None else generate_symbolic_entropy(n)
    M = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            try:
                v = (phi ** recursion_depth) * np.sin(np.pi * (i + 1) / (j + 1))
                v += np.log2((j + 1) + np.abs(Ψ[i, j]) + 1e-9)
                M[i, j] = np.floor(v) % F_n
            except Exception:
                M[i, j] = 0
    return M

def generate_cRHCM(n: int, recursion_depth: float = 1.618, entropy_matrix: Optional[np.ndarray] = None) -> np.ndarray:
    Ψ = entropy_matrix if entropy_matrix is not None else generate_symbolic_entropy(n)
    M = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            try:
                v = (phi ** recursion_depth) * np.sin(np.pi * (i + 1) / (j + 1))
                v += np.log2((j + 1) + np.abs(Ψ[i, j]) + 1e-9)
                M[i, j] = v
            except Exception:
                M[i, j] = 0
    return M

def generate_RHCM_inverse(M: np.ndarray, n: int) -> np.ndarray:
    F_n = max(1, fibonacci(n))
    return (F_n - M)

def run_feedback_simulation(n: int, depth: float = 1.618, seed: int = 42, iterations: int = 6, continuous: bool = False) -> List[np.ndarray]:
    Ψ = generate_symbolic_entropy(n, seed)
    seq = []
    for _ in range(max(1, iterations)):
        M = generate_cRHCM(n, depth, Ψ) if continuous else generate_RHCM(n, depth, Ψ)
        Minv = -M if continuous else generate_RHCM_inverse(M, n)
        Ψ = M - Minv
        seq.append(Ψ.copy())
    return seq

def compute_normalized_determinants(max_n: int = 15) -> List[float]:
    out = []
    for n in range(2, max_n + 1):
        M = generate_RHCM(n)
        norm = float(np.max(np.abs(M)) or 1.0)
        out.append(float(np.linalg.det(M)) / norm)
    return out

def track_std_over_feedback(sequence: List[np.ndarray]) -> List[float]:
    return [float(np.std(s)) for s in sequence]

def track_unique_values(sequence: List[np.ndarray]) -> List[int]:
    return [int(np.unique(s).size) for s in sequence]

def compute_EPI(sequence: List[np.ndarray], reference: np.ndarray, threshold: float = 1e-5) -> float:
    matches = [bool(np.allclose(s, reference, atol=threshold)) for s in sequence]
    return float(sum(matches) / max(1, len(sequence)))

def get_rhcm_feedback_profile(n: int = 8, depth: float = 1.618, seed: int = 42) -> Dict[str, Any]:
    seq = run_feedback_simulation(n, depth, seed, iterations=6, continuous=False)
    return {
        "std": track_std_over_feedback(seq),
        "unique": track_unique_values(seq),
        "EPI": compute_EPI(seq, seq[0])
    }

# -----------------------------
# Shared lightweight calibrator
# -----------------------------
class SigmoidCalibrator:
    def __init__(self, A: Optional[float]=None, B: Optional[float]=None):
        try:
            self.A = float(os.getenv("HYBRID_CAL_A")) if A is None and os.getenv("HYBRID_CAL_A") else (1.0 if A is None else float(A))
            self.B = float(os.getenv("HYBRID_CAL_B")) if B is None and os.getenv("HYBRID_CAL_B") else (0.0 if B is None else float(B))
        except Exception:
            self.A, self.B = 1.0, 0.0
    def predict(self, x: float) -> float:
        try:
            z = self.A * float(x) + self.B
            z = max(-20.0, min(20.0, z))
            p = 1.0 / (1.0 + math.exp(-z))
            return float(max(0.0, min(1.0, p)))
        except Exception:
            try:
                return float(max(0.0, min(1.0, float(x))))
            except Exception:
                return 0.5
    def fit(self, xs: List[float], ys: List[int], iters: int = 50, lr: float = 0.05):
        try:
            A, B = self.A, self.B
            n = max(1, len(xs))
            for _ in range(iters):
                gA = gB = 0.0
                for xi, yi in zip(xs, ys):
                    z = A * float(xi) + B
                    z = max(-20.0, min(20.0, z))
                    p = 1.0 / (1.0 + math.exp(-z))
                    g = (p - yi)
                    gA += g * xi
                    gB += g
                A -= lr * (gA / n)
                B -= lr * (gB / n)
            self.A, self.B = float(A), float(B)
        except Exception:
            pass

# -----------------------------
# Unified local emitter (downsample + strict + topic cap)
# -----------------------------
class _ModuleEmitter:
    def __init__(self, meta=None, module_name: str="module"):
        self.meta = meta
        self.module_name = module_name
        self._ctr = 0
        self._ds = max(1, int(META_TEL_DOWNSAMPLE_N))
        self._topic_counts: Dict[str, int] = {}
        self._topic_cap = int(os.getenv("META_TOPIC_CAP", "100000"))  # generous, but finite
    def emit(self, topic: str, **payload):
        # downsample
        self._ctr += 1
        if (self._ctr % self._ds) != 0:
            return
        # per-topic cap
        c = self._topic_counts.get(topic, 0) + 1
        self._topic_counts[topic] = c
        if c > self._topic_cap:
            return
        rec = {"module": self.module_name, "event": topic, "time": time.time(), **payload}
        # meta_log via safe shim if available
        try:
            if "_meta_log" in globals() and callable(globals().get("_meta_log")):
                _meta_log(topic, **rec)  # type: ignore[name-defined]
        except Exception:
            if META_EMIT_STRICT: raise
        # EXPLAIN
        try:
            ex = globals().get("EXPLAIN")
            if ex is not None and hasattr(ex, "log"):
                ex.log(topic, rec)
        except Exception:
            if META_EMIT_STRICT: raise
        # Meta.telemetry
        try:
            if self.meta is not None and hasattr(self.meta, "_log_telemetry"):
                self.meta._log_telemetry(rec)
        except Exception:
            if META_EMIT_STRICT: raise
        # Holo telemetry echo (append-only)
        try:
            _h = getattr(self.meta, "holo", None)
            if _h is not None and hasattr(_h, "_telemetry"):
                _h._telemetry(topic, **rec)
        except Exception:
            pass
        return rec

# ----------------------------
# Meta-log dispatch (async/non-blocking)
# ----------------------------
def _META_LOG_NOOP(topic: str, payload: Dict[str, Any]) -> None:
    return

if "META_LOG_EVENT" not in globals():
    META_LOG_EVENT: Callable[[str, Dict[str, Any]], None] = _META_LOG_NOOP

_meta_pool = ThreadPoolExecutor(max_workers=2)

def set_meta_log_event(callback: Optional[Callable[[str, Dict[str, Any]], None]]) -> None:
    global META_LOG_EVENT
    META_LOG_EVENT = callback if callable(callback) else _META_LOG_NOOP

def _meta_log_impl(topic: str, payload: Dict[str, Any]) -> None:
    try:
        # propagate KEEL dials if present
        if 'kairos' in globals():
            k = globals().get("kairos")
            if hasattr(k, "step"):
                k.step(int(time.time() % 1000))
            if hasattr(k, "update_keel_ratio"):
                rin  = payload.get("keel_in",  payload.get("keel_ratio_in"))
                rout = payload.get("keel_out", payload.get("keel_ratio_out"))
                psnr = payload.get("keel_psnr_out")
                ssim = payload.get("keel_ssim_out")
                if any(v is not None for v in (rin, rout, psnr, ssim)):
                    k.update_keel_ratio(rin, rout, psnr, ssim)
        META_LOG_EVENT(topic, dict(payload))
    except Exception:
        pass

def meta_log(topic: str, **payload: Any) -> None:
    try:
        _meta_pool.submit(_meta_log_impl, topic, dict(payload))
    except Exception:
        # last-resort fallback
        try:
            _META_LOG_NOOP(topic, dict(payload))
        except Exception:
            pass

# -------------------------------------
# Logging setup (+ entropy-synced formatter)
# -------------------------------------
class KairosFormatter(logging.Formatter):
    def format(self, record):
        record.kairos_state = getattr(globals().get("kairos", None), "symbolic_state", "Ω₀")
        record.entropy_flux = getattr(globals().get("kairos", None), "last_entropy_flux", 0.0)
        return super().format(record)

def _build_logger():
    fmt = "%(asctime)s %(levelname)s [Ω=%(kairos_state)s φ=%(entropy_flux).3f] %(message)s"
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(KairosFormatter(fmt))
    lg = logging.getLogger(__name__)
    lg.setLevel(logging.INFO)
    lg.handlers.clear()
    lg.addHandler(handler)
    return lg

logger = _build_logger()

def set_log_level(level: str = "INFO"):
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))
    meta_log("logger.level", level=level)

# ----------------------------------------------------------
# Unified Rule/RuleRecord schema (canonical): params
# ----------------------------------------------------------
def _as_dict(x):
    try:
        return dict(x or {})
    except Exception:
        return {}

@dataclass
class Rule:
    kind: str
    params: Dict[str, Any] = field(default_factory=dict)

    @property
    def payload(self) -> Dict[str, Any]:
        return self.params
    @payload.setter
    def payload(self, v: Dict[str, Any]):
        self.params = _as_dict(v)

    # Convenience: construct from either payload/params mapping
    @staticmethod
    def from_any(
        kind: str,
        mapping: Optional[Dict[str, Any]] = None,
        *,
        payload: Optional[Dict[str, Any]] = None,
        params: Optional[Dict[str, Any]] = None
    ) -> "Rule":
        if params is not None:
            return Rule(kind=kind, params=_as_dict(params))
        if payload is not None:
            return Rule(kind=kind, params=_as_dict(payload))
        return Rule(kind=kind, params=_as_dict(mapping))

@dataclass
class RuleRecord:
    input_grid: np.ndarray
    output_grid: np.ndarray
    rule: Rule
    meta: Dict[str, Any]

# ----------------------------------------------------------
# Helper: APPLY_OPS / apply_ops (global, robust resolver)
# ----------------------------------------------------------
_APPLY_OPS_FN = None  # lazy-resolved and cached

def _resolve_apply_ops():
    """Find and cache the concrete apply-ops implementation."""
    global _APPLY_OPS_FN
    if callable(_APPLY_OPS_FN):
        return _APPLY_OPS_FN
    # Preferred order: sandbox-specific, then generic
    for name in ("sandbox_apply_ops", "apply_ops_impl", "apply_ops"):
        fn = globals().get(name)
        if callable(fn):
            _APPLY_OPS_FN = fn
            try:
                if "meta_log" in globals():
                    meta_log("apply_ops.resolved", impl=name)  # noqa: F821
            except Exception:
                pass
            return _APPLY_OPS_FN
    return None

def APPLY_OPS(inp, ops, *, strict: bool = None):   
    if strict is None:
        try:
            strict = bool(int(os.getenv("APPLY_OPS_STRICT", "1")))
        except Exception:
            strict = True

    fn = _resolve_apply_ops()
    if not callable(fn):
        try:
            if "meta_log" in globals():
                meta_log("apply_ops.unresolved", strict=bool(strict))  # noqa: F821
        except Exception:
            pass
        if strict:
            raise RuntimeError("apply_ops: no implementation found (sandbox_apply_ops/apply_ops_impl/apply_ops)")
        return inp

    # Basic input normalization for common failure modes
    try:
        # None or empty ops -> identity
        if ops is None:
            return inp
        # If ops is not iterable, wrap it
        if not isinstance(ops, (list, tuple)):
            ops = [ops]
    except Exception:
        # fall back to raw call; let the impl decide
        pass

    try:
        return fn(inp, ops)
    except Exception as e:
        # Emit best-effort telemetry and either raise or return input
        try:
            if "meta_log" in globals():
                meta_log("apply_ops.error", err=str(e))  # noqa: F821
        except Exception:
            pass
        if strict:
            raise
        return inp

# Lowercase alias for legacy callers
def apply_ops(inp, ops, **kw):   
    return APPLY_OPS(inp, ops, **kw)


# -----------------------------
# Normalizers (in-memory + I/O)
# -----------------------------
def normalize_rule(r: Any) -> Rule:
    try:
        kind = str(getattr(r, "kind"))
    except Exception:
        raise TypeError("normalize_rule: missing 'kind' on rule-like object")
    # Prefer params if present, else payload
    if hasattr(r, "params"):
        return Rule(kind=kind, params=_as_dict(getattr(r, "params")))
    if hasattr(r, "payload"):
        return Rule(kind=kind, params=_as_dict(getattr(r, "payload")))
    # Dict-like fallback
    rp = getattr(r, "__dict__", {})
    if "params" in rp:
        return Rule(kind=kind, params=_as_dict(rp["params"]))
    if "payload" in rp:
        return Rule(kind=kind, params=_as_dict(rp["payload"]))
    return Rule(kind=kind, params={})

def normalize_record(rec: Any) -> RuleRecord:
    if isinstance(rec, RuleRecord):
        # Ensure its rule is normalized (might have been constructed with 'payload')
        if not isinstance(rec.rule, Rule) or (hasattr(rec.rule, "payload") and not hasattr(rec.rule, "params")):
            return RuleRecord(
                input_grid=np.array(rec.input_grid, dtype=int),
                output_grid=np.array(rec.output_grid, dtype=int),
                rule=normalize_rule(rec.rule),
                meta=_as_dict(rec.meta),
            )
        return rec
    # dict-like input
    try:
        d = dict(rec)
    except Exception:
        raise TypeError("normalize_record: expected RuleRecord or dict-like")
    inp = np.array(d["input_grid"], dtype=int)
    out = np.array(d["output_grid"], dtype=int)
    rj  = d.get("rule", {})
    # rj may be object or dict
    if isinstance(rj, dict):
        kind = str(rj.get("kind", "unknown"))
        params = rj.get("params")
        payload = rj.get("payload")
        r = Rule.from_any(kind, params=params, payload=payload)
    else:
        r = normalize_rule(rj)
    meta = _as_dict(d.get("meta", {}))
    return RuleRecord(input_grid=inp, output_grid=out, rule=r, meta=meta)

# INSERT ONCE – params-only contract validator
def validate_rule_contract(rule_like: Any) -> None:
    """
    Enforces Rule interface: tuple(kind, params_dict) or Rule(kind, params).
    - kind: str
    - params: dict (no 'payload' allowed; use 'params' only)
    - for kind == "xform": params must include 'ops' (list of (op, dict))
    """
    # Normalize shape
    if hasattr(rule_like, "kind") and hasattr(rule_like, "params"):
        kind = getattr(rule_like, "kind")
        params = getattr(rule_like, "params")
    elif isinstance(rule_like, tuple) and len(rule_like) == 2:
        kind, params = rule_like
    else:
        raise TypeError("validate_rule_contract: unexpected rule shape")

    if not isinstance(kind, str):
        raise TypeError("validate_rule_contract: kind must be str")
    if not isinstance(params, dict):
        raise TypeError("validate_rule_contract: params must be dict")
    if "payload" in params:
        raise TypeError("validate_rule_contract: 'payload' not allowed (params-only policy)")
    if kind == "xform":
        ops = params.get("ops", None)
        if not isinstance(ops, (list, tuple)):
            raise TypeError("validate_rule_contract: xform.params['ops'] must be list/tuple")
        for ix, op in enumerate(ops):
            if not (isinstance(op, (list, tuple)) and len(op) == 2 and isinstance(op[0], str) and isinstance(op[1], dict)):
                raise TypeError(f"validate_rule_contract: ops[{ix}] must be (str, dict)")



# ----------------------------------------------------------
# Unified Rule/RuleRecord schema (canonical): PARAMS-ONLY
# ----------------------------------------------------------
from types import SimpleNamespace  # required by splits/open-ended loader

def _as_dict(x):
    try:
        return dict(x or {})
    except Exception:
        return {}

@dataclass
class Rule:
    kind: str
    params: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        # Strict: params-only (no payload compatibility). Enforce mapping.
        if not isinstance(self.params, dict):
            try:
                self.params = dict(self.params)  # last-chance coercion
            except Exception:
                raise TypeError("Rule.params must be a mapping (dict-like)")

    @staticmethod
    def from_any(kind: str, *, params: Optional[Dict[str, Any]] = None) -> "Rule":       
        if params is None:
            params = {}
        if not isinstance(params, dict):
            try:
                params = dict(params)
            except Exception:
                raise TypeError("Rule.from_any: 'params' must be a mapping (dict-like)")
        return Rule(kind=kind, params=params)

@dataclass
class RuleRecord:
    input_grid: np.ndarray
    output_grid: np.ndarray
    rule: Rule
    meta: Dict[str, Any]

# -----------------------------
# Normalizers (in-memory + I/O)
# -----------------------------
def normalize_rule(r: Any) -> Rule:    
    # Direct Rule
    if isinstance(r, Rule):
        if not isinstance(r.params, dict):
            raise TypeError("normalize_rule: Rule.params must be a mapping")
        return r

    # Object with attributes
    kind = getattr(r, "kind", None) if hasattr(r, "kind") else None
    if kind is None:
        # dict-like fallback to try to get kind
        rp = getattr(r, "__dict__", {})
        if isinstance(r, dict):
            rp = r
        kind = rp.get("kind")
    if kind is None:
        raise TypeError("normalize_rule: missing 'kind'")

    # Dict-like extraction
    if isinstance(r, dict):
        if "payload" in r:
            raise TypeError("normalize_rule: 'payload' is not supported (params-only policy)")
        params = r.get("params", {})
        return Rule.from_any(str(kind), params=params)

    # Attribute extraction
    if hasattr(r, "payload"):
        # Explicitly disallow residual payload usage
        raise TypeError("normalize_rule: '.payload' is not supported (params-only policy)")
    params = getattr(r, "params", {})
    return Rule.from_any(str(kind), params=params)

def normalize_record(rec: Any) -> RuleRecord:   
    if isinstance(rec, RuleRecord):
        # ensure grids are int arrays and rule is normalized
        return RuleRecord(
            input_grid=np.array(rec.input_grid, dtype=int),
            output_grid=np.array(rec.output_grid, dtype=int),
            rule=normalize_rule(rec.rule),
            meta=_as_dict(rec.meta),
        )
    # dict-like input only
    try:
        d = dict(rec)
    except Exception:
        raise TypeError("normalize_record: expected RuleRecord or dict-like")

    if "payload" in d or ("rule" in d and isinstance(d["rule"], dict) and "payload" in d["rule"]):
        raise TypeError("normalize_record: 'payload' is not supported (params-only policy)")

    inp = np.array(d["input_grid"], dtype=int)
    out = np.array(d["output_grid"], dtype=int)
    rj  = d.get("rule", {})
    if isinstance(rj, dict):
        kind = str(rj.get("kind", "unknown"))
        params = rj.get("params", {})
        r = Rule.from_any(kind, params=params)
    else:
        r = normalize_rule(rj)
    meta = _as_dict(d.get("meta", {}))
    return RuleRecord(input_grid=inp, output_grid=out, rule=r, meta=meta)

# -----------------------------
# JSON helpers (strict: params-only)
# -----------------------------
def rule_to_json(rule: Rule) -> Dict[str, Any]:
    return {"kind": rule.kind, "params": dict(rule.params)}

def rule_from_json(d: Dict[str, Any]) -> Rule:
    if "params" not in d:
        raise TypeError("rule_from_json: 'params' missing (params-only policy)")
    if "payload" in d:
        raise TypeError("rule_from_json: 'payload' not allowed (params-only policy)")
    return Rule(kind=str(d.get("kind", "unknown")), params=dict(d["params"]))

def record_to_json(rec: RuleRecord) -> Dict[str, Any]:
    return {
        "input_grid": np.array(rec.input_grid, dtype=int).tolist(),
        "output_grid": np.array(rec.output_grid, dtype=int).tolist(),
        "rule": rule_to_json(rec.rule),
        "meta": dict(rec.meta or {}),
    }

def record_from_json(d: Dict[str, Any]) -> RuleRecord:
    if "payload" in d or ("rule" in d and isinstance(d["rule"], dict) and "payload" in d["rule"]):
        raise TypeError("record_from_json: 'payload' not allowed (params-only policy)")
    return normalize_record(d)

# ============================================================================
# CSV helpers for rules & predictions  • ATOMIC • PARAMS-ONLY • KEEL (Memory-only)
# ============================================================================

# --- tiny safe utils (namespaced to avoid collisions) -------------------------
_provenance_header_dict = _prov_header_csv           # type: ignore[name-defined]
_holo_snapshot = _holo_snapshot_csv                  # type: ignore[name-defined]
_zip_dir = _zip_dir_csv                              # type: ignore[name-defined]

def _now_iso_csv() -> str:
    try:
        return time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime())
    except Exception:
        return str(time.time())

def _byte_entropy_csv(path: str) -> float:
    try:
        with open(path, "rb") as f:
            raw = f.read()
        if not raw or np is None:
            return 0.0
        cnts = np.bincount(np.frombuffer(raw, dtype=np.uint8), minlength=256).astype(float)
        p = cnts / max(1.0, cnts.sum())
        p = p[p > 0]
        return float(-(p * np.log2(p)).sum()) if p.size else 0.0
    except Exception:
        return 0.0

def _holo_snapshot_csv(tag: str):
    try:
        if "holo" in globals() and hasattr(holo, "snapshot"):
            holo.snapshot(tag)  # noqa: F821
    except Exception:
        pass

def _zip_dir_csv(dir_path: str, out_zip: Optional[str] = None) -> Optional[str]:
    try:
        out_zip = out_zip or (dir_path.rstrip("/\\") + ".zip")
        tmp = out_zip + ".tmp"
        with zipfile.ZipFile(tmp, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            for root, _, files in os.walk(dir_path):
                for fn in files:
                    fp = os.path.join(root, fn)
                    arc = os.path.relpath(fp, start=dir_path)
                    zf.write(fp, arc)
        os.replace(tmp, out_zip)
        return out_zip
    except Exception as e:
        try: meta_log("rule_cards.export_fail", fn=dir_path, err=str(e))  # noqa: F821
        except Exception: pass
        if "logger" in globals():
            logger.exception(f"[zip] failed for {dir_path}: {e}")
        return None

def _prov_header_csv(extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    from uuid import uuid4 as _uuid4  # avoid clobbering global uuid if present
    hdr = {
        "run_id": _uuid4().hex[:8],
        "timestamp": _now_iso_csv(),
        "host": socket.gethostname() if hasattr(socket, "gethostname") else "kaggle",
        "solver_version": globals().get("__version__", "unknown"),
        "kairos_seed": getattr(globals().get("kairos", None), "seed", None),
    }
    if extra:
        hdr.update(extra)
    try:
        meta_log("csv.header", **hdr)  # noqa: F821
    except Exception:
        pass
    return hdr

def _attempts_from_prediction(pred: np.ndarray, attempts: int = 2) -> List[Dict[str, List[List[int]]]]:
    # Be tolerant to upstream providing float preds; sanitize to int
    try:
        pred_list = _sanitize_grid(pred).tolist()  # noqa: F821
    except Exception:
        arr = np.asarray(pred)
        if np.issubdtype(arr.dtype, np.floating):
            arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)
        pred_list = np.asarray(arr, dtype=int).tolist()
    attempts = max(1, int(attempts))
    d = {}
    for i in range(1, attempts + 1):
        d[f"attempt_{i}"] = pred_list
    return [d]

def _atomic_write_csv_rows(path: str, headers: List[str], rows: List[Dict[str, Any]], *, schema: str) -> None:
    tmp = path + ".tmp"
    with open(tmp, "w", newline="", encoding="utf-8") as f:
        f.write(f"# schema:{schema}\n")
        w = csv.DictWriter(f, fieldnames=headers)
        w.writeheader()
        for r in rows:
            w.writerow(r)
    os.replace(tmp, path)

# provenance sidecar + validation + optional KEEL sidecar (Memory-only)
def _write_csv_training_rules(
    rulebase: 'GlobalRulebase',
    csv_path: str,
    *,
    holo_snap: bool = True,
    is_internal_keel: bool = False  # Memory-only callers set True
) -> None:
    if holo_snap: _holo_snapshot_csv(f"csv_pre_{os.path.basename(csv_path)}")
    prov = _prov_header_csv({"kind": "training_rules", "schema": "training_rules.v1"})
    try:
        os.makedirs(os.path.dirname(csv_path) or ".", exist_ok=True)
        tmp = csv_path + ".tmp"
        with open(tmp, "w", newline="", encoding="utf-8") as f:
            f.write("# schema:training_rules.v1\n")
            w = csv.writer(f)
            w.writerow(["idx","task_id","train_index","rule_kind",
                        "in_shape","out_shape","hist_in_hash","hist_out_hash","glyph_hash"])
            for i, rec in enumerate(getattr(rulebase, "records", [])):
                hi = rec.meta.get("hist_in", {}) if hasattr(rec, "meta") else {}
                ho = rec.meta.get("hist_out", {}) if hasattr(rec, "meta") else {}
                tid = rec.meta.get("task_id", "")
                tix = rec.meta.get("train_index", -1)
                kind = rec.rule.kind
                glyph_hash = hashlib.sha1(f"{tid}:{tix}:{kind}".encode()).hexdigest()[:10]
                w.writerow([
                    i,
                    tid,
                    tix,
                    kind,
                    str(tuple(rec.input_grid.shape)) if rec.input_grid is not None else "(0,0)",
                    str(tuple(rec.output_grid.shape)) if rec.output_grid is not None else "(0,0)",
                    hashlib.sha1(str(sorted(hi.items())).encode()).hexdigest()[:8],
                    hashlib.sha1(str(sorted(ho.items())).encode()).hexdigest()[:8],
                    glyph_hash
                ])
        os.replace(tmp, csv_path)

        # provenance sidecar
        meta_path = csv_path + ".meta.json"
        tmpm = meta_path + ".tmp"
        with open(tmpm, "w", encoding="utf-8") as mf:
            json.dump(prov, mf, indent=2)
        os.replace(tmpm, meta_path)

        # validation
        try:
            assert os.path.getsize(csv_path) > 0, "Empty CSV detected"
        except Exception as e:
            try: meta_log("rules.csv.validation_fail", fn=csv_path, err=str(e))  # noqa: F821
            except Exception: pass
            if "logger" in globals(): logger.exception(f"[training_rules] validation failed: {e}")

        # KEEL sidecar (Memory-only)
        keel_out = None
        if is_internal_keel:
            try:
                keel_out, _ = _maybe_keel(csv_path, kind="csv", is_internal=True)  # noqa: F821
            except Exception:
                keel_out = None

        # exposure hook (no-op for CSV)
        try:
            _ = prepare_for_download(csv_path)  # noqa: F821
        except Exception:
            pass

        if holo_snap: _holo_snapshot_csv(f"csv_post_{os.path.basename(csv_path)}")
        try:
            meta_log("rules.csv_written", path=csv_path,
                     count=len(getattr(rulebase, "records", [])),
                     sidecar=meta_path, keel_out=keel_out)  # noqa: F821
        except Exception:
            pass
    except Exception as e:
        try:
            meta_log("rules.csv.write_fail", fn=csv_path, err=str(e))  # noqa: F821
        except Exception:
            pass
        if "logger" in globals():
            logger.exception(f"[_write_csv_training_rules] failed: {e}")

# provenance + validation + optional KEEL sidecar (Memory-only)
def _write_csv_eval_predictions(
    rows: List[Dict[str,Any]],
    csv_path: str,
    *,
    holo_snap: bool = True,
    is_internal_keel: bool = False
) -> None:
    keys = ["task_id","test_index","prediction","ok_shape",
            "rule_kind","sim_score","correct"]
    if holo_snap: _holo_snapshot_csv(f"csv_pre_{os.path.basename(csv_path)}")
    prov = _prov_header_csv({"kind": "eval_predictions", "schema": "eval_predictions.v1"})
    try:
        os.makedirs(os.path.dirname(csv_path) or ".", exist_ok=True)
        tmp = csv_path + ".tmp"
        with open(tmp, "w", newline="", encoding="utf-8") as f:
            f.write("# schema:eval_predictions.v1\n")
            w = csv.DictWriter(f, fieldnames=keys)
            w.writeheader()
            for r in rows:
                pred_json = json.dumps(r["prediction"])
                sim = float(r.get("sim_score", 0.0))
                w.writerow({
                    "task_id": r["task_id"],
                    "test_index": r["test_index"],
                    "prediction": pred_json,
                    "ok_shape": bool(r.get("ok_shape", False)),
                    "rule_kind": r.get("rule_kind", ""),
                    "sim_score": round(sim, 6),
                    "correct": r.get("correct", "")
                })
        os.replace(tmp, csv_path)

        # sidecar
        meta_path = csv_path + ".meta.json"
        tmpm = meta_path + ".tmp"
        with open(tmpm, "w", encoding="utf-8") as mf:
            json.dump(prov, mf, indent=2)
        os.replace(tmpm, meta_path)

        # validation
        try:
            assert os.path.getsize(csv_path) > 0, "Empty CSV detected"
        except Exception as e:
            try: meta_log("eval.csv.validation_fail", fn=csv_path, err=str(e))  # noqa: F821
            except Exception: pass
            if "logger" in globals(): logger.exception(f"[eval_predictions] validation failed: {e}")

        # KEEL sidecar (Memory-only)
        keel_out = None
        if is_internal_keel:
            try:
                keel_out, _ = _maybe_keel(csv_path, kind="csv", is_internal=True)  # noqa: F821
            except Exception:
                keel_out = None

        # expose hook (no-op for CSV)
        try:
            _ = prepare_for_download(csv_path)  # noqa: F821
        except Exception:
            pass

        if holo_snap: _holo_snapshot_csv(f"csv_post_{os.path.basename(csv_path)}")
        try:
            meta_log("eval.csv_written", path=csv_path, count=len(rows),
                     sidecar=meta_path, keel_out=keel_out)  # noqa: F821
        except Exception:
            pass
    except Exception as e:
        try:
            meta_log("eval.csv.write_fail", fn=csv_path, err=str(e))  # noqa: F821
        except Exception:
            pass
        if "logger" in globals():
            logger.exception(f"[_write_csv_eval_predictions] failed: {e}")

def export_rule_cards(
    rulebase: 'GlobalRulebase',
    out_dir: str = "kaggle_cards",
    csv_index: str = "rule_cards.csv",
    *,
    parallel: bool = False,
    max_workers: Optional[int] = None,
    compress: Optional[str] = None,   # None | "zip" (directory only)
    holo_snap: bool = True,
    solver: Optional['ARCSymbolicUltra'] = None,
    is_internal_keel: bool = False
):
    # Toggle & environment guard
    if solver and hasattr(solver, "toggles") and not solver.toggles.CSV_EXPORTS:
        if "logger" in globals(): logger.info("[export_rule_cards] CSV_EXPORTS disabled by toggles.")
        return

    os.makedirs(out_dir, exist_ok=True)
    rows: List[Dict[str, Any]] = []
    errors: List[str] = []

    # Holo snapshot before write
    if holo_snap and "holo" in globals():
        try: holo.snapshot(f"csv_pre_{os.path.basename(csv_index)}")  # noqa: F821
        except Exception: pass

    # Provenance header for meta sidecar
    prov = _prov_header_csv({"kind": "rule_cards", "schema": "rule_cards.v1", "out_dir": out_dir})

    # Inner card writer (parallel OK; JSON files are distinct; CSV rows aggregated in memory)
    def _write_card(i: int, rec: Any) -> Tuple[int, Optional[str], Optional[str], Dict[str, Any]]:
        tid = rec.meta.get("task_id", f"unknown_{i}")
        tix = rec.meta.get("train_index", None)
        meta = {
            "task_id": tid,
            "train_index": tix,
            "rule_kind": rec.rule.kind,
            "input_shape": (list(rec.input_grid.shape) if rec.input_grid is not None else [0,0]),
            "output_shape": (list(rec.output_grid.shape) if rec.output_grid is not None else [0,0]),
            "hist_in": {int(k): int(v) for k,v in rec.meta.get("hist_in", {}).items()},
            "hist_out": {int(k): int(v) for k,v in rec.meta.get("hist_out", {}).items()},
            "pattern_in": rec.meta.get("pattern_in", []),
            "pattern_out": rec.meta.get("pattern_out", []),
            "objects_in": rec.meta.get("objects_in", []),
            "objects_out": rec.meta.get("objects_out", []),
            "rule_params": dict(rec.rule.params),
        }
        glyph_hash = hashlib.sha1(f"{tid}:{tix}:{rec.rule.kind}".encode()).hexdigest()[:10]
        json_path = os.path.join(out_dir, f"{tid}__train{tix if tix is not None else i}.json")
        err = None
        try:
            tmpj = json_path + ".tmp"
            with open(tmpj, "w", encoding="utf-8") as f:
                json.dump(meta, f, indent=2)
            os.replace(tmpj, json_path)
        except Exception as e:
            err = str(e)
            if "logger" in globals(): logger.exception(f"[export_rule_cards] write json failed for {json_path}: {e}")
            try: meta_log("rule_cards.export_fail", fn=json_path, err=str(e))  # noqa: F821
            except Exception: pass

        row = {
            "idx": i,
            "task_id": tid,
            "train_index": tix if tix is not None else -1,
            "rule_kind": rec.rule.kind,
            "in_shape": str(tuple(rec.input_grid.shape)) if rec.input_grid is not None else "(0,0)",
            "out_shape": str(tuple(rec.output_grid.shape)) if rec.output_grid is not None else "(0,0)",
            "hist_in_hash": hashlib.sha1(str(sorted(meta["hist_in"].items())).encode()).hexdigest()[:8],
            "hist_out_hash": hashlib.sha1(str(sorted(meta["hist_out"].items())).encode()).hexdigest()[:8],
            "glyph_hash": glyph_hash,
        }
        return i, json_path, err, row

    # Write JSON cards (parallel optional); collect CSV rows in memory to avoid parallel appends
    records = list(getattr(rulebase, "records", []))
    if parallel and len(records) > 1:
        try:
            from concurrent.futures import ThreadPoolExecutor as _TPE, as_completed as _as_completed
        except Exception:
            _TPE = None; _as_completed = None
        if _TPE and _as_completed:
            max_workers = max_workers or min(8, (os.cpu_count() or 4))
            futures = {}
            with _TPE(max_workers=max_workers) as ex:
                for i, rec in enumerate(records):
                    futures[ex.submit(_write_card, i, rec)] = i
                for fut in _as_completed(futures):
                    try:
                        _, _, err, row = fut.result()
                        if err: errors.append(err)
                        rows.append(row)
                    except Exception as e:
                        errors.append(str(e))
                        if "logger" in globals():
                            logger.exception(f"[export_rule_cards.parallel] card task failed: {e}")
                        try: meta_log("rule_cards.export_fail", fn=f"card[{futures[fut]}]", err=str(e))  # noqa: F821
                        except Exception: pass
        else:
            for i, rec in enumerate(records):
                _, _, err, row = _write_card(i, rec)
                if err: errors.append(err)
                rows.append(row)
    else:
        for i, rec in enumerate(records):
            _, _, err, row = _write_card(i, rec)
            if err: errors.append(err)
            rows.append(row)

    # Single atomic CSV write (no parallel appends)
    try:
        headers = [
            "idx","task_id","train_index","rule_kind",
            "in_shape","out_shape","hist_in_hash","hist_out_hash","glyph_hash"
        ]
        _atomic_write_csv_rows(csv_index, headers, rows, schema="rule_cards.v1")

        # Provenance sidecar
        meta_path = csv_index + ".meta.json"
        tmpm = meta_path + ".tmp"
        with open(tmpm, "w", encoding="utf-8") as mf:
            json.dump(prov, mf, indent=2)
        os.replace(tmpm, meta_path)

        # Validate and optional directory packaging
        assert os.path.getsize(csv_index) > 0, "Empty CSV detected"
        comp_out = None
        if compress == "zip":
            comp_out = _zip_dir_csv(out_dir)

        # KEEL sidecar for index (Memory-only)
        keel_out = None
        if is_internal_keel:
            try:
                keel_out, _ = _maybe_keel(csv_index, kind="csv", is_internal=True)  # noqa: F821
            except Exception:
                keel_out = None

        # Holo snapshot post-write
        if holo_snap and "holo" in globals():
            try: holo.snapshot(f"csv_post_{os.path.basename(csv_index)}")  # noqa: F821
            except Exception: pass

        # exposure hook
        try:
            _ = prepare_for_download(csv_index)  # noqa: F821
        except Exception:
            pass

        try:
            meta_log("rule_cards.exported", path=out_dir, count=len(rows),
                     index=csv_index, compressed_dir=comp_out, keel_out=keel_out,
                     errors=len(errors) or 0)  # noqa: F821
        except Exception: pass

    except Exception as e:
        try: meta_log("rule_cards.export_fail", fn=csv_index, err=str(e))  # noqa: F821
        except Exception: pass
        if "logger" in globals():
            logger.exception(f"[export_rule_cards] write csv failed: {e}")

# ----------------------
#  append helper retained for compatibility; prefer atomic writers
# ----------------------
def _append_csv_chunk(path: str, chunk_rows: List[Dict[str, Any]]):
    try:
        with open(path, "a", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=[
                "idx","task_id","train_index","rule_kind",
                "in_shape","out_shape","hist_in_hash","hist_out_hash","glyph_hash"
            ])
            for r in chunk_rows:
                w.writerow(r)
    except Exception as e:
        if "logger" in globals():
            logger.exception(f"[_append_csv_chunk] failed: {e}")
        try: meta_log("rules.csv.write_fail", fn=path, err=str(e))  # noqa: F821
        except Exception: pass

# =========================================================
# Housekeeping & Exports (Holo maintenance + light exports)
# =========================================================
def periodic_housekeeping_and_exports(epoch_idx: int = 0):
    try:
        # -- maintenance (decay + prune + attractor drift)
        if "holo" in globals() and getattr(holo, "_maybe_decay_maintenance", None):
            holo._maybe_decay_maintenance()  # never throws by design

        # -- light exports every 3 epochs
        if (epoch_idx % 3) == 0:
            out_dir = os.path.join("exports", "holo")
            try: os.makedirs(out_dir, exist_ok=True)
            except Exception: pass

            # stats → CSV
            try:
                stats = holo.stats() if ("holo" in globals() and hasattr(holo, "stats")) else {}
                csv_path = os.path.join(out_dir, "holo_stats.csv")
                if stats:
                    with open(csv_path, "w", newline="", encoding="utf-8") as f:
                        w = csv.writer(f); w.writerow(["metric", "value"])
                        for k, v in stats.items():
                            if isinstance(v, dict):
                                for kk, vv in v.items():
                                    w.writerow([f"{k}.{kk}", vv])
                            else:
                                w.writerow([k, v])
                    meta_log("holo.export_stats_csv", path=csv_path)  # noqa: F821
                    # Optional KEEL sidecar for housekeeping CSVs (Memory-only)
                    try:
                        _maybe_keel(csv_path, kind="csv", is_internal=True)  # noqa: F821
                    except Exception:
                        pass
            except Exception as e:
                meta_log("holo.export_stats_csv_fail", error=str(e))  # noqa: F821

            # plots (only if helpers exist and matplotlib is around)
            try:
                if hasattr(holo, "plot_history"):
                    holo.plot_history(os.path.join(out_dir, "holo_history.png"))  # noqa: F821
            except Exception as e:
                meta_log("holo.plot_history_fail", error=str(e))  # noqa: F821
            try:
                if hasattr(holo, "plot_pca_map"):
                    holo.plot_pca_map(os.path.join(out_dir, "holo_pca.png"))  # noqa: F821
            except Exception as e:
                meta_log("holo.plot_pca_map_fail", error=str(e))  # noqa: F821

    except Exception as e:
        meta_log("holo.housekeeping_fail", error=str(e))  # noqa: F821
        raise

# ================================================================
# Rulebase + Human Rules + Explain Hooks (params-only)
# ================================================================
def _hash_rule_content(content: str) -> str:
    try:
        return hashlib.sha1(content.strip().lower().encode("utf-8")).hexdigest()[:12]
    except Exception:
        return hex(abs(hash(content.strip().lower())))[2:14]

def _canonicalize_rule_text(text: str) -> str:
    s = re.sub(r"\s+", " ", (text or "")).strip().lower()
    s = re.sub(r"\s*#.*$", "", s)  # strip trailing inline comment (simple)
    return s

def _parse_shape_token(tok: str):
    if not tok:
        return None
    s = str(tok).strip()
    m = re.match(r"^\(?\s*(\d+)\s*,\s*(\d+)\s*\)?$", s)
    return (int(m.group(1)), int(m.group(2))) if m else None

def _parse_human_rule(text: str) -> dict:
    canon = _canonicalize_rule_text(text)
    rid = _hash_rule_content(canon)
    return {
        "id": rid,
        "human_text": text,
        "rule_kind": "human",
        "conditions": [],
        "actions": [text.strip()],
        "meta": {
            "canonical": canon,
            "created_at": time.time(),
            "provenance": "manual",
        }
    }

def _parse_human_rule_rich(text: str, *, author: str = "anon", tags=None,
                           shape: "Optional[tuple[int,int]]" = None,
                           priority: int = 0, version: str = "v1") -> dict:
    base = _parse_human_rule(text)
    base["meta"].update({
        "author": author,
        "tags": list(tags or []),
        "priority": int(priority),
        "version": version,
        "shape": list(shape) if shape else None,
    })
    return base

def inject_human_rule(solver, text: str, csv_path: str = CSV_RULES_OUTPUT, **rich_meta):
    rule_dict = _parse_human_rule_rich(text, **rich_meta) if rich_meta else _parse_human_rule(text)
    rule_id = rule_dict["id"]

    try:
        # dedupe by id (canonicalized content) across records
        for rec in getattr(solver.rulebase, "records", []):
            rid_rec = getattr(rec, "id", None)
            if rid_rec is None:
                m = getattr(rec, "meta", None)
                if isinstance(m, dict):
                    rid_rec = m.get("id")
                elif isinstance(rec, dict):
                    rid_rec = rec.get("meta", {}).get("id")
            if rid_rec == rule_id:
                print(f"⚠️ Human rule already exists: {text}")
                meta_log("rules.human_exists", rule=text, id=rule_id)  # noqa: F821
                return rule_id

        if hasattr(solver.rulebase, "add_from_dict"):
            solver.rulebase.add_from_dict(rule_dict)
        else:
            # fallback append
            if not hasattr(solver.rulebase, "records"):
                setattr(solver.rulebase, "records", [])
            solver.rulebase.records.append({"meta": {"id": rule_id, "human_text": text}, "rule": {"kind": "human", "params": {}}})
        print(f"✅ Injected human rule: {text}")

        # persist (stable header)
        os.makedirs(os.path.dirname(csv_path) or ".", exist_ok=True)
        FIELDNAMES = ["id","human_text","rule_kind","conditions","actions","meta"]
        file_exists = os.path.isfile(csv_path)
        with open(csv_path, "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=FIELDNAMES)
            if not file_exists:
                writer.writeheader()
            row = dict(rule_dict)
            try:
                row["meta"] = json.dumps(row.get("meta", {}), ensure_ascii=False)
            except Exception:
                row["meta"] = "{}"
            writer.writerow({k: row.get(k) for k in FIELDNAMES})
        meta_log("rules.human_injected", rule=text, id=rule_id, path=csv_path)  # noqa: F821

        # fold to KB/Holo/Sandbox (guarded)
        try:
            if hasattr(solver, "kb") and solver.kb and hasattr(solver.kb, "add_fact"):
                solver.kb.add_fact("human_rule", {"id": rule_id, "text": text})
            if hasattr(solver, "holo") and solver.holo:
                solver.holo.add(None, None, {"subject": "human_rule", "id": rule_id})
            if hasattr(solver, "sandbox") and solver.sandbox and hasattr(solver.sandbox, "ingest_rule"):
                solver.sandbox.ingest_rule(rule_dict, origin="human")
            meta_log("rules.human_folded", id=rule_id)  # noqa: F821
        except Exception as e:
            meta_log("rules.human_fold_failed", id=rule_id, error=str(e))  # noqa: F821
        return rule_id
    except Exception as e:
        print(f"⚠️ Could not inject rule: {e}")
        meta_log("rules.human_inject_failed", rule=text, error=str(e))  # noqa: F821
        return None

def preload_human_rules_from_file(solver, txt_path: str = "human_rules.txt"):
    if not os.path.isfile(txt_path):
        print(f"⚠️ No human rules file at {txt_path}")
        meta_log("rules.human_preload_failed", path=txt_path, reason="not_found")  # noqa: F821
        return
    added = 0
    dups  = 0
    with open(txt_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            rid = inject_human_rule(solver, line)
            if rid:
                added += 1
            else:
                dups += 1
    print(f"✅ Loaded {added} human rules from {txt_path} (dups {dups})")
    meta_log("rules.human_preloaded", added=added, dups=dups, path=txt_path)  # noqa: F821

# ============================================================================
# DATA LOADER — Open-Ended Splits (no prompts)  •  aligns to header roots
# ============================================================================
ARC2025_ROOT_DEFAULT = globals().get("DATA_ROOT_2025") or "/kaggle/input/arc-prize-2025"
ARCDB_ROOT_DEFAULT   = globals().get("DATA_ROOT_ARCDB") or "/kaggle/input/arc-database/arc_data"

def _load_json(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def _canon_grid(g: Any) -> np.ndarray:
    arr = np.array(g, dtype=int)
    if arr.ndim == 2: return arr
    if arr.ndim == 1: return arr.reshape(1, -1)
    return arr.squeeze()

def _normalize_tasks(obj: Any) -> List[Dict[str, Any]]:
    out = []
    if isinstance(obj, dict):
        for tid, td in obj.items():
            if isinstance(td, dict):
                item = {"id": tid}
                item.update(td)
                out.append(item)
    elif isinstance(obj, list):
        for td in obj:
            if isinstance(td, dict) and "id" in td:
                out.append(td)
    return out

def _canon_task_io(task: Dict[str, Any]) -> Dict[str, Any]:
    t = dict(task)
    tr, te = [], []
    for p in (t.get("train") or []):
        if not isinstance(p, dict): continue
        ii = _canon_grid(p.get("input", p))
        if "output" in p: tr.append({"input": ii, "output": _canon_grid(p["output"])})
        else:             tr.append({"input": ii})
    for p in (t.get("test") or []):
        if not isinstance(p, dict): continue
        ii = _canon_grid(p.get("input", p))
        if "output" in p: te.append({"input": ii, "output": _canon_grid(p["output"])})
        else:             te.append({"input": ii})
    t["train"], t["test"] = tr, te
    return t

def _canon_tasks(seq: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    return [_canon_task_io(x) for x in (seq or [])]

def _split_into_k(seq: List[Any], k: int) -> List[List[Any]]:
    seq = list(seq or [])
    k = max(1, int(k))
    n = len(seq)
    base = n // k
    rem  = n - base * k
    out, i = [], 0
    for j in range(k):
        sz = base + (1 if j < rem else 0)
        out.append(seq[i:i+sz]); i += sz
    return out

def _load_arc_2025(root: str = ARC2025_ROOT_DEFAULT):
    f_train_ch = os.path.join(root, "arc-agi_training_challenges.json")
    f_train_sol= os.path.join(root, "arc-agi_training_solutions.json")
    f_eval_ch  = os.path.join(root, "arc-agi_evaluation_challenges.json")
    f_eval_sol = os.path.join(root, "arc-agi_evaluation_solutions.json")
    f_test_ch  = os.path.join(root, "arc-agi_test_challenges.json")  # optional

    if not os.path.isfile(f_train_ch): raise FileNotFoundError(f"[2025] Missing {f_train_ch}")
    if not os.path.isfile(f_eval_ch):  raise FileNotFoundError(f"[2025] Missing {f_eval_ch}")

    ch_train_raw = _load_json(f_train_ch)
    ch_eval_raw  = _load_json(f_eval_ch)
    ch_test_raw  = _load_json(f_test_ch) if os.path.isfile(f_test_ch) else []

    train_sol_raw = _load_json(f_train_sol) if os.path.isfile(f_train_sol) else {}
    eval_sol_raw  = _load_json(f_eval_sol)  if os.path.isfile(f_eval_sol)  else {}

    train_ch = _canon_tasks(_normalize_tasks(ch_train_raw))
    eval_ch  = _canon_tasks(_normalize_tasks(ch_eval_raw))
    test_ch  = _canon_tasks(_normalize_tasks(ch_test_raw))

    def _solutions_lookup(sol_json: Any) -> Dict[Tuple[str,int], np.ndarray]:
        lu: Dict[Tuple[str,int], np.ndarray] = {}
        if isinstance(sol_json, dict):
            for tid, val in sol_json.items():
                if not isinstance(val, dict): continue
                seq = val.get("solutions")
                if isinstance(seq, list):
                    for k, out_grid in enumerate(seq):
                        try: lu[(tid, k)] = _canon_grid(out_grid)
                        except Exception: pass
                for section in ("train", "test"):
                    for p in (val.get(section, []) or []):
                        if isinstance(p, dict) and "output" in p:
                            k = len([kk for (t,kk) in lu.keys() if t == tid])
                            try: lu[(tid, k)] = _canon_grid(p["output"])
                            except Exception: pass
        elif isinstance(sol_json, list):
            for item in sol_json:
                if not isinstance(item, dict): continue
                tid = item.get("id")
                seq = item.get("solutions")
                if isinstance(seq, list):
                    for k, out_grid in enumerate(seq):
                        try: lu[(tid, k)] = _canon_grid(out_grid)
                        except Exception: pass
        return lu

    return dict(
        train_ch=train_ch,
        eval_ch=eval_ch,
        test_ch=test_ch,
        train_lookup=_solutions_lookup(train_sol_raw) if train_sol_raw else {},
        eval_lookup=_solutions_lookup(eval_sol_raw) if eval_sol_raw else {},
    )

def _ids_in(dir_path: str) -> List[str]:
    if not os.path.isdir(dir_path): return []
    ids = [os.path.splitext(fn)[0] for fn in os.listdir(dir_path) if fn.endswith(".json")]
    ids.sort()
    return ids

def _pair_ids(q_dir: str, a_dir: str) -> List[str]:
    return sorted(set(_ids_in(q_dir)) & set(_ids_in(a_dir)))

def _root_section(obj: Any) -> Dict[str, Any]:
    if isinstance(obj, dict) and "root" in obj and isinstance(obj["root"], dict):
        return obj["root"]
    return obj if isinstance(obj, dict) else {}

def _load_question_file(q_path: str, tid: str) -> Dict[str, Any]:
    obj = _load_json(q_path)
    root = _root_section(obj)
    task = {"id": tid, "train": [], "test": []}
    for p in (root.get("train") or []):
        if not isinstance(p, dict): continue
        ii = _canon_grid(p.get("input", p))
        if "output" in p: task["train"].append({"input": ii, "output": _canon_grid(p["output"])})
        else:             task["train"].append({"input": ii})
    for p in (root.get("test") or []):
        if not isinstance(p, dict): continue
        ii = _canon_grid(p.get("input", p))
        if "output" in p: task["test"].append({"input": ii, "output": _canon_grid(p["output"])})
        else:             task["test"].append({"input": ii})
    return _canon_task_io(task)

def _answers_lookup(a_obj: Any, tid: str) -> Dict[Tuple[str,int], np.ndarray]:
    lu: Dict[Tuple[str,int], np.ndarray] = {}
    if isinstance(a_obj, dict) and isinstance(a_obj.get("solutions"), list):
        for k, grid in enumerate(a_obj["solutions"]):
            try: lu[(tid, k)] = _canon_grid(grid)
            except Exception: pass
        return lu
    root = _root_section(a_obj)
    k = 0
    for p in (root.get("test") or []):
        if isinstance(p, dict) and "output" in p:
            try:
                lu[(tid, k)] = _canon_grid(p["output"])
                k += 1
            except Exception:
                pass
    return lu

def _load_arcdb(root_arcdb: str = ARCDB_ROOT_DEFAULT):
    TR_Q = os.path.join(root_arcdb, "training", "questions")
    TR_A = os.path.join(root_arcdb, "training", "answers")
    EV_Q = os.path.join(root_arcdb, "evaluation", "questions")
    EV_A = os.path.join(root_arcdb, "evaluation", "answers")

    tr_ids = _pair_ids(TR_Q, TR_A)
    ev_ids = _pair_ids(EV_Q, EV_A)

    train_tasks, eval_tasks = [], []
    train_lookup: Dict[Tuple[str,int], np.ndarray] = {}
    eval_lookup:  Dict[Tuple[str,int], np.ndarray] = {}

    for tid in tr_ids:
        try:
            task = _load_question_file(os.path.join(TR_Q, tid + ".json"), tid)
            train_tasks.append(task)
        except Exception:
            continue
        try:
            a_obj = _load_json(os.path.join(TR_A, tid + ".json"))
            train_lookup.update(_answers_lookup(a_obj, tid))
        except Exception:
            pass

    for tid in ev_ids:
        try:
            task = _load_question_file(os.path.join(EV_Q, tid + ".json"), tid)
            eval_tasks.append(task)
        except Exception:
            continue
        try:
            a_obj = _load_json(os.path.join(EV_A, tid + ".json"))
            eval_lookup.update(_answers_lookup(a_obj, tid))
        except Exception:
            pass

    return dict(
        train_ch=train_tasks,
        eval_ch=eval_tasks,
        train_lookup=train_lookup,
        eval_lookup=eval_lookup
    )

def build_splits_open(
    root_2025: str = ARC2025_ROOT_DEFAULT,
    root_arcdb: str = ARCDB_ROOT_DEFAULT,
    *,
    shuffle_seed: Optional[int] = None  # optional stable shuffle
) -> SimpleNamespace:

    y2025 = _load_arc_2025(root_2025)
    adb   = _load_arcdb(root_arcdb)

    def maybe_shuffle(seq: List[Any]) -> List[Any]:
        if shuffle_seed is None: return seq
        rng = random.Random(shuffle_seed)
        seq = list(seq); rng.shuffle(seq); return seq

    # ARC 2025 training -> 4 sessions (auto-balanced)
    tr25_all = maybe_shuffle(y2025.get("train_ch", []))
    train_2025_sessions = _split_into_k(tr25_all, 4)

    # ARC 2025 evaluation -> full set (no split)
    eval_2025_full = y2025.get("eval_ch", [])

    # ARC 2025 tests from training -> split in half (auto-balanced)
    test_2025_from_train_pre1, test_2025_from_train_pre2 = _split_into_k(tr25_all, 2)

    # ARC 2025 submission test -> full set
    submission_test_2025_all = y2025.get("test_ch", [])

    # ARC-DB evaluation -> split in half (auto-balanced)
    ev_arc_all = maybe_shuffle(adb.get("eval_ch", []))
    test_arcdb_eval_pre1, test_arcdb_eval_pre2 = _split_into_k(ev_arc_all, 2)

    return SimpleNamespace(
        # 2025 training sessions (4-way)
        train_2025_s1=train_2025_sessions[0] if len(train_2025_sessions) > 0 else [],
        train_2025_s2=train_2025_sessions[1] if len(train_2025_sessions) > 1 else [],
        train_2025_s3=train_2025_sessions[2] if len(train_2025_sessions) > 2 else [],
        train_2025_s4=train_2025_sessions[3] if len(train_2025_sessions) > 3 else [],

        # 2025 eval + tests
        eval_2025_full=eval_2025_full,
        test_2025_from_train_pre1=test_2025_from_train_pre1,
        test_2025_from_train_pre2=test_2025_from_train_pre2,
        submission_test_2025_all=submission_test_2025_all,

        # ARC-DB eval tests (2 halves)
        test_arcdb_eval_pre1=test_arcdb_eval_pre1,
        test_arcdb_eval_pre2=test_arcdb_eval_pre2,

        # Lookups
        eval_2025_lookup=y2025.get("eval_lookup", {}),
        train_2025_lookup=y2025.get("train_lookup", {}),
        arcdb_train_lookup=adb.get("train_lookup", {}),
        arcdb_eval_lookup=adb.get("eval_lookup", {}),

        # Convenience
        arcdb_train_all=adb.get("train_ch", []),
        arcdb_eval_all=ev_arc_all,
    )

def describe_splits_open(splits: SimpleNamespace):
    def n(x): return len(x) if isinstance(x, list) else (0 if x is None else -1)
    print("[SPLITS: OPEN-ENDED]")
    print(f"2025 train S1..S4: {n(splits.train_2025_s1)}, {n(splits.train_2025_s2)}, {n(splits.train_2025_s3)}, {n(splits.train_2025_s4)}")
    print(f"2025 eval full   : {n(splits.eval_2025_full)}")
    print(f"2025 test from train pre1/pre2: {n(splits.test_2025_from_train_pre1)}, {n(splits.test_2025_from_train_pre2)}")
    print(f"2025 submission  : {n(splits.submission_test_2025_all)}")
    print(f"ARC-DB eval tests pre1/pre2   : {n(splits.test_arcdb_eval_pre1)}, {n(splits.test_arcdb_eval_pre2)}")
    print(f"Lookups -> 2025(eval/train): {len(splits.eval_2025_lookup)}, {len(splits.train_2025_lookup)} | ARC-DB(train/eval): {len(splits.arcdb_train_lookup)}, {len(splits.arcdb_eval_lookup)}")

# ============================================================================
# Bootstrap Guard
# ============================================================================
def ensure_trainer_ready(solver):
    if not hasattr(solver, "trainer") or solver.trainer is None:
        attach_unified_trainer(solver)
        if "logger" in globals():
            logger.info("[BOOTSTRAP] Trainer attached automatically")
        try:
            meta_log("bootstrap.attach_trainer", ok=True)  # noqa: F821
        except Exception:
            pass


# ==========================================================
# Capability Registrar — make orchestrator.capabilities all True
# ==========================================================
def _g(name): return globals().get(name)

def _log_safe(topic: str, **payload):
    try: logger.info(f"[{topic}] {payload}")  # noqa: F821
    except Exception: print(f"[{topic}] {payload}")

# --- define only if missing ---
if not _g("load_json_safe"):
    def load_json_safe(path: str):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

if not _g("_loop_learn_misses_json"):
    def _loop_learn_misses_json(solver, json_path: str, passes: int = 1):
        _log_safe("loop_learn_misses_json.stub", path=json_path, passes=passes)

if not _g("_snapshot_holomemory"):
    def _snapshot_holomemory(holo, out_path: str):
        data = {"ok": False}
        try:
            data = {"ok": True, "stats": (holo.stats() if (holo and hasattr(holo, "stats")) else {})}
        except Exception:
            pass
        try:
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(data, f)
        except Exception:
            pass
        _log_safe("snapshot.holo", path=out_path, ok=data.get("ok", False))

if not _g("_export_holomemory_pca"):
    def _export_holomemory_pca(holo, out_csv: str):
        rows = []
        try:
            if holo and hasattr(holo, "items"):
                for i, it in enumerate(holo.items() or []):
                    rows.append({"id": getattr(it, "id", i), "x": float(i), "y": float(-i)})
        except Exception:
            pass
        _ensure_dir(os.path.dirname(out_csv) or ".")
        with open(out_csv, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["id","x","y"])
            w.writeheader(); w.writerows(rows)
        _log_safe("export.holo_pca", path=out_csv, n=len(rows))

if not _g("_export_resonance_map"):
    def _export_resonance_map(solver, prefix: str = "res"):
        fp = f"exports/resonance/{prefix}_resonance.json"
        _ensure_dir(os.path.dirname(fp))
        with open(fp, "w", encoding="utf-8") as f:
            json.dump({"conf_ema": float(getattr(solver, "_conf_ema", 0.75)), "t": time.time()}, f)
        _log_safe("export.resonance", path=fp)

if not _g("ensure_arc_dataset_ready"):
    def ensure_arc_dataset_ready(root_2025=None, root_arcdb=None) -> bool:
        # Prefer SSOT roots if present
        r25 = root_2025 or globals().get("DATA_ROOT_2025") or globals().get("ARC2025_ROOT_DEFAULT") or "/kaggle/input/arc-prize-2025"
        adb = root_arcdb or globals().get("DATA_ROOT_ARCDB") or globals().get("ARCDb_ROOT_DEFAULT") or "/kaggle/input/arc-database/arc_data"

        req25 = [
            "arc-agi_training_challenges.json","arc-agi_training_solutions.json",
            "arc-agi_evaluation_challenges.json","arc-agi_evaluation_solutions.json"
        ]
        ok_2025 = all(os.path.isfile(os.path.join(r25, fn)) for fn in req25)

        # ARC-DB (optional, if root exists)
        ok_arcdb = False
        try:
            tr_q = os.path.join(adb, "training", "questions")
            tr_a = os.path.join(adb, "training", "answers")
            ev_q = os.path.join(adb, "evaluation", "questions")
            ev_a = os.path.join(adb, "evaluation", "answers")
            ok_arcdb = all(os.path.isdir(p) for p in (tr_q, tr_a, ev_q, ev_a))
        except Exception:
            ok_arcdb = False

        _log_safe("ensure_arc_dataset_ready", ok_2025=ok_2025, ok_arcdb=ok_arcdb, root_2025=r25, root_arcdb=adb)
        return bool(ok_2025)

if not _g("sandbox_guided_warmup"):
    def sandbox_guided_warmup(solver, tasks, subset_n: int = 64):
        sub = list(tasks or [])[:max(0, int(subset_n))]
        for t in sub:
            try:
                if hasattr(solver, "solve_task"):
                    solver.solve_task(t)
            except Exception:
                pass
        _log_safe("sandbox_guided_warmup", subset=len(sub))

if not _g("_compress_file_inplace_or_copy"):
    def _compress_file_inplace_or_copy(path: str, method: str = "zip", delete_original: bool = False) -> bool:
        """
        Policy: disallow creation of new gzip artifacts.
        - Only 'zip' is permitted here (primarily for directories); other methods are refused.
        - This helper does NOT create Keel; Memory-only components manage Keel separately.
        """
        if not os.path.exists(path):
            return False
        if method != "zip":
            _log_safe("compress.forbidden", src=path, method=method)  # policy alignment
            return False
        try:
            # If it's a directory, zip its contents; if it's a file, zip the single file.
            outp = path.rstrip("/\\") + ".zip"
            tmp = outp + ".tmp"
            with zipfile.ZipFile(tmp, "w", compression=zipfile.ZIP_DEFLATED) as z:
                if os.path.isdir(path):
                    for root, _, files in os.walk(path):
                        for fn in files:
                            fp = os.path.join(root, fn)
                            arc = os.path.relpath(fp, start=path)
                            z.write(fp, arc)
                else:
                    z.write(path, arcname=os.path.basename(path))
            os.replace(tmp, outp)
            if delete_original:
                if os.path.isdir(path):
                    shutil.rmtree(path, ignore_errors=True)
                else:
                    os.remove(path)
            _log_safe("compress.done", src=path, method="zip", out=outp)
            return True
        except Exception as e:
            _log_safe("compress.error", src=path, err=str(e))
            return False

if not _g("_checkpoint_save"):
    def _checkpoint_save(tag: str = "default"):
        _ensure_dir("checkpoints")
        tmp = f"checkpoints/{tag}.json.tmp"
        out = f"checkpoints/{tag}.json"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump({"tag": tag, "ts": time.time()}, f)
        os.replace(tmp, out)
        _log_safe("checkpoint.save", tag=tag)

if not _g("_checkpoint_restore"):
    def _checkpoint_restore(tag: str = "default") -> bool:
        fp = f"checkpoints/{tag}.json"
        ok = os.path.isfile(fp)
        _log_safe("checkpoint.restore", tag=tag, ok=ok)
        return ok

if not _g("_get_test_inputs"):
    def _get_test_inputs(task):
        try:
            return [p.get("input") for p in (task.get("test") or [])]
        except Exception:
            return []

if not _g("_clamp_to_palette"):
    def _clamp_to_palette(grid, palette=None):
        return grid

if not _g("_palette_of"):
    def _palette_of(grid) -> list:
        try:
            return sorted(set(map(int, np.unique(np.array(grid, dtype=int)))))
        except Exception:
            return []

# Optional: emit a snapshot so your logs show all True now
def _emit_capabilities_snapshot():
    helpers = [
        "_canon_tasks","_loop_learn_misses_json","_snapshot_holomemory","_export_holomemory_pca",
        "_export_resonance_map","preload_rules_from_csv","ensure_arc_dataset_ready",
        "sandbox_guided_warmup","validate_and_repair_submission_file",
        "_compress_file_inplace_or_copy","_checkpoint_save","_checkpoint_restore",
        "_get_test_inputs","apply_ops","_clamp_to_palette","_palette_of","load_json_safe"
    ]
    caps = {name: bool(_g(name)) for name in helpers}
    _log_safe("capabilities.after_registrar", **caps)

# --- optional glyph gate shim (keeps if you already have the real one) ---
glyph_constrained_candidates = globals().get("glyph_constrained_candidates", None)
if not callable(glyph_constrained_candidates):
    def glyph_constrained_candidates(cands, ref_glyph, max_keep=None):
        return cands

# ===========================================================
# GlobalRulebase  (params-only; schema v2.0)
# ===========================================================
class GlobalRulebase:
    def __init__(
        self,
        kb: Optional['SymbolicKB'] = None,
        csv_path: str = "training-rules/training_rules.csv",
        human_rules_path: str = "human_rules.txt",
        meta: Optional[Any] = None,
        ultra: Optional[Any] = None,
        kairos: Optional[Any] = None,
        holo: Optional[Any] = None,
        encoder: Optional[Any] = None,
        sandbox: Optional[Any] = None,
        solver: Optional[Any] = None,
    ):
        # collaborators / context
        self.meta = meta
        self.ultra = ultra
        self.kairos = kairos
        self.holo = holo
        self.encoder = encoder
        self.sandbox = sandbox
        self.solver = solver

        # KB link (bi-directional; idempotent)
        self.kb = kb
        try:
            if self.kb is None and 'SymbolicKB' in globals():
                self.kb = SymbolicKB()  # type: ignore[name-defined]
        except Exception:
            self.kb = None
        if self.kb is not None:
            try:
                if getattr(self.kb, "rulebase", None) is None:
                    self.kb.rulebase = self
            except Exception:
                try:
                    self.kb.rulebase = self
                except Exception:
                    pass

        # storage/config
        self.csv_path = csv_path
        self.human_rules_path = human_rules_path
        self.records: List['RuleRecord'] = []  # type: ignore[name-defined]
        self.narrations: List[str] = []
        self.schema_version = "2.0"  # params-only CSV

        # --- single-preload guard & churn tracking (per run/phase) ---
        self._preload_done_keys: set = set()
        self._human_preload_done_keys: set = set()
        self._last_kind_counts: Counter = Counter()
        self._last_total: int = 0

        self._emit("rulebase.init", status="ok", csv=self.csv_path, schema=self.schema_version)

    # ---------- late wiring (single authoritative method) ----------
    def set_context(
        self,
        *,
        meta=None,
        ultra=None,
        kairos=None,
        holo=None,
        encoder=None,
        sandbox=None,
        solver=None,
        kb=None,
    ):
        if meta is not None:
            self.meta = meta
        if ultra is not None:
            self.ultra = ultra
        if kairos is not None:
            self.kairos = kairos
        if holo is not None:
            self.holo = holo
        if encoder is not None:
            self.encoder = encoder
        if sandbox is not None:
            self.sandbox = sandbox
        if solver is not None:
            self.solver = solver
        if kb is not None:
            self.kb = kb
            try:
                if getattr(self.kb, "rulebase", None) is None:
                    self.kb.rulebase = self
            except Exception:
                pass
        return self

    # ---------- shared pulse / telemetry ----------
    def _pulse(self, topic: str, **payload):
        try:
            _ml = globals().get("_meta_log")
            if callable(_ml):
                _ml(topic, **payload)
            elif 'meta_log' in globals():
                meta_log(topic, **payload)  # type: ignore[name-defined]
        except Exception:
            pass
        try:
            if self.meta is not None and hasattr(self.meta, "_log_telemetry"):
                self.meta._log_telemetry({"module": "GlobalRulebase", "event": topic, **payload})
        except Exception:
            pass
        try:
            if self.ultra is not None and hasattr(self.ultra, "observe"):
                self.ultra.observe("rulebase_" + topic.replace(".", "_"), **payload)
        except Exception:
            pass

    def _tick_kairos(self, hint: float = 0.25):
        try:
            if self.kairos is None:
                return
            t = getattr(self.kairos, "phase_time", 0)
            self.kairos.step(t + 1)
            st = self.kairos.get_state()
            if st:
                self._pulse("kairos.tick", **st, hint=float(hint))
        except Exception:
            pass

    def _holo_echo(self, tag: str, payload: dict):
        try:
            if self.holo is None or not hasattr(self.holo, "add") or np is None:
                return
            _z = np.zeros((1, 1), dtype=int)
            self.holo.add(_z, _z, {"subject": "rulebase", "note": tag, **payload})
        except Exception:
            pass

    # ===========================================================
    # Internal telemetry helper
    # ===========================================================
    def _emit(self, topic: str, **payload):
        rec = {"module": "GlobalRulebase", "event": topic, "time": time.time(), **payload}
        try:
            _ml = globals().get("_meta_log")
            if callable(_ml):
                _ml(topic, **rec)
            elif 'meta_log' in globals():
                meta_log(topic, **rec)  # type: ignore[name-defined]
        except Exception:
            pass
        try:
            if self.kb is not None and hasattr(self.kb, "push_meta_stats"):
                self.kb.push_meta_stats(rec)
            elif self.kb is not None and hasattr(self.kb, "narrations"):
                self.kb.narrations.append(f"[Rulebase] {topic}: {payload}")
        except Exception:
            pass
        self._pulse(topic, **rec)
        self._holo_echo(topic, {"payload": payload})
        self._tick_kairos(hint=0.5 if topic.endswith(".init") else 0.1)

    # ---------- helpers ----------
    def _run_phase_key(self) -> str:
        try:
            run_id = getattr(self.meta, "run_id", None)
            phase = getattr(self.meta, "phase", None)
        except Exception:
            run_id, phase = None, None
        if run_id is None:
            try:
                run_id = os.getenv("RUN_ID", "na")
            except Exception:
                run_id = "na"
        if phase is None:
            try:
                phase = os.getenv("PHASE", "na")
            except Exception:
                phase = "na"
        return f"{run_id}:{phase}:{self.csv_path}"

    @staticmethod
    def _extract_kind_params_meta_from_rec(rec: Any):
        try:
            if hasattr(rec, "rule"):
                kind = getattr(rec.rule, "kind", "unknown")
                params = getattr(rec.rule, "params", {})
                meta = getattr(rec, "meta", {})
                return kind, (params if isinstance(params, dict) else {}), (meta if isinstance(meta, dict) else {})
            if isinstance(rec, dict):
                r = rec.get("rule", {})
                kind = r.get("kind", rec.get("rule_kind", "unknown"))
                params = r.get("params", {})
                meta = rec.get("meta", {})
                return kind, (params if isinstance(params, dict) else {}), (meta if isinstance(meta, dict) else {})
        except Exception:
            pass
        return "unknown", {}, {}

    def _summary_counts(self) -> Counter:
        try:
            kinds = [self._extract_kind_params_meta_from_rec(r)[0] for r in self.records]
            return Counter(kinds)
        except Exception:
            return Counter()

    def _emit_summary_snapshot(self, reason: str = "manual"):
        counts = self._summary_counts()
        total = int(sum(counts.values()))
        try:
            delta_total = int(total - self._last_total)
        except Exception:
            delta_total = 0
        try:
            kind_delta = {k: int(counts.get(k, 0) - self._last_kind_counts.get(k, 0)) for k in set(counts) | set(self._last_kind_counts)}
        except Exception:
            kind_delta = {}
        self._emit("rulebase.summary", reason=reason, total=total, kinds=dict(counts),
                   delta_total=delta_total, delta_by_kind=kind_delta)
        try:
            os.makedirs("deployment", exist_ok=True)
            path = os.path.join("deployment", "rulebase_counts.csv")
            write_header = not os.path.isfile(path)
            run_id = getattr(self.meta, "run_id", os.getenv("RUN_ID", "na"))
            phase = getattr(self.meta, "phase", os.getenv("PHASE", "na"))
            ts = time.time()
            with open(path, "a", newline="", encoding="utf-8") as f:
                w = csv.writer(f)
                if write_header:
                    w.writerow(["ts", "run_id", "phase", "kind", "count", "reason"])
                for k, v in counts.items():
                    w.writerow([f"{ts:.3f}", run_id, phase, k, int(v), reason])
        except Exception:
            pass
        self._last_kind_counts = counts.copy()
        self._last_total = total

    # ===========================================================
    # Core API
    # ===========================================================
    def add(self, rec: Any):
        # If a dict sneaks in, accept but telemetry it
        if not hasattr(rec, "rule") and not (isinstance(rec, dict) and "rule" in rec):
            self._emit("rulebase.add_untyped", detail=str(type(rec)))
        self.records.append(rec)
        # KB assist
        try:
            kind, params, _meta = self._extract_kind_params_meta_from_rec(rec)
            if self.kb is not None and hasattr(self.kb, "add_fact"):
                self.kb.add_fact("rule", kind)
        except Exception:
            kind = "unknown"
        # Encoder feedback (optional)
        try:
            if self.encoder is not None and hasattr(self.encoder, "record_feedback"):
                self.encoder.record_feedback(label=f"rule_{kind}", memory_layer="rules", success=True)  # type: ignore[name-defined]
        except Exception:
            pass
        # Telemetry (hash large params)
        try:
            pr_params = params
            if isinstance(pr_params, dict):
                try:
                    js = json.dumps(pr_params, sort_keys=True)
                    if len(js) > 512:
                        pr_params = {"_hash": hash(js)}
                except Exception:
                    pr_params = {"_hash": "unserializable"}
            self._emit("rulebase.add", rule=kind, params=pr_params, total=len(self.records))
        except Exception:
            self._emit("rulebase.add_emit_error")

    def __len__(self):
        return len(self.records)

    def __iter__(self):
        return iter(self.records)

    def find(self, rule_kind: str) -> List[Any]:
        out: List[Any] = []
        for r in self.records:
            try:
                k = getattr(getattr(r, "rule", None), "kind", None)
                if k is None and isinstance(r, dict):
                    k = r.get("rule", {}).get("kind")
                if k == rule_kind:
                    out.append(r)
            except Exception:
                continue
        return out

    # ===========================================================
    # CSV / Human Rule I/O (delegates to unified loaders)
    # ===========================================================
    def preload_from_csv(self):
        key = ("csv", self._run_phase_key())
        if key in self._preload_done_keys:
            self._emit("rulebase.preload_csv_skipped", reason="already_preloaded_this_run_phase", path=self.csv_path)
            return 0
        try:
            loader = globals().get("preload_rules_from_csv")
            if callable(loader):
                host = self.solver if getattr(self, "solver", None) is not None else self
                n = int(loader(host, self.csv_path))
                self._emit("rulebase.preload_csv", added=n, path=self.csv_path)
                self._preload_done_keys.add(key)
                self._emit_summary_snapshot(reason="preload_csv")
                return n
            else:
                self._emit("rulebase.preload_csv_unavailable", path=self.csv_path)
                return 0
        except Exception as e:
            self._emit("rulebase.preload_csv_error", error=str(e), path=self.csv_path)
            return 0

    def save_to_csv(self):
        if not getattr(self, "records", None):
            self._emit("rulebase.save_empty")
            return
        field_fixed = ["schema_version", "rule_kind", "params", "confidence"]
        extras: List[str] = []
        # discover extra meta keys
        for rec in self.records:
            try:
                _, _, meta = self._extract_kind_params_meta_from_rec(rec)
                for k in (meta or {}).keys():
                    if k not in field_fixed and k not in extras:
                        extras.append(k)
            except Exception:
                continue
        fieldnames = field_fixed + sorted(extras)
        try:
            os.makedirs(os.path.dirname(self.csv_path) or ".", exist_ok=True)
        except Exception:
            pass
        # atomic write
        tmp = self.csv_path + ".tmp"
        try:
            with open(tmp, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for rec in self.records:
                    kind, params, meta = self._extract_kind_params_meta_from_rec(rec)
                    row = dict(meta or {})
                    row["schema_version"] = self.schema_version
                    row["rule_kind"] = kind
                    try:
                        row["params"] = json.dumps(params, ensure_ascii=False)
                    except Exception:
                        row["params"] = "{}"
                    try:
                        row["confidence"] = float((meta or {}).get("confidence", 0.0))
                    except Exception:
                        row["confidence"] = 0.0
                    writer.writerow(row)
            os.replace(tmp, self.csv_path)
            kinds = Counter([self._extract_kind_params_meta_from_rec(r)[0] for r in self.records])
            self._emit("rulebase.save_csv", count=len(self.records), kinds=dict(kinds),
                       path=self.csv_path, schema=self.schema_version)
            self._emit_summary_snapshot(reason="save_csv")
        except Exception as e:
            try:
                if os.path.exists(tmp):
                    os.remove(tmp)
            except Exception:
                pass
            self._emit("rulebase.save_error", error=str(e), path=self.csv_path)

    def preload_human_rules(self):
        key = ("human", self._run_phase_key(), self.human_rules_path)
        if key in self._human_preload_done_keys:
            self._emit("rulebase.human_rules_skipped", reason="already_preloaded_this_run_phase", path=self.human_rules_path)
            return
        try:
            loader = globals().get("preload_human_rules_from_file")
            if callable(loader):
                host = self.solver if getattr(self, "solver", None) is not None else self
                loader(host, self.human_rules_path)
                self._human_preload_done_keys.add(key)
                self._emit("rulebase.human_rules_loaded", path=self.human_rules_path)
                self._emit_summary_snapshot(reason="preload_human_rules")
            else:
                self._emit("rulebase.human_rules_unavailable", path=self.human_rules_path)
        except Exception as e:
            self._emit("rulebase.human_rules_error", error=str(e), path=self.human_rules_path)

    def add_from_dict(self, d: Dict[str, Any]):
        # 1) Normalize input into (kind, params_dict, meta_dict) — params-only
        try:
            rule_obj = d.get("rule") if isinstance(d.get("rule"), dict) else None
            kind = (rule_obj.get("kind") if rule_obj else None) or d.get("rule_kind") or d.get("kind") or "unknown"
            params = (rule_obj.get("params") if rule_obj else (d.get("params") or {}))
            if "payload" in d or (rule_obj and "payload" in rule_obj):
                self._emit("rulebase.payload_ignored", kind=kind)
            if not isinstance(params, dict):
                # Coerce and log drop
                self._emit("rulebase.params_coerced", kind=kind, dropped=True)
                params = {}
            meta = {k: v for k, v in d.items() if k not in {"rule", "rule_kind", "kind", "params", "payload"}}
        except Exception as e:
            self._emit("rulebase.add_from_dict_normalize_error", error=str(e))
            return

        # 2) Construct Rule with strict params-only API
        R = globals().get("Rule")
        RR = globals().get("RuleRecord")
        rule_inst = None
        try:
            if callable(getattr(R, "from_any", None)):
                rule_inst = R.from_any(kind, params=params)
            if rule_inst is None and R is not None:
                rule_inst = R(kind=kind, params=params)
        except Exception:
            rule_inst = None

        # 3) Append as RuleRecord or normalized dict
        if rule_inst is None:
            self._emit("rulebase.rule_construct_failed", kind=kind)
            self.records.append({"input_grid": None, "output_grid": None,
                                 "rule": {"kind": kind, "params": params}, "meta": meta})
            return

        try:
            if RR is not None:
                rec = RR(None, None, rule_inst, meta)
                self.add(rec)
            else:
                self.records.append({"input_grid": None, "output_grid": None,
                                     "rule": {"kind": kind, "params": params}, "meta": meta})
                self._emit("rulebase.rulerecord_missing_fallback", kind=kind)
        except Exception as e:
            self._emit("rulebase.add_from_dict_add_error", error=str(e), kind=kind)

# Compatibility alias (if other parts refer to Rulebase)
Rulebase = GlobalRulebase

# ==========================================================
# SymbolicKB — Rewritten (Thread-safe, Hybrid-aware, Unified 'params' schema)
# ==========================================================

# ----------------------------------------------------------
# Utilities
# ----------------------------------------------------------
class _LRU(OrderedDict):
    def __init__(self, cap: int):
        super().__init__(); self._cap = int(max(8, cap))
    def get(self, key, default=None):
        if key in self:
            val = super().pop(key); super().__setitem__(key, val); return val
        return default
    def put(self, key, val):
        if key in self: super().pop(key)
        super().__setitem__(key, val)
        while len(self) > self._cap:
            self.popitem(last=False)
    def delete(self, key):
        try: super().pop(key)
        except Exception: pass

_DEF_HASH_SEED = b"kb_v2"

def _crc32_bytes(b: bytes) -> int:
    try:
        import zlib
        return zlib.crc32(b)
    except Exception:
        return int(hashlib.sha1(b).hexdigest()[:8], 16)

def hash_grid(arr: "np.ndarray") -> str:
    if np is None:
        return hashlib.sha1(str(arr).encode()).hexdigest()[:16]
    a = np.asarray(arr, dtype=int)
    buf = a.tobytes() + _DEF_HASH_SEED
    return f"{_crc32_bytes(buf):08x}:{a.shape}"

# Fallback glyph if invariants missing: shape + coarse hist + centroid
def _fallback_glyph_id(a: "np.ndarray") -> str:
    if np is None:
        return hashlib.sha1(str(a).encode()).hexdigest()[:12]
    x = np.asarray(a, dtype=int)
    R, C = x.shape if x.ndim == 2 else (1, x.size)
    # Coarse histogram on minified view
    try:
        xs = x if x.ndim == 2 else x.reshape((R, C))
        # downsample to at most 12x12 for hashing
        rstep = max(1, R // 12); cstep = max(1, C // 12)
        mini = xs[::rstep, ::cstep]
        vals, cnts = np.unique(mini, return_counts=True)
        # rough centroid (ignore -1 if present)
        mask = (mini != -1)
        if mask.any():
            rr, cc = np.where(mask)
            cent_r = float(np.mean(rr)); cent_c = float(np.mean(cc))
        else:
            cent_r = cent_c = 0.0
        sig = {
            "shape": (int(R), int(C)),
            "hist": dict(zip([int(v) for v in vals.tolist()], [int(c) for c in cnts.tolist()])),
            "cent": (round(cent_r, 1), round(cent_c, 1)),
        }
        h = hashlib.sha1(json.dumps(sig, sort_keys=True).encode()).hexdigest()[:16]
        return h
    except Exception:
        return hashlib.sha1(np.asarray(x, dtype=int).tobytes()).hexdigest()[:16]


# ==========================================================
# SYMBOLIC ULTRA (Policy-Compliant, Params-Only Telemetry, No-Gzip)
# ==========================================================
try:
    _SYMBOLIC_ULTRA_INSTALLED
except NameError:
    _SYMBOLIC_ULTRA_INSTALLED = True

    # ---- local, safe emitter (use shared _ModuleEmitter if available) -------
    _GlobalModuleEmitter = globals().get("_ModuleEmitter", None)
    class _UltraEmitter:
        def __init__(self, module_name: str, meta=None):
            if callable(_GlobalModuleEmitter):
                self._impl = _GlobalModuleEmitter(meta=meta, module_name=module_name)
            else:
                self._impl = None
            self.module_name = module_name

        def emit(self, topic: str, **payload):
            try:
                if self._impl is not None and hasattr(self._impl, "emit"):
                    self._impl.emit(topic, **payload)
                else:
                    logger = globals().get("logger", None)
                    if logger is not None:
                        logger.info(f"[{self.module_name}:{topic}] {payload}")
                    else:
                        print(f"[{self.module_name}:{topic}] {payload}")
            except Exception:
                pass

    # ---- utilities ----------------------------------------------------------
    def _ultra_sanitize_meta(d: dict, max_str: int = 256) -> dict:
        out = {}
        for k, v in (d or {}).items():
            try:
                if isinstance(v, (int, float, bool)) or v is None:
                    out[k] = v
                elif isinstance(v, (str, bytes)):
                    s = v.decode("utf-8", "ignore") if isinstance(v, bytes) else str(v)
                    # keep fact-string safe and compact
                    s = s.replace("|", "·").replace("=", ":")
                    out[k] = s if len(s) <= max_str else (s[:max_str] + "...")
                else:
                    # store small repr + hash to keep fact string light
                    s = str(type(v).__name__)
                    try:
                        h = hashlib.sha1(repr(v).encode("utf-8")).hexdigest()[:10]
                    except Exception:
                        h = "na"
                    out[k] = f"{s}#{h}"
            except Exception:
                out[k] = "unserializable"
        return out

    def _ultra_run_phase_dir(run_id: str, phase: str) -> str:
        base = os.path.join("runs", str(run_id), str(phase))
        try:
            os.makedirs(base, exist_ok=True)
        except Exception:
            pass
        return base

    def _rotate_if_big(path: str, rotate_bytes: int = 2_000_000):
        """
        Policy-compliant rotation (no gzip). Rolls current file into a .zip archive
        next to it, then atomically truncates the original.
        """
        try:
            if rotate_bytes and os.path.isfile(path) and os.path.getsize(path) >= int(rotate_bytes):
                rot = path + ".1.zip"
                with zipfile.ZipFile(rot, "w", compression=zipfile.ZIP_DEFLATED) as z:
                    z.write(path, arcname=os.path.basename(path))
                tmp = path + ".tmp"
                with open(tmp, "w", encoding="utf-8") as f:
                    f.write("")  # atomic truncate
                os.replace(tmp, path)
                _UltraEmitter("SymbolicUltra").emit("ultra.rotate", path=path, rotated_to=rot)
        except Exception:
            pass

    # ---- SymbolicUltraKB ----------------------------------------------------
    class SymbolicUltraKB:
        def __init__(self, max_flat: int = 100_000, max_struct: int = 50_000, dedupe_window_s: float = 3.0):
            self.facts: deque = deque(maxlen=int(max_flat))
            self.echo = 0
            self.facts_struct: deque = deque(maxlen=int(max_struct))
            self.meta_index = defaultdict(list)
            self._meta_keys = OrderedDict()
            self.MAX_META_KEYS = int(os.getenv("ULTRA_MAX_META_KEYS", "10000"))
            self._rid_ctr: int = 0
            self._rid_to_pos: OrderedDict = OrderedDict()
            self._dedupe_window_s = float(max(0.0, dedupe_window_s))
            self._recent_fact_ts: OrderedDict = OrderedDict()
            self._phase_start_count = 0
            self._last_export_ts = 0.0
            self._emit = _UltraEmitter("SymbolicUltraKB")

        # ---------- core ops ----------
        def _maybe_evicted_cleanup(self):
            try:
                lim = max(1, self.facts_struct.maxlen or len(self.facts_struct))
                while len(self._rid_to_pos) > lim:
                    self._rid_to_pos.popitem(last=False)
            except Exception:
                pass

        def _dedupe_ok(self, f: str, now: float) -> bool:
            if self._dedupe_window_s <= 0.0:
                return True
            try:
                last = self._recent_fact_ts.get(f, None)
                if last is not None and (now - last) < self._dedupe_window_s:
                    self.echo += 1
                    return False
                self._recent_fact_ts[f] = now
                # bound recent map
                if len(self._recent_fact_ts) > 50_000:
                    self._recent_fact_ts.popitem(last=False)
            except Exception:
                pass
            return True

        def assert_fact(self, f: str, *, weight: float = 1.0, meta: Optional[dict] = None,
                        w_kairos: float = 1.0, w_keel: float = 1.0) -> dict:
            now = time.time()
            if f in self.facts:
                self.echo += 1
            self.facts.append(f)

            if not self._dedupe_ok(f, now):
                return {"t_event": now, "fact": f, "w_final": float(weight), "meta": dict(meta or {})}

            try:
                rid = self._rid_ctr = (self._rid_ctr + 1)
                rec = {
                    "rid": int(rid),
                    "t_event": float(now),     # immutable event time
                    "t_last": float(now),      # for decay integration
                    "fact": f,
                    "w_kairos": float(w_kairos),
                    "w_keel": float(w_keel),
                    "w_final": float(weight),
                    "meta": dict(meta) if isinstance(meta, dict) else {},
                }

                self.facts_struct.append(rec)
                self._rid_to_pos[rid] = len(self.facts_struct) - 1
                self._maybe_evicted_cleanup()

                for k, v in rec["meta"].items():
                    try:
                        key = f"{k}={v}"
                        self.meta_index[key].append(rid)
                        if len(self.meta_index[key]) > 50_000:
                            self.meta_index[key] = self.meta_index[key][-25_000:]
                        if key not in self._meta_keys:
                            self._meta_keys[key] = True
                        if len(self._meta_keys) > self.MAX_META_KEYS:
                            old_key, _ = self._meta_keys.popitem(last=False)
                            self.meta_index.pop(old_key, None)
                    except Exception:
                        continue
                return rec
            except Exception:
                return {"t_event": now, "fact": f, "w_final": float(weight), "meta": dict(meta or {})}

        # ---------- queries ----------
        def query(self, prefix: str):
            try:
                return [x for x in self.facts if isinstance(x, str) and x.startswith(prefix)]
            except Exception:
                return []

        def query_struct(self,
                         kind: Optional[str] = None,
                         time_from: Optional[float] = None,
                         time_to: Optional[float] = None,
                         min_w: Optional[float] = None,
                         key: Optional[str] = None) -> list:
            out = []
            try:
                for rec in self.facts_struct:
                    if kind is not None:
                        # 'kind' is encoded inside 'fact' as 'phase:kind|...'
                        s = rec.get("fact", "")
                        if ":" not in s:
                            continue
                        _, k = s.split(":", 1)
                        k = k.split("|", 1)[0]
                        if k != kind:
                            continue
                    t = rec.get("t_event", 0.0)
                    if time_from is not None and t < float(time_from):
                        continue
                    if time_to is not None and t > float(time_to):
                        continue
                    if min_w is not None and float(rec.get("w_final", 0.0)) < float(min_w):
                        continue
                    if key is not None:
                        # key is 'k=v' string; we match against meta_index for speed, but here we re-check to avoid drift
                        kv = key.split("=", 1)
                        mv = rec.get("meta", {})
                        if len(kv) == 2 and str(mv.get(kv[0], None)) != kv[1]:
                            continue
                    out.append(dict(rec))
            except Exception:
                pass
            return out

        # ---------- weight dynamics ----------
        def reinforce_fact(self, f: str, gain: float = 0.05):
            try:
                # walk backwards over struct; update first match
                for i in range(len(self.facts_struct) - 1, -1, -1):
                    if self.facts_struct[i].get("fact") == f:
                        self.facts_struct[i]["w_final"] = float(
                            min(2.0, float(self.facts_struct[i].get("w_final", 1.0)) * (1.0 + float(gain))))
                        break
            except Exception:
                pass

        def decay(self, lam: float = 0.001):
            now = time.time()
            try:
                if np is None:
                    # simple fallback
                    for rec in self.facts_struct:
                        dt = max(0.0, now - float(rec.get("t_last", now)))
                        rec["w_final"] = float(max(0.0, rec.get("w_final", 0.0) * (0.999 ** dt)))
                        rec["t_last"] = now
                    return
                for rec in self.facts_struct:
                    t_last = float(rec.get("t_last", rec.get("t_event", now)))
                    dt = max(0.0, now - t_last)
                    rec["w_final"] = float(rec.get("w_final", 0.0)) * float(np.exp(-float(lam) * dt))
                    rec["t_last"] = now
            except Exception:
                pass

        # ---------- drift & summaries ----------
        def drift_since_phase_begin(self) -> int:
            try:
                return max(0, len(self.facts) - int(self._phase_start_count))
            except Exception:
                return 0

        def phase_mark_begin(self):
            self._phase_start_count = len(self.facts)

        def summarize(self) -> dict:
            total = len(self.facts)
            try:
                haar = sum(1 for s in self.facts if isinstance(s, str) and "|meth=haar|" in s)
                fractal = sum(1 for s in self.facts if isinstance(s, str) and "|meth=fractal|" in s)
                sandbox = sum(1 for s in self.facts if isinstance(s, str) and s.startswith("sandbox"))
                holo = sum(1 for s in self.facts if isinstance(s, str) and s.startswith("holo"))
            except Exception:
                haar = fractal = sandbox = holo = 0
            return {
                "facts": total,
                "echo": self.echo,
                "tiles_total": haar + fractal,
                "haar": haar,
                "fractal": fractal,
                "sandbox": sandbox,
                "holo": holo,
            }

        def summarize_extended(self) -> dict:
            try:
                total = len(self.facts_struct)
                if total == 0:
                    return {"facts_struct": 0, "w_mean": 0.0, "w_max": 0.0, "drift": 0}
                ws = [float(rec.get("w_final", 1.0)) for rec in self.facts_struct]
                w_mean = sum(ws) / max(1, len(ws))
                w_max = max(ws) if ws else 0.0
                return {
                    "facts_struct": total,
                    "w_mean": float(w_mean),
                    "w_max": float(w_max),
                    "drift": self.drift_since_phase_begin(),
                }
            except Exception:
                return {"facts_struct": 0, "w_mean": 0.0, "w_max": 0.0, "drift": 0}
# ----------------------------------------------------------
# Utilities (LRU, hashing, glyph fallback)
# ----------------------------------------------------------
class _LRU(OrderedDict):
    def __init__(self, cap: int):
        super().__init__(); self._cap = int(max(8, cap))
    def get(self, key, default=None):
        if key in self:
            val = super().pop(key); super().__setitem__(key, val); return val
        return default
    def put(self, key, val):
        if key in self: super().pop(key)
        super().__setitem__(key, val)
        while len(self) > self._cap:
            self.popitem(last=False)
    def delete(self, key):
        try: super().pop(key)
        except Exception: pass

_DEF_HASH_SEED = b"kb_v2"

def _crc32_bytes(b: bytes) -> int:
    try:
        import zlib
        return zlib.crc32(b)
    except Exception:
        return int(hashlib.sha1(b).hexdigest()[:8], 16)

def hash_grid(arr: "np.ndarray") -> str:
    if np is None:
        return hashlib.sha1(str(arr).encode()).hexdigest()[:16]
    a = np.asarray(arr, dtype=int)
    buf = a.tobytes() + _DEF_HASH_SEED
    return f"{_crc32_bytes(buf):08x}:{a.shape}"

# Fallback glyph if invariants missing: shape + coarse hist + centroid
def _fallback_glyph_id(a: "np.ndarray") -> str:
    if np is None:
        return hashlib.sha1(str(a).encode()).hexdigest()[:12]
    x = np.asarray(a, dtype=int)
    R, C = x.shape if x.ndim == 2 else (1, x.size)
    try:
        xs = x if x.ndim == 2 else x.reshape((R, C))
        rstep = max(1, R // 12); cstep = max(1, C // 12)
        mini = xs[::rstep, ::cstep]
        vals, cnts = np.unique(mini, return_counts=True)
        mask = (mini != -1)
        if mask.any():
            rr, cc = np.where(mask)
            cent_r = float(np.mean(rr)); cent_c = float(np.mean(cc))
        else:
            cent_r = cent_c = 0.0
        sig = {
            "shape": (int(R), int(C)),
            "hist": dict(zip([int(v) for v in vals.tolist()], [int(c) for c in cnts.tolist()])),
            "cent": (round(cent_r, 1), round(cent_c, 1)),
        }
        return hashlib.sha1(json.dumps(sig, sort_keys=True).encode()).hexdigest()[:16]
    except Exception:
        return hashlib.sha1(np.asarray(x, dtype=int).tobytes()).hexdigest()[:16]


# ----------------------------------------------------------
# Symbolic Knowledge Base (thread-safe, hybrid-aware)
# ----------------------------------------------------------
class SymbolicKB:
    def __init__(self, persist_dir: str = "deployment"):
        # ---- soft-guarded globals / constants ----
        self.schema_version = str(globals().get("KB_SCHEMA_VERSION", "kb/2"))
        cache_cap           = int(globals().get("KB_RECALL_CACHE_CAP", 1024))
        self.CONF_FLOOR     = float(globals().get("KB_CONF_FLOOR", 0.01))
        self.CONF_CEIL      = float(globals().get("KB_CONF_CEIL", 0.99))
        self.FAIL_DECAY     = float(globals().get("KB_FAIL_DECAY", 0.985))
        self._cal_score_fn  = globals().get("_calibrated_score", None)
        self._atomic_write  = globals().get("_atomic_write", None)
        self._maybe_keel    = globals().get("_maybe_keel", lambda *_a, **_k: None)  # no-op if missing

        # Paths/schema
        self.persist_dir = persist_dir
        os.makedirs(self.persist_dir, exist_ok=True)
        self.store_path  = os.path.join(self.persist_dir, "kb_store.json")
        self.rollup_csv  = os.path.join(self.persist_dir, "kb_rollup.csv")
        self.ledger_path = os.path.join(self.persist_dir, "kb_ledger.jsonl")

        # Core stores & indices
        self.records: List[RuleRecord] = []
        self.mems: List[Dict[str, Any]] = []  # flattened XformMem-style dicts (backward compat)
        self.idx_by_glyph_shape: Dict[Tuple[str, Tuple[int, int]], List[int]] = {}

        # Failure stats: use fail/seen to track a rate
        self.idx_fail_stats: Dict[str, Dict[str, float]] = defaultdict(lambda: {"fail": 0.0, "seen": 0.0})

        # LRU recall cache
        self._recall_cache = _LRU(cache_cap)

        # Tunables
        self.decay_rate = 0.985
        self.promo_gain = 1.050
        self.max_records = 10000

        # Telemetry (bounded)
        self._telemetry_rows: List[Dict[str, Any]] = []
        self._tel_ds_n = max(1, int(globals().get("META_TEL_DOWNSAMPLE_N", 5)))
        self._tel_ctr = 0

        # Collaborators (soft links; set by solver)
        self.meta     = globals().get("meta", None)
        self.holo     = globals().get("holo", None)
        self.sandbox  = globals().get("sandbox", None)
        self.ultra    = globals().get("ultra", None)
        self.kairos   = globals().get("kairos", None)
        self.rulebase = globals().get("rulebase", None)
        self.rulegen  = globals().get("rulegen", None)
        self.hybrid   = None  # set by solver (e.g., solver.sim)
        self.encoder  = None
        try:
            sim = globals().get("sim", None)
            if sim is not None and hasattr(sim, "meta") and getattr(sim.meta, "encoder", None) is not None:
                self.encoder = sim.meta.encoder
        except Exception:
            pass

        # Concurrency
        self._lock = threading.RLock()

        # Background I/O pool
        self._io_pool = concurrent.futures.ThreadPoolExecutor(max_workers=2)

        # Load existing
        self._load_if_exists()

        self._emit("kb.init", schema=self.schema_version, n_records=len(self.records), n_mems=len(self.mems),
                   cache_cap=cache_cap)

    # ---- tiny helpers -------------------------------------------------------
    def _get_compute_invariants(self):
        return globals().get("compute_invariants", None)

    def _append_jsonl_atomic(self, path: str, obj: dict):
        try:
            line = json.dumps(obj, ensure_ascii=False) + "\n"
            tmp = path + ".tmp"
            with open(tmp, "a", encoding="utf-8") as f:
                f.write(line)
            os.replace(tmp, path)
        except Exception:
            # best-effort fallback
            try:
                with open(path, "a", encoding="utf-8") as f:
                    f.write(line)
            except Exception:
                pass

    # ------------------------------------------------------
    # Internal emit/log helpers
    # ------------------------------------------------------
    def _emit(self, topic: str, **payload):
        self._tel_ctr += 1
        if (self._tel_ctr % self._tel_ds_n) != 0:
            return
        rec = {"module": "kb", "event": topic, "time": time.time(), **payload}
        try:
            if callable(globals().get("_meta_log")): globals()["_meta_log"](topic, **rec)
        except Exception: pass
        try:
            _EXPLAIN = globals().get("_EXPLAIN", None)
            if _EXPLAIN is not None and hasattr(_EXPLAIN, "log"): _EXPLAIN.log(topic, rec)
        except Exception: pass
        try:
            if self.holo is not None and hasattr(self.holo, "_telemetry"): self.holo._telemetry(topic, **rec)
        except Exception: pass
        try:
            if self.kairos is not None and hasattr(self.kairos, "step"):
                conf = float(payload.get("confidence", payload.get("conf", 0.6)))
                _ = float((np.clip((conf - 0.6) * 0.02, -0.05, 0.05) if np is not None else 0.0))
                self.kairos.step(time_step=len(self.records) + len(self.mems))
        except Exception:
            pass

    def _log_tel(self, row: Dict[str, Any]):
        with self._lock:
            self._telemetry_rows.append(row)
            cap = int(globals().get("KB_TELEMETRY_CAP", 5000))
            if len(self._telemetry_rows) > cap:
                self._telemetry_rows = self._telemetry_rows[-(cap // 2):]

    # ------------------------------------------------------
    # Indexing helpers
    # ------------------------------------------------------
    def _index_record(self, idx: int, glyph_in: str, shape_in: Tuple[int, int]):
        with self._lock:
            key = (glyph_in, tuple(shape_in))
            self.idx_by_glyph_shape.setdefault(key, []).append(idx)

    def _rebuild_indices(self):
        with self._lock:
            self.idx_by_glyph_shape.clear()
            for i, rec in enumerate(self.records):
                try:
                    g = rec.meta.get("glyph_in", None)
                    s = tuple(getattr(rec.input_grid, "shape", ()))
                    if g and s:
                        self._index_record(i, g, s)
                except Exception:
                    pass

    # ------------------------------------------------------
    # Local ops preview helper (fallback when sandbox_apply_ops absent)
    # ------------------------------------------------------
    def _safe_apply_ops(self, inp: "np.ndarray", ops: List[Tuple[str, Dict[str, Any]]]):
        try:
            # Prefer global sandbox_apply_ops if present
            sap = globals().get("sandbox_apply_ops", None)
            if callable(sap):
                return True, sap(inp, ops)  # type: ignore[misc]
            # local simple fallbacks (subset)
            g = np.array(inp, dtype=int) if np is not None else inp
            for name, params in (ops or []):
                if name == "rot" and np is not None:
                    g = np.rot90(g, k=int(params.get("k", 1)))
                elif name == "flip_lr" and np is not None:
                    g = np.fliplr(g)
                elif name == "flip_ud" and np is not None:
                    g = np.flipud(g)
            return True, g
        except Exception as e:
            try:
                if callable(globals().get("_meta_log")):
                    globals()["_meta_log"]("kb.recall.candidate_error", reason=str(e), ops_len=len(ops or []))
            except Exception: pass
            try:
                _EXPLAIN = globals().get("_EXPLAIN", None)
                if _EXPLAIN is not None and hasattr(_EXPLAIN, "log"):
                    _EXPLAIN.log("kb.recall.candidate_error", {"reason": str(e), "ops_len": len(ops or [])})
            except Exception: pass
            return False, None

    # ------------------------------------------------------
    # Public API — remember & recall
    # ------------------------------------------------------
    def remember_xform(self, inp: "np.ndarray", out: "np.ndarray",
                       ops: List[Tuple[str, Dict[str, Any]]],
                       meta: Optional[Dict[str, Any]] = None,
                       confidence: float = 0.6) -> int:
        meta = dict(meta or {})
        if np is not None:
            a_in  = np.array(inp, dtype=int)
            a_out = np.array(out, dtype=int)
        else:
            a_in, a_out = inp, out

        # Compute invariants (best effort, NameError-safe)
        gi = go = None
        _ci = self._get_compute_invariants()
        try:
            if callable(_ci):
                gi = _ci(a_in)
                go = _ci(a_out)
        except Exception:
            gi = go = None

        glyph_in  = getattr(gi, "glyph_id", None) or _fallback_glyph_id(a_in)
        glyph_out = getattr(go, "glyph_id", None) or _fallback_glyph_id(a_out)
        shape_in  = tuple(map(int, getattr(a_in, "shape", ())))
        shape_out = tuple(map(int, getattr(a_out, "shape", ())))

        # Confidence & meta defaults
        ts_now = float(meta.get("ts", time.time()))
        c0 = float(min(max(confidence, self.CONF_FLOOR), self.CONF_CEIL))
        meta.setdefault("entropy_slope_in", float(getattr(gi, "entropy_slope", 0.0) if gi else 0.0))
        meta.setdefault("entropy_slope_out", float(getattr(go, "entropy_slope", 0.0) if go else 0.0))
        meta.setdefault("epi_in", float(getattr(gi, "epi", 0.0) if gi else 0.0))
        meta.setdefault("epi_out", float(getattr(go, "epi", 0.0) if go else 0.0))
        meta.setdefault("binder_in", float(getattr(gi, "binder_last", 0.0) if gi else 0.0))
        meta.setdefault("binder_out", float(getattr(go, "binder_last", 0.0) if go else 0.0))
        meta.update({
            "glyph_in": glyph_in,
            "glyph_out": glyph_out,
            "confidence": c0,
            "ts": ts_now,
            "schema": self.schema_version,
            "rule_kind": meta.get("rule_kind", "xform"),
        })

        rule = Rule("xform", params={"ops": list(ops or [])})
        rec = RuleRecord(input_grid=a_in, output_grid=a_out, rule=rule, meta=meta)

        with self._lock:
            self.records.append(rec)
            idx = len(self.records) - 1
            self._index_record(idx, glyph_in, shape_in)
            # Flat mem row (back-compat)
            self.mems.append({
                "tid": str(meta.get("task_id", "")),
                "train_index": int(meta.get("train_index", -1)),
                "glyph_in": glyph_in, "glyph_out": glyph_out,
                "shape_in": shape_in, "shape_out": shape_out,
                "entropy_slope_in": float(meta["entropy_slope_in"]),
                "entropy_slope_out": float(meta["entropy_slope_out"]),
                "epi_in": float(meta["epi_in"]), "epi_out": float(meta["epi_out"]),
                "binder_in": float(meta["binder_in"]), "binder_out": float(meta["binder_out"]),
                "ops": list(ops or []),
                "confidence": c0,
                "ts": ts_now,
                "rule_kind": str(meta["rule_kind"])
            })
            # Invalidate recall cache for this key
            self._recall_cache.delete((glyph_in, tuple(shape_in)))

            # Trim if needed
            trimmed = False
            if len(self.records) > self.max_records:
                def _key(i: int):
                    r = self.records[i]
                    c = float(r.meta.get("confidence", 0.0)); t = float(r.meta.get("ts", 0.0))
                    return (c, t)
                idx_sorted = sorted(range(len(self.records)), key=_key, reverse=True)[:self.max_records]
                self.records = [self.records[i] for i in idx_sorted]
                self.mems    = [self.mems[i]    for i in idx_sorted]
                self._rebuild_indices()
                self._recall_cache = _LRU(self._recall_cache._cap)
                # Clean stale glyphs from failure stats
                live_glyphs = {g for (g, _s) in self.idx_by_glyph_shape.keys()}
                for g in list(self.idx_fail_stats.keys()):
                    if g not in live_glyphs:
                        self.idx_fail_stats.pop(g, None)
                trimmed = True

        # Telemetry + encoder + sandbox + ledger
        self._log_tel({"time": ts_now, "kind": "remember_xform", "glyph_in": glyph_in, "glyph_out": glyph_out,
                       "shape_in": str(shape_in), "shape_out": str(shape_out), "confidence": float(c0), "ops_len": len(ops or [])})
        self._emit("kb.remember_xform", glyph_in=glyph_in, glyph_out=glyph_out, shape_in=shape_in, shape_out=shape_out,
                   ops_len=len(ops or []), confidence=float(c0), trimmed=bool(trimmed))
        try:
            if self.encoder is not None and hasattr(self.encoder, "record_feedback"):
                self.encoder.record_feedback(label="kb_xform", memory_layer="kb", success=True)
        except Exception: pass
        try:
            if self.sandbox is not None and hasattr(self.sandbox, "ingest_kb_record"):
                self.sandbox.ingest_kb_record(rec)
        except Exception: pass
        try:
            self._append_jsonl_atomic(self.ledger_path, {
                "schema": self.schema_version,
                "ts": ts_now, "glyph_in": glyph_in, "glyph_out": glyph_out,
                "confidence": float(c0),
                "hash": hashlib.sha1(f"{glyph_in}_{glyph_out}_{c0:.6f}_{ts_now:.6f}".encode()).hexdigest()
            })
        except Exception: pass

        return idx

    def _score_record(self, rec: RuleRecord, now: float) -> float:
        try:
            c = float(rec.meta.get("confidence", 0.5)); t = float(rec.meta.get("ts", 0.0))
            if callable(self._cal_score_fn):
                try:
                    return float(self._cal_score_fn(c, t, now=now))
                except Exception:
                    pass
            # simple fallback: recency-weighted confidence
            return float(max(0.0, min(1.0, c * ((0.97 if now >= t else 1.0) ** max(0.0, now - t)))))
        except Exception:
            return 0.5

    def recall_xforms(self, glyph_in: str, shape_in: Tuple[int, int], top_k: int = 8) -> List[RuleRecord]:
        key = (glyph_in, tuple(shape_in))
        cached = self._recall_cache.get(key)
        if cached is not None:
            self._emit("kb.recall_cache_hit", glyph_in=glyph_in, shape_in=tuple(shape_in), n=len(cached or []))
            return list(cached or [])[:top_k]

        with self._lock:
            ids = list(self.idx_by_glyph_shape.get(key, []))
            try:
                self.idx_fail_stats[glyph_in]["seen"] += 1.0
            except Exception:
                pass
        if not ids:
            self._recall_cache.put(key, [])
            self._emit("kb.recall", glyph_in=glyph_in, shape_in=tuple(shape_in), top_k=int(top_k), returned=0)
            return []

        now = time.time()
        scored = []
        with self._lock:
            for i in ids:
                try:
                    rec = self.records[i]
                    s = self._score_record(rec, now)
                    scored.append((i, s))
                except Exception:
                    pass
        scored.sort(key=lambda x: -x[1])
        out = [self.records[i] for i, _ in scored[:max(1, top_k)]]
        self._recall_cache.put(key, out)
        self._emit("kb.recall", glyph_in=glyph_in, shape_in=tuple(shape_in), top_k=int(top_k), returned=len(out))
        return out

    # Optional: return (record, predicted_grid) with per-candidate guard
    def recall_xforms_preview(self, inp: "np.ndarray", glyph_in: str, shape_in: Tuple[int,int],
                              top_k: int = 8) -> List[Tuple[RuleRecord, "np.ndarray"]]:
        recs = self.recall_xforms(glyph_in, shape_in, top_k=top_k)
        pairs: List[Tuple[RuleRecord, "np.ndarray"]] = []
        for rec in recs:
            ops = rec.rule.params.get("ops", []) if hasattr(rec, "rule") else []
            ok, pred = self._safe_apply_ops(inp, ops)
            if ok and pred is not None:
                pairs.append((rec, np.array(pred, dtype=int) if np is not None else pred))
        self._emit("kb.recall_preview", glyph_in=glyph_in, shape_in=shape_in, returned=len(pairs))
        return pairs

    # Failure accounting (rate-based)
    def record_failure(self, inp: "np.ndarray"):
        try:
            _ci = self._get_compute_invariants()
            if callable(_ci):
                glyph = _ci(np.asarray(inp, dtype=int) if np is not None else inp).glyph_id  # type: ignore
            else:
                glyph = _fallback_glyph_id(np.asarray(inp, dtype=int) if np is not None else inp)
            with self._lock:
                st = self.idx_fail_stats[glyph]
                st["fail"] = float(st.get("fail", 0.0) + 1.0)
            self._emit("kb.failure", glyph=glyph, fail=float(self.idx_fail_stats[glyph]["fail"]))
        except Exception:
            pass

    def _failure_penalty(self, glyph: str) -> float:
        st = self.idx_fail_stats.get(glyph)
        if not st: return 0.0
        seen = float(st.get("seen", 0.0)); fail = float(st.get("fail", 0.0))
        if seen <= 1e-9: return 0.0
        rate = max(0.0, min(1.0, fail / seen))
        return float(min(0.05, 0.02 * (rate * 100.0) ** 0.5))  # gentle, bounded

    # Candidate filter with Hybrid re-ranking (now accepts base_input)
    def candidate_filter_by_glyph(self, candidates: List["np.ndarray"], target_glyph: str,
                                  max_keep: int = 20, base_input: Optional["np.ndarray"] = None) -> List["np.ndarray"]:
        if not candidates:
            return []
        result: List["np.ndarray"]
        branch = "fallback"
        _ci = self._get_compute_invariants()
        if callable(_ci) and np is not None:
            kept: List["np.ndarray"] = []
            for c in candidates:
                try:
                    g = _ci(np.asarray(c, dtype=int)).glyph_id  # type: ignore
                    if g == target_glyph:
                        kept.append(c)
                        if len(kept) >= max_keep: break
                except Exception:
                    pass
            result = kept if kept else candidates[:max_keep]
            branch = "invariants"
        else:
            result = candidates[:max_keep]

        # Optional Hybrid ranking with provenance + base grid
        try:
            if self.hybrid is not None and hasattr(self.hybrid, "rank_candidates"):
                fail_pen = float(self._failure_penalty(target_glyph))
                meta_list = []
                for _c in result:
                    meta_list.append({"source": "kb_xform", "confidence": 0.75, "glyph": target_glyph, "failures": fail_pen})
                ranked = self.hybrid.rank_candidates(list(zip(result, meta_list)), base=base_input)  # type: ignore[arg-type]
                result = [r[0] for r in ranked]
        except Exception:
            pass

        self._emit("kb.candidate_filter_by_glyph", target_glyph=target_glyph,
                   in_n=len(candidates), out_n=len(result), branch=branch)
        return result

    # ------------------------------------------------------
    # Commit helper (unified Holo/Meta/Rulebase/RuleGen)
    # ------------------------------------------------------
    def commit_xform(self, inp: "np.ndarray", out: "np.ndarray", ops: List[Tuple[str, Dict[str, Any]]],
                     solver: Any = None,
                     meta_extra: Optional[Dict[str, Any]] = None,
                     confidence: float = 0.7):
        meta = dict(meta_extra or {})
        ridx = self.remember_xform(inp, out, ops, meta=meta, confidence=confidence)

        # Holo
        try:
            target = getattr(solver, "holo", None) if solver is not None else self.holo
            if target is not None and hasattr(target, "add"):
                target.add(np.array(inp, dtype=int) if np is not None else inp,
                           np.array(out, dtype=int) if np is not None else out,
                           {"subject": "kb.commit", "rule_kind": "xform",
                            "ops": list(ops or []), "confidence": float(confidence)})
        except Exception:
            pass

        # Meta / Ultra emit
        self._emit("kb.commit_xform", ops=len(ops or []), confidence=float(confidence), ridx=int(ridx))

        # Encoder feedback (symbolic ML)
        try:
            ctl = None
            if solver is not None:
                if hasattr(solver, "meta") and hasattr(solver.meta, "symbolic"):
                    ctl = solver.meta.symbolic
                elif hasattr(solver, "symbolic"):
                    ctl = solver.symbolic
            ctl = ctl or self.encoder
            if ctl is not None and hasattr(ctl, "record_feedback"):
                ctl.record_feedback("kb_commit", "kb", success=True)
        except Exception:
            pass

        # Local rulebase add
        try:
            rb = getattr(solver, "rulebase", None) if solver is not None else self.rulebase
            if rb is not None and hasattr(rb, "add"):
                rb.add(RuleRecord(np.array(inp, dtype=int) if np is not None else inp,
                                  np.array(out, dtype=int) if np is not None else out,
                                  Rule("xform", params={"ops": list(ops or [])}),
                                  {"rule_kind": "xform", "confidence": float(confidence)}))
        except Exception:
            pass

        # Optional rule generator refinement
        try:
            rg = getattr(solver, "rulegen", None) if solver is not None else self.rulegen
            if rg is not None and hasattr(rg, "refine_from_kb"):
                rg.refine_from_kb(inp=np.array(inp, dtype=int) if np is not None else inp,
                                  out=np.array(out, dtype=int) if np is not None else out,
                                  ops=list(ops or []),
                                  confidence=float(confidence))
        except Exception:
            pass

    # ------------------------------------------------------
    # Adaptive promotion / decay
    # ------------------------------------------------------
    def promote(self, idx: int, factor: Optional[float] = None):
        try:
            with self._lock:
                rec = self.records[idx]
                c0 = float(rec.meta.get("confidence", 0.6))
                f  = float(self.promo_gain if factor is None else factor)
                gi = float(rec.meta.get("entropy_slope_in", 0.0))
                go = float(rec.meta.get("entropy_slope_out", 0.0))
                epi = float(rec.meta.get("epi_out", 0.0))
                f *= (1.0 + 0.05 * max(0.0, 0.5 - abs(go - gi)) + 0.05 * max(0.0, epi))
                c1 = float(min(max(c0 * f, self.CONF_FLOOR), self.CONF_CEIL))
                rec.meta["confidence"] = c1
            self._emit("kb.promote", idx=int(idx), conf=float(c1))
        except Exception:
            pass

    def promote_on_success(self, glyph_in: str):
        with self._lock:
            ids = [i for (g, _), lst in self.idx_by_glyph_shape.items() if g == glyph_in for i in lst]
        for i in ids:
            self.promote(i, factor=1.03)

    def decay_all(self, rate: Optional[float] = None):
        r = float(self.decay_rate if rate is None else rate)
        with self._lock:
            try:
                ent_out = [float(rec.meta.get("entropy_slope_out", 0.0)) for rec in self.records]
                avg_entropy = float(np.mean(ent_out)) if (np is not None and ent_out) else 0.0
            except Exception:
                avg_entropy = 0.0
            adaptive_r = float(min(max(r - (avg_entropy * 0.002), 0.95), 0.999))
            for rec in self.records:
                try:
                    c0 = float(rec.meta.get("confidence", 0.6))
                    rec.meta["confidence"] = float(min(max(c0 * adaptive_r, self.CONF_FLOOR), self.CONF_CEIL))
                except Exception:
                    pass
            # decay failure stats
            for k in list(self.idx_fail_stats.keys()):
                self.idx_fail_stats[k]["fail"] = float(self.idx_fail_stats[k].get("fail", 0.0) * self.FAIL_DECAY)
                self.idx_fail_stats[k]["seen"] = float(self.idx_fail_stats[k].get("seen", 0.0) * self.FAIL_DECAY)
                if (self.idx_fail_stats[k]["fail"] + self.idx_fail_stats[k]["seen"]) < 1e-6:
                    self.idx_fail_stats.pop(k, None)
        self._emit("kb.decay_all", rate=float(adaptive_r), n=len(self.records))

        # Confidence tail normalization (p95 clamp)
        try:
            confs = [float(rec.meta.get("confidence", 0.6)) for rec in self.records]
            if confs:
                if np is not None:
                    p95 = float(np.percentile(np.array(confs, dtype=float), 95))
                else:
                    confs_sorted = sorted(confs)
                    p95 = confs_sorted[int(max(0, min(len(confs_sorted)-1, 0.95*len(confs_sorted))))]
                if p95 > self.CONF_CEIL:
                    scale = self.CONF_CEIL / max(1e-9, p95)
                    with self._lock:
                        for rec in self.records:
                            rec.meta["confidence"] = float(min(self.CONF_CEIL, max(self.CONF_FLOOR, rec.meta.get("confidence", 0.6) * scale)))
                    self._emit("kb.conf_norm", p95=p95, scaled=len(self.records))
        except Exception:
            pass

    # ------------------------------------------------------
    # Persistence (atomic) + schema migration
    # ------------------------------------------------------
    def _rec_to_json(self, rec: RuleRecord) -> Dict[str, Any]:
        try:
            ig = np.array(rec.input_grid, dtype=int).tolist() if np is not None else rec.input_grid
            og = np.array(rec.output_grid, dtype=int).tolist() if np is not None else rec.output_grid
        except Exception:
            ig, og = rec.input_grid, rec.output_grid
        return {
            "input_grid": ig,
            "output_grid": og,
            "rule": {"kind": rec.rule.kind, "params": dict(rec.rule.params or {})},
            "meta": dict(rec.meta or {})
        }

    def _rec_from_json(self, d: Dict[str, Any]) -> RuleRecord:
        if "input_grid" not in d or "output_grid" not in d or "rule" not in d or "meta" not in d:
            raise ValueError("Malformed record in kb_store.json")
        inp = np.array(d["input_grid"], dtype=int) if np is not None else d["input_grid"]
        out = np.array(d["output_grid"], dtype=int) if np is not None else d["output_grid"]
        rj  = d["rule"]
        # Migrate from legacy payload schema if present
        if "params" in rj:
            params = dict(rj.get("params", {}))
        elif "payload" in rj:
            params = dict(rj.get("payload", {}))
        else:
            params = {}
        rule = Rule(str(rj.get("kind", "xform")), params=params)
        meta = dict(d.get("meta", {}))
        meta.setdefault("schema", self.schema_version)
        return RuleRecord(input_grid=inp, output_grid=out, rule=rule, meta=meta)

    def _save_impl(self):
        try:
            with self._lock:
                data = {
                    "schema": self.schema_version,
                    "version": 2,
                    "created_with": "SymbolicKB-v2",
                    "records": [self._rec_to_json(r) for r in self.records[-self.max_records:]],
                    "mems": list(self.mems[-self.max_records:]),
                    "idx_fail_stats": {k: {"fail": float(v.get("fail", 0.0)), "seen": float(v.get("seen", 0.0))} for k, v in self.idx_fail_stats.items()},
                }
            payload = json.dumps(data).encode("utf-8")
            if callable(self._atomic_write):
                self._atomic_write(self.store_path, payload)
            else:
                tmp = self.store_path + ".tmp"
                with open(tmp, "wb") as f: f.write(payload)
                os.replace(tmp, self.store_path)
            self._emit("kb.save", path=self.store_path, n=len(data["records"]))
            try: self._maybe_keel(self.store_path, kind="store_json")
            except Exception: pass
        except Exception as e:
            try:
                if callable(globals().get("_meta_log")): globals()["_meta_log"]("kb.save_error", err=str(e))
            except Exception: pass

    def save(self):
        self._io_pool.submit(self._save_impl)

    def save_sync(self):
        self._save_impl()

    def _rollup_impl(self):
        try:
            rows = []
            with self._lock:
                for i, rec in enumerate(self.records):
                    g_in  = rec.meta.get("glyph_in", "")
                    g_out = rec.meta.get("glyph_out", "")
                    shp_in  = tuple(getattr(rec.input_grid, "shape", ()))
                    shp_out = tuple(getattr(rec.output_grid, "shape", ()))
                    rows.append({
                        "idx": i,
                        "glyph_in": g_in, "glyph_out": g_out,
                        "shape_in": str(shp_in), "shape_out": str(shp_out),
                        "confidence": float(rec.meta.get("confidence", 0.6)),
                        "ops_len": len(rec.rule.params.get("ops", [])) if hasattr(rec, "rule") else 0,
                        "epi_delta": float(rec.meta.get("epi_out", 0.0)) - float(rec.meta.get("epi_in", 0.0)),
                        "ts": float(rec.meta.get("ts", 0.0))
                    })
            if not rows:
                return
            hdr = list(rows[0].keys())
            tmp_csv = self.rollup_csv + ".tmp"
            with open(tmp_csv, "w", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=hdr)
                w.writeheader(); w.writerows(rows)
            os.replace(tmp_csv, self.rollup_csv)
            self._emit("kb.rollup", path=self.rollup_csv, n=len(rows))
            try: self._maybe_keel(self.rollup_csv, kind="rollup_csv")
            except Exception: pass
        except Exception as e:
            try:
                if callable(globals().get("_meta_log")): globals()["_meta_log"]("kb.rollup_error", err=str(e))
            except Exception: pass

    def export_rollup(self):
        self._io_pool.submit(self._rollup_impl)

    def rollup_sync(self):
        self._rollup_impl()

    def shutdown(self, flush_rollup: bool = True):
        try:
            if flush_rollup:
                self.export_rollup()
            self.save()
        except Exception:
            pass
        try:
            self._io_pool.shutdown(wait=True, cancel_futures=False)
        except Exception:
            pass
        self._emit("kb.shutdown", ok=True)

    # ------------------------------------------------------
    # Visuals & Telemetry export (guarded)
    # ------------------------------------------------------
    def export_visuals(self, prefix: str = "kb_visuals"):
        try:
            import matplotlib.pyplot as plt
        except Exception:
            return
        os.makedirs(prefix, exist_ok=True)

        try:
            xs, ys, cs = [], [], []
            with self._lock:
                sample_stride = max(1, len(self.records) // 3000)
                for rec in itertools.islice(self.records, 0, None, sample_stride):
                    xs.append(float(rec.meta.get("confidence", 0.6)))
                    ys.append(float(rec.meta.get("binder_out", 0.0)))
                    cs.append(float(rec.meta.get("entropy_slope_out", 0.0)) - float(rec.meta.get("entropy_slope_in", 0.0)))
            if xs:
                plt.figure(figsize=(4.4, 3.4))
                sc = plt.scatter(xs, ys, c=cs, s=9)
                plt.xlabel("confidence"); plt.ylabel("binder_out")
                cb = plt.colorbar(sc); cb.set_label("Δ entropy_slope(out-in)")
                plt.tight_layout(); plt.savefig(os.path.join(prefix, "conf_vs_binder.png"), dpi=140); plt.close()
        except Exception:
            pass

        try:
            with self._lock:
                items = sorted(((k, v.get("fail", 0.0)) for k, v in self.idx_fail_stats.items()), key=lambda kv: -kv[1])[:200]
            if items:
                keys = [k for (k, _) in items]; vals = [v for (_, v) in items]
                plt.figure(figsize=(max(4, len(keys)*0.06), 2.6))
                plt.bar(range(len(keys)), vals); plt.title("failures per glyph (top-200)")
                plt.tight_layout(); plt.savefig(os.path.join(prefix, "glyph_failures.png"), dpi=140); plt.close()
        except Exception:
            pass

        try:
            with self._lock:
                ts   = [float(r.get("time", 0.0)) for r in self._telemetry_rows]
                conf = [float(r.get("confidence", 0.6)) for r in self._telemetry_rows]
            if ts:
                t0 = min(ts); xs = [t - t0 for t in ts]
                step = max(1, len(xs) // 3000)
                xs_d = xs[::step]; conf_d = conf[::step]
                plt.figure(figsize=(4.2, 3.4))
                plt.scatter(xs_d, conf_d, s=6)
                plt.xlabel("time (s from start)"); plt.ylabel("confidence (telemetry)")
                plt.tight_layout(); plt.savefig(os.path.join(prefix, "telemetry_conf.png"), dpi=140); plt.close()
        except Exception:
            pass

        self._emit("kb.export_visuals", prefix=prefix)

    def export_telemetry_csv(self, path: str = "kb_telemetry.csv"):
        try:
            with self._lock:
                if not self._telemetry_rows: return
                rows = list(self._telemetry_rows)
            with open(path, "w", newline="", encoding="utf-8") as f:
                keys = sorted(set().union(*[set(r.keys()) for r in rows]))
                w = csv.DictWriter(f, fieldnames=keys); w.writeheader()
                for r in rows: w.writerow({k: r.get(k) for k in keys})
            self._emit("kb.export_telemetry", path=path, n=len(rows))
        except Exception as e:
            try:
                if callable(globals().get("_meta_log")): globals()["_meta_log"]("kb.export_telemetry_error", err=str(e))
            except Exception: pass

    # Optional periodic health snapshot
    def export_health_snapshot(self, path: Optional[str] = None):
        if not bool(globals().get("KB_DIAG_EXPORTS", True)): return
        out = path or os.path.join(self.persist_dir, "kb_health.jsonl")
        try:
            with self._lock:
                n_recs = len(self.records)
                glyphs = {g for (g, _s) in self.idx_by_glyph_shape.keys()}
                avg_conf = float(np.mean([float(r.meta.get("confidence", 0.6)) for r in self.records])) if (np is not None and self.records) else 0.0
            snap = {"t": time.time(), "n_records": n_recs, "n_glyphs": len(glyphs), "avg_conf": avg_conf}
            self._append_jsonl_atomic(out, snap)
            self._emit("kb.health_snapshot", **snap)
        except Exception: pass

    # ------------------------------------------------------
    # Load (with migration support)
    # ------------------------------------------------------
    def _load_if_exists(self):
        if not os.path.isfile(self.store_path):
            return
        try:
            with open(self.store_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            schema = str(data.get("schema", "kb/0"))
            recs = [self._rec_from_json(r) for r in data.get("records", [])]
            with self._lock:
                self.records = recs
                # mems best-effort
                self.mems = []
                for m in data.get("mems", []):
                    try:
                        self.mems.append(dict(m))
                    except Exception:
                        pass
                # failure stats (rate-based)
                self.idx_fail_stats = defaultdict(lambda: {"fail": 0.0, "seen": 0.0})
                for k, v in (data.get("idx_fail_stats", {}) or {}).items():
                    try:
                        self.idx_fail_stats[str(k)] = {"fail": float(v.get("fail", 0.0)), "seen": float(v.get("seen", 0.0))}
                    except Exception:
                        pass
                self._rebuild_indices()
                self._recall_cache = _LRU(self._recall_cache._cap)
            if schema != self.schema_version:
                self._emit("kb.migrate", from_schema=schema, to_schema=self.schema_version)
            self._emit("kb.load", schema=schema, n_records=len(self.records), n_mems=len(self.mems))
        except Exception as e:
            try:
                if callable(globals().get("_meta_log")): globals()["_meta_log"]("kb.load_error", err=str(e))
            except Exception: pass

    # ------------------------------------------------------
    # Dials
    # ------------------------------------------------------
    def set_recency_tau(self, tau: float):        
        try:
            globals()["KB_RECENCY_TAU"] = float(tau)
            self._emit("kb.set_recency_tau", tau=float(tau))
        except Exception:
            pass
# ----------------------
# Kairos
# ----------------------
class KairosPulseManager:    
    def __init__(self, n=30, pulse_amplitude=0.3, adapt=True, flux_hist=64, flux_viz_every=0):
        self.n = int(n)
        self._pulse_amplitude = float(pulse_amplitude)
        self.depth = 1.0
        self.phase_time = 0
        self.current_epoch = 0  # unified clock for external telemetry
        self.symbolic_state = "Ω₀"
        self.last_entropy_flux = 0.0
        self.state_history: List[Tuple[int, str, float]] = []
        self._adapt = bool(adapt)
        self._flux_hist = deque(maxlen=int(max(8, flux_hist)))
        self._viz_every = int(max(0, flux_viz_every))
        self._emit_ctr = 0  # DS gate
        self._ds_n = _get_tel_downsample_n_fallback()

        # Optional: provenance seed for headers
        try:
            self.seed = int(os.getenv("KAIROS_SEED", str(int(time.time()) % 10_000_000)))
        except Exception:
            self.seed = int(time.time()) % 10_000_000

        # Hysteresis to avoid Ω thrashing
        self._omega_hysteresis = _envf("KAIROS_OMEGA_HYST", 0.05)

        # Flux feature buffer (rolling analytics)
        self._flux_feat_window = _envi("KAIROS_FLUX_FEAT_WIN", 64)
        self._flux_feat_hist = deque(maxlen=self._flux_feat_window)

        # Optional Matplotlib availability
        self._has_plt = bool(_KAIROS_HAS_PLT)

        # Optional pluggable backend
        self._flux_backend = os.getenv("KAIROS_FLUX_BACKEND", "rhcm")  # "rhcm" | "noise" | "file"
        self._flux_file    = os.getenv("KAIROS_FLUX_FILE", "")

        # Health metrics (pulse jitter, drift, last emit age)
        self._last_emit_wall_ts: Optional[float] = None
        self._last_emit_step: Optional[int] = None
        self._emit_wall_hist: deque = deque(maxlen=128)  # seconds between emits
        self._emit_step_hist: deque = deque(maxlen=128)  # steps between emits

        # Heartbeat and brain coupling
        self._heartbeat_active: bool = False
        self._on_state_change: Optional[Callable[[str, str], None]] = None

        # Optional mirrored system-health gauges (generic; no Keel coupling)
        self._mem_gain_ewma: float = 0.0
        self._io_pressure_ewma: float = 0.0

    # ---------- helpers ----------
    def _refresh_ds_from_meta(self):
        # If Meta updated dials at runtime, pick them up lazily
        try:
            if "get_meta_dials" in globals() and callable(globals()["get_meta_dials"]):
                d = globals()["get_meta_dials"]()  # type: ignore
                self._ds_n = max(1, int(d.get("tel_downsample_n", self._ds_n)))
        except Exception:
            pass

    def _ds_ok(self) -> bool:
        self._emit_ctr += 1
        if (self._emit_ctr % max(1, self._ds_n)) == 0:
            return True
        return False

    def _adaptive_amp(self) -> float:
        if not self._adapt or len(self._flux_hist) < 8:
            return self._pulse_amplitude
        v = float(np.std(np.array(self._flux_hist, dtype=float)))
        # bounded adaptive amplitude (R-172)
        adj = float(np.clip(KAIROS_AMP_BASE + KAIROS_AMP_SCALE * np.tanh(v / 10.0),
                            KAIROS_AMP_MIN, KAIROS_AMP_MAX))
        return adj

    @property
    def _pulse_amplitude(self) -> float:
        return self.__amp

    @_pulse_amplitude.setter
    def _pulse_amplitude(self, v: float):
        self.__amp = float(np.clip(v, KAIROS_AMP_MIN, KAIROS_AMP_MAX))

    def _emit_flux_map(self):
        # optional, headless-safe
        if not self._viz_every or not self._has_plt:
            return
        try:
            if (len(self.state_history) % self._viz_every) != 0:
                return
            M = self._compute_flux_matrix(self.phase_time)  # consistent with backend
            os.makedirs("deployment/flux_maps", exist_ok=True)
            out = f"deployment/flux_maps/flux_{self.phase_time:06d}.png"
            plt.figure(figsize=(3,3)); plt.imshow(M, aspect="auto"); plt.axis("off"); plt.tight_layout()
            plt.savefig(out, dpi=120); plt.close()
            if self._ds_ok():
                _safe_emit("kairos.flux_map", {"path": out})
        except Exception:
            pass

    def _classify_omega(self, entropy_rhcm: float, entropy_inv: float, flux: float) -> str:
        # Ω₀: near equality; hysteresis to avoid boundary thrash
        if math.isclose(entropy_rhcm, entropy_inv, rel_tol=0.02, abs_tol=0.1 * (1.0 + self._omega_hysteresis)):
            return "Ω₀"
        if flux < KAIROS_OMEGA1_MAX:
            return "Ω₁"
        if flux < KAIROS_OMEGA2_MAX:
            return "Ω₂"
        if flux < KAIROS_OMEGA3_MAX:
            return "Ω₃"
        return "Ω₄"

    def _compute_flux_matrix(self, time_step: int) -> np.ndarray:
        # future: plug in alternate backends via env
        if self._flux_backend == "rhcm" or not self._flux_backend:
            return generate_RHCM(self.n, recursion_depth=self.depth)
        elif self._flux_backend == "noise":
            rng = np.random.default_rng(seed=(self.seed ^ time_step) + int(self.depth * 1e3))
            return rng.normal(0.0, 1.0, size=(self.n, self.n))
        elif self._flux_backend == "file" and os.path.isfile(self._flux_file):
            try:
                # simple CSV matrix loader
                return np.loadtxt(self._flux_file, delimiter=",")
            except Exception:
                return generate_RHCM(self.n, recursion_depth=self.depth)
        else:
            return generate_RHCM(self.n, recursion_depth=self.depth)

    def _emit_health(self):
        """Surface a small kairos.health metric: pulse jitter, drift, last emit age."""
        try:
            now = time.time()
            last_age = None if self._last_emit_wall_ts is None else float(now - self._last_emit_wall_ts)
            # Drift: difference between observed mean step gap and expected gap (≈ _ds_n)
            step_arr = np.array(self._emit_step_hist, dtype=float) if self._emit_step_hist else np.array([], dtype=float)
            wall_arr = np.array(self._emit_wall_hist, dtype=float) if self._emit_wall_hist else np.array([], dtype=float)
            mean_step_gap = float(step_arr.mean()) if step_arr.size else 0.0
            jitter_wall = float(wall_arr.std()) if wall_arr.size else 0.0
            drift_steps = float(mean_step_gap - float(self._ds_n)) if mean_step_gap else 0.0
            _safe_emit("kairos.health", {
                "age": last_age if last_age is not None else -1.0,
                "jitter": jitter_wall,
                "drift_steps": drift_steps,
                "ds_n": int(self._ds_n),
            })
        except Exception:
            pass

    # ---------- public API ----------
    def step(self, time_step: int):
        # refresh downsample dial opportunistically (SSOT)
        self._refresh_ds_from_meta()

        self.phase_time = int(time_step)
        self.current_epoch = self.phase_time

        amp = self._adaptive_amp()
        # bounded sinusoidal modulation of “depth” (kept internal / non-destructive)
        mod_depth = self.depth + amp * math.sin(2 * math.pi * self.phase_time / 10.0)
        # we keep self.depth stable as a control dial; mod_depth is for backend seeding if desired

        # Compute flux matrix
        M = self._compute_flux_matrix(self.phase_time)
        try:
            Minv = np.linalg.pinv(M)
        except Exception:
            Minv = np.zeros_like(M)

        entropy_rhcm = float(np.std(M))
        entropy_inv  = float(np.std(Minv))
        entropy_flux = float(entropy_rhcm - entropy_inv)

        # update state
        prev_state = self.symbolic_state
        self.last_entropy_flux = entropy_flux
        self._flux_hist.append(entropy_flux)
        self.symbolic_state = self._classify_omega(entropy_rhcm, entropy_inv, entropy_flux)
        self.state_history.append((self.phase_time, self.symbolic_state, entropy_flux))

        # optional brain callback
        try:
            if self._on_state_change is not None and prev_state != self.symbolic_state:
                self._on_state_change(prev_state, self.symbolic_state)
        except Exception:
            pass

        # Flux analytics vector
        try:
            self._flux_feat_hist.append(entropy_flux)
            arr = np.array(self._flux_feat_hist, dtype=float)
            m = float(arr.mean()); s = float(arr.std() + 1e-12)
            feats = {
                "mean": m,
                "std":  float(arr.std()),
                "skew": float(((arr - m)**3).mean()/(s**3)),
                "kurt": float(((arr - m)**4).mean()/(s**4)),
                "zxc":  int(np.count_nonzero(np.diff(np.sign(arr)) != 0))
            }
        except Exception:
            feats = {}

        # unified emit (downsampled)
        if self._ds_ok():
            # update health trackers prior to emitting
            now = time.time()
            if self._last_emit_wall_ts is not None:
                self._emit_wall_hist.append(float(now - self._last_emit_wall_ts))
            if self._last_emit_step is not None:
                self._emit_step_hist.append(float(self.phase_time - self._last_emit_step))
            self._last_emit_wall_ts = now
            self._last_emit_step = self.phase_time

            payload = {
                "time": self.phase_time,
                "state": self.symbolic_state,
                "flux": entropy_flux,
                "depth": self.depth,
                "amp": amp,
                **({f"flux_feat_{k}": v for k, v in feats.items()} if feats else {})
            }
            # include optional mirrored system-health gauges if present
            try:
                payload["mem_gain"] = float(self._mem_gain_ewma)
                payload["io_pressure"] = float(self._io_pressure_ewma)
            except Exception:
                pass

            _safe_emit("kairos.pulse", payload)

            # health metric alongside pulse
            self._emit_health()

        self._emit_flux_map()

    def update_system_health(self, mem_gain: Optional[float] = None, io_pressure: Optional[float] = None, alpha: float = 0.02):
        """Optional, generic health gauges mirrored into pulses (no Keel coupling)."""
        try:
            if mem_gain is not None:
                self._mem_gain_ewma = float((1.0 - alpha) * self._mem_gain_ewma + alpha * float(mem_gain))
            if io_pressure is not None:
                self._io_pressure_ewma = float((1.0 - alpha) * self._io_pressure_ewma + alpha * float(io_pressure))
        except Exception:
            pass

    def attach_brain(self, on_event: Optional[Callable[[str, str], None]] = None):
        """Optionally register a callback on state changes: on_event(prev_omega, new_omega)."""
        self._on_state_change = on_event

    def start_heartbeat(self, run_id: str, phase: str, toggles: Any, meta_path: str = "explanations.jsonl",
                        state_path: str = "symbolic_ml_state.json", window: int = 200, daemon: bool = True):
        """Begin Kairos-owned heartbeat with injected dependencies; best-effort, never-throw."""
        try:
            if "start_live_meta_heartbeat" in globals() and callable(globals()["start_live_meta_heartbeat"]):
                globals()["start_live_meta_heartbeat"](  # type: ignore
                    self,
                    meta_path=meta_path,
                    state_path=state_path,
                    window=window,
                    daemon=daemon
                )
        except Exception:
            try:
                if "meta_log" in globals() and callable(globals()["meta_log"]):
                    globals()["meta_log"]("solver.heartbeat_init_failed")
            except Exception:
                pass

        # Attach summary (single emit)
        try:
            _safe_emit("solver.attach_summary", {
                "has_kb": bool(getattr(self, "kb", None)),
                "has_rulebase": bool(getattr(self, "rulebase", None)),
                "has_sbx": bool(getattr(self, "sandbox", None)),
                "has_ml": bool(getattr(self, "ml", None)),
                "has_blender": bool(getattr(self, "blender", None)),
                "has_holo": bool(getattr(self, "holo", None)),
            })
        except Exception:
            pass

        try:
            _safe_emit("solver.init", {
                "run_id": run_id,
                "phase": phase,
                "toggles": (asdict(toggles) if hasattr(toggles, "__dict__") or hasattr(toggles, "__annotations__") else (toggles if isinstance(toggles, dict) else {}))
            })
        except Exception:
            pass

        _safe_emit("solver.initialized", {"run_id": run_id, "phase": phase})

        # cadence counters
        self._pred_count   = 0
        self._accept_count = 0
        self._heartbeat_active = True

    def stop_heartbeat(self):
        """Stop Kairos-owned heartbeat (flag only; external stoppers may be owned elsewhere)."""
        self._heartbeat_active = False

    def get_state(self) -> Dict[str, Any]:
        return {
            "time": self.phase_time,
            "depth": self.depth,
            "state": self.symbolic_state,
            "entropy_flux": self.last_entropy_flux
        }

    def get_history(self) -> List[Tuple[int, str, float]]:
        return list(self.state_history)

    def get_health(self) -> Dict[str, Any]:
        """Return current kairos.health snapshot (non-emitting)."""
        now = time.time()
        last_age = None if self._last_emit_wall_ts is None else float(now - self._last_emit_wall_ts)
        wall_arr = np.array(self._emit_wall_hist, dtype=float) if self._emit_wall_hist else np.array([], dtype=float)
        step_arr = np.array(self._emit_step_hist, dtype=float) if self._emit_step_hist else np.array([], dtype=float)
        mean_step_gap = float(step_arr.mean()) if step_arr.size else 0.0
        jitter_wall = float(wall_arr.std()) if wall_arr.size else 0.0
        drift_steps = float(mean_step_gap - float(self._ds_n)) if mean_step_gap else 0.0
        return {
            "age": last_age if last_age is not None else -1.0,
            "jitter": jitter_wall,
            "drift_steps": drift_steps,
            "ds_n": int(self._ds_n),
        }


# ----------------------------------------------------------------
# Kairos Pulse Manager  (single authority • unified emit • guarded • SSOT dials)
# ----------------------------------------------------------------


# --- SSOT dials (borrow from Meta when available) ----------------------------
def _get_tel_downsample_n_fallback() -> int:
    try:
        # If Meta's SSOT accessor is present, use it
        if "get_meta_dials" in globals() and callable(globals()["get_meta_dials"]):
            d = globals()["get_meta_dials"]()  # type: ignore
            return max(1, int(d.get("tel_downsample_n", 1)))
    except Exception:
        pass
    # Fallback to env
    try:
        return max(1, int(os.getenv("META_TEL_DOWNSAMPLE_N", "1")))
    except Exception:
        return 1

def _envi(key: str, dflt: int) -> int:
    try: return int(os.getenv(key, str(dflt)))
    except Exception: return dflt

# Ω-thresholds for entropy_flux bands (Ω₀ handled by near-equality check)
KAIROS_OMEGA1_MAX = _envf("KAIROS_OMEGA1_MAX", 5.0)
KAIROS_OMEGA2_MAX = _envf("KAIROS_OMEGA2_MAX", 20.0)
KAIROS_OMEGA3_MAX = _envf("KAIROS_OMEGA3_MAX", 50.0)

# Adaptive amplitude dials
KAIROS_AMP_BASE  = _envf("KAIROS_AMP_BASE", 0.15)
KAIROS_AMP_SCALE = _envf("KAIROS_AMP_SCALE", 0.35)
KAIROS_AMP_MIN   = _envf("KAIROS_AMP_MIN", 0.10)
KAIROS_AMP_MAX   = _envf("KAIROS_AMP_MAX", 0.50)


def _safe_emit(topic: str, payload: dict):
        # Best-effort fan-out; never throws
    try:
        meta_log(topic, **payload)  # type: ignore[name-defined]
    except Exception:
        pass
    try:
        if "EXPLAIN" in globals() and EXPLAIN is not None:
            EXPLAIN.log(topic, payload)  # type: ignore[name-defined]
    except Exception:
        pass
    try:
        u = globals().get("ultra", None)
        if u is not None and hasattr(u, "observe"):
            u.observe(topic.replace(".", "_"), **payload)
    except Exception:
            pass
    try:
        h = globals().get("holo", None)
        if h is not None and isinstance(getattr(h, "memory_log", None), list):
            h.memory_log.append(dict({"kind": topic, "ts": time.time()}, **payload))
    except Exception:
        pass

# ------------------------------------------------------
# ---- SymbolicUltraAgent ------------------------------
# ------------------------------------------------------
class SymbolicUltraAgent:
    def __init__(self, enable=True, run_id="na", phase="init"):
        self.enable = bool(enable)
        self.kb = SymbolicUltraKB()
        self.rng = random.Random(1337)
        self.run_id = str(run_id)
        self.phase = str(phase)
        self._emit = _UltraEmitter("SymbolicUltra", meta=None)

        # bounded telemetry weights (env-tunable)
        self.W_MIN = float(os.getenv("ULTRA_W_MIN", "0.8"))
        self.W_MAX = float(os.getenv("ULTRA_W_MAX", "1.2"))

        # Weight-blender v2 (EWMA state)
        self._w_ewma = 1.0
        self._w_alpha = float(os.getenv("ULTRA_W_EWMA_ALPHA", "0.15"))

        # I/O envelope metric (EWMA of write latency) + async writer toggle
        self._io_latency_ewma = 0.0
        self._io_alpha = float(os.getenv("ULTRA_IO_EWMA_ALPHA", "0.2"))
        self._async_writer = bool(int(os.getenv("ULTRA_ASYNC_WRITER", "0")))
        self._lock = threading.Lock()
        self._q = None
        self._writer_thread = None
        if self._async_writer:
            try:
                import queue
                self._q = queue.Queue(maxsize=int(os.getenv("ULTRA_ASYNC_QUEUE_MAX", "1024")))
                self._writer_thread = threading.Thread(target=self._drain_writer, daemon=True)
                self._writer_thread.start()
            except Exception:
                self._async_writer = False

    # ---------- phase & orchestration ----------
    def set_phase(self, phase: str):
        self.phase = str(phase)
        try:
            self.kb.phase_mark_begin()
        except Exception:
            pass
        try:
            # Phase proof-of-work: include bus checksum + Kairos Ω state
            omega = str(getattr(globals().get("kairos", None), "symbolic_state", "Ω?"))
            payload = {"phase": self.phase, "checksum": self._bus_checksum(), "omega": omega}
            self._append_phase_log("phase_begin", **payload)
        except Exception:
            pass

    # ---------- weight blender v2 ----------
    def _weight_blend(self, w_kairos: float, w_mem_gain: float, curiosity_pressure: float = 0.0) -> float:
        # curiosity_pressure expected in [0,1]; softly upweight up to +3%
        try:
            c_adj = 1.0 + 0.03 * float(max(0.0, min(1.0, curiosity_pressure)))
        except Exception:
            c_adj = 1.0
        raw = float(w_kairos) * float(w_mem_gain) * c_adj
        # EWMA smoothing then clamp
        self._w_ewma = (1.0 - self._w_alpha) * self._w_ewma + self._w_alpha * raw
        return float(max(self.W_MIN, min(self.W_MAX, self._w_ewma)))

    # ---------- calibration components ----------
    def _kairos_weight(self) -> float:
        try:
            flux = float(getattr(globals().get("kairos", None), "last_entropy_flux", 0.0))
            return float(1.0 + 0.05 * (np.tanh(flux / 25.0) if np is not None else 0.0))
        except Exception:
            return 1.0

    def _compression_gain(self) -> float:
        """
        Memory-only Keel policy:
        - Do NOT peek Keel metrics from Kairos.
        - Prefer a Memory shim if provided (e.g., memory_keel_gain()).
        - Default to neutral gain of 1.0.
        """
        try:
            getter = globals().get("memory_keel_gain", None)
            if callable(getter):
                return float(getter())
            # conservative fallback via Holo if exposed
            h = globals().get("holo", None)
            if h is not None and hasattr(h, "compression_ratio"):
                cr = float(getattr(h, "compression_ratio"))
                return float(1.0 + 0.02 * (np.tanh(cr - 1.0) if np is not None else 0.0))
        except Exception:
            pass
        return 1.0

    def _curiosity_pressure(self, meta: dict) -> float:
        """Optional curiosity/creativity pressure in [0,1], normalized if registry is present."""
        try:
            if "normalize_curiosity_tags" in globals() and callable(globals()["normalize_curiosity_tags"]):
                norm = globals()["normalize_curiosity_tags"](meta)  # type: ignore
                return float(max(0.0, min(1.0, norm.get("curiosity_pressure", 0.0))))
        except Exception:
            pass
        # heuristic fallback
        v = meta.get("curiosity_pressure", 0.0)
        try:
            return float(max(0.0, min(1.0, v)))
        except Exception:
            return 0.0

    # ---------- exports ----------
    def _paths(self) -> dict:
        base = _ultra_run_phase_dir(self.run_id, self.phase)
        return {
            "base": base,
            "summary": os.path.join(base, "symbolic_ultra_summary.json"),
            "facts_jsonl": os.path.join(base, "ultra_facts.jsonl"),
            "rollup_csv": os.path.join(base, "ultra_rollup.csv"),
            "phase_log": os.path.join(base, "phase_log.jsonl"),
            "conf_audit": os.path.join(base, "confidence_audit.jsonl"),
        }

    # ---------- async writer (optional) ----------
    def _drain_writer(self):
        # Non-blocking queue → atomic file appends by a single thread
        while True:
            try:
                path, line, header = self._q.get()
                self._atomic_append(path, line, header=header, _async_internal=True)
            except Exception:
                pass

    def _atomic_append(self, path: str, line: str, header: Optional[str] = None, _async_internal: bool = False):
        # optional async path: queue work instead of inline write
        if self._async_writer and not _async_internal:
            try:
                self._q.put_nowait((path, line, header))
                return
            except Exception:
                # fall back to sync if queue is full or unavailable
                pass

        t0 = time.time()
        tmp = path + ".tmp"
        with self._lock:
            try:
                exists = os.path.isfile(path) and os.path.getsize(path) > 0
                os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
                with open(tmp, "a", encoding="utf-8") as f:
                    if not exists and header:
                        f.write(header + "\n")
                    f.write(line + "\n")
                os.replace(tmp, path)
                self._emit.emit("ultra.atomic_write", path=path)
            except Exception:
                pass
        # update IO EWMA
        try:
            dt = max(0.0, time.time() - t0)
            self._io_latency_ewma = (1.0 - self._io_alpha) * self._io_latency_ewma + self._io_alpha * dt
        except Exception:
            pass

    def _export_append_jsonl(self, rec: dict):
        try:
            p = self._paths()["facts_jsonl"]
            _rotate_if_big(p, 2_000_000)
            rec["schema_version"] = "ultra_facts.v2"
            line = json.dumps(rec, ensure_ascii=False)
            self._atomic_append(p, line, header="# schema: ultra_facts.v2")
        except Exception:
            pass

    def _export_rollup_row(self, kind: str, w: float, meta: dict):
        try:
            p = self._paths()["rollup_csv"]
            exists = os.path.isfile(p) and os.path.getsize(p) > 0
            tmp = p + ".tmp"
            with self._lock:
                with open(tmp, "a", newline="", encoding="utf-8") as f:
                    wtr = csv.writer(f)
                    if not exists:
                        wtr.writerow(["schema_version", "t", "phase", "kind", "w",
                                      "keys_count", "meta_size", "io_latency_ewma"])
                    wtr.writerow([
                        "ultra_rollup.v2",
                        int(time.time()),
                        self.phase,
                        kind,
                        float(w),
                        int(len(meta or {})),
                        int(len(json.dumps(meta, ensure_ascii=False)) if isinstance(meta, dict) else 0),
                        float(self._io_latency_ewma),
                    ])
                os.replace(tmp, p)
            # feed IO envelope up to Kairos (if available)
            try:
                k = globals().get("kairos", None)
                if k is not None and hasattr(k, "update_system_health"):
                    k.update_system_health(io_pressure=float(self._io_latency_ewma))
            except Exception:
                pass
        except Exception:
            pass

    # ---------- observe API ----------
    def observe(self, kind: str, **meta):
        if not self.enable:
            return
        try:
            # normalize curiosity/creativity tags if registry is available
            try:
                if "normalize_curiosity_tags" in globals() and callable(globals()["normalize_curiosity_tags"]):
                    meta = dict(globals()["normalize_curiosity_tags"](meta))  # type: ignore
            except Exception:
                pass

            smeta = _ultra_sanitize_meta(meta)
            w_k = self._kairos_weight()
            w_c = self._compression_gain()  # from Memory shim or neutral
            c_p = self._curiosity_pressure(meta)
            w_final = self._weight_blend(w_k, w_c, c_p)

            # flat fact string (phase:kind|k=v|...)
            keys = sorted(smeta.keys())
            fact = f"{self.phase}:{kind}"
            if keys:
                fact += "|" + "|".join([f"{k}={smeta[k]}" for k in keys])

            # structured insert
            rec = self.kb.assert_fact(
                fact,
                weight=w_final,
                meta=dict(meta) if isinstance(meta, dict) else {},
                w_kairos=w_k,
                w_keel=w_c,  # schema-compatible; sourced from memory shim
                w_curiosity=c_p,
                w_final=w_final,
            )

            # telemetry via emitter (downsampled upstream)
            self._emit.emit("ultra.observe", phase=self.phase, kind=kind, w=float(w_final))

            # facts JSONL
            self._export_append_jsonl({
                "t": rec.get("t_event", time.time()),
                "phase": self.phase,
                "kind": kind,
                "w_kairos": float(w_k),
                "w_keel": float(w_c),
                "w_curiosity": float(c_p),
                "w_final": float(rec.get("w_final", w_final)),
                "meta": dict(meta) if isinstance(meta, dict) else {},
            })

            # tiny rollup CSV (+ io envelope)
            self._export_rollup_row(kind, w_final, meta)

        except Exception:
            pass

    def observe_tile(self, y: int, x: int, method: int, qidx: int, var: float, zlen: int):
        if not self.enable:
            return
        meth = "haar" if method == 0 else "fractal"
        self.observe("tile", y=int(y), x=int(x), meth=meth, qidx=int(qidx), var=float(var), zlen=int(zlen))

    def observe_pulse(self, **meta):
        self.observe("pulse", **meta)

    # ---------- state & exports ----------
    def emit_state(self) -> dict:
        state = self.summarize()
        try:
            ext = self.kb.summarize_extended()
            state.update(ext)
        except Exception:
            pass
        state["run_id"] = self.run_id
        state["phase"] = self.phase
        state["io_latency_ewma"] = float(self._io_latency_ewma)
        return state

    def summarize(self) -> dict:
        if not self.enable:
            return {}
        base = self.kb.summarize()
        base.update({"run_id": self.run_id, "phase": self.phase})
        return base

    def export_json(self, path: Optional[str] = None):
        try:
            p = path if isinstance(path, str) else self._paths()["summary"]
            data = self.emit_state()
            tmp = p + ".tmp"
            with open(tmp, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2)
            os.replace(tmp, p)
            self._emit.emit("ultra.summary_written", path=p)
        except Exception:
            pass

    # ---------- helpers ----------
    def _bus_checksum(self) -> str:
        try:
            ex = globals().get("EXPLAIN", None)
            ml = globals().get("meta_log", None)
            kr = globals().get("kairos", None)
            s = f"{id(ex)}|{id(ml)}|{id(kr)}"
            return hashlib.sha1(s.encode("utf-8")).hexdigest()
        except Exception:
            return "na"

    def _append_phase_log(self, kind: str, **payload):
        try:
            p = self._paths()["phase_log"]
            _rotate_if_big(p, 2_000_000)
            line = json.dumps({"t": time.time(), "kind": kind, **payload}, ensure_ascii=False)
            self._atomic_append(p, line, header="# schema: ultra_phase_log.v1")
        except Exception:
            pass



# ==========================================================
# HoloMemory (strict)
# ==========================================================
class HoloMemory:
    ENABLE_VIS_HOLO_HISTORY = True
    ENABLE_VIS_HOLO_PCA     = True

    # maintenance knobs
    DECAY_INTERVAL_SEC      = 30.0
    MIN_CONF_FOR_KEEP       = 0.03
    CONSOLIDATION_BONUS     = 0.06
    CONSOLIDATION_MAX_BOOST = 0.30
    ATTR_ALPHA              = 0.20
    ATTR_DECAY_LAMBDA       = 0.002

    # per-basin soft-cap (prevents basin explosion); can be tuned
    PER_BASIN_MAX = 256

    def __init__(self,
                 max_items: int = 10264,
                 decay_rate: float = 0.001,
                 enable_kairos: bool = True,
                 keel_q_ll: float = 3.0,
                 recall_scoring: Optional[RecallScoring] = None,
                 prune_policy: Optional[PrunePolicy] = None,
                 admission_threshold: float = 0.0,
                 kairos: Optional['KairosPulseManager'] = None):
        # Validate globals up front — fail fast if wiring is off.
        _require_globals_for_holo()
        _capabilities_snapshot()

        # Legacy episodic store (explicitly required by upstream code)
        self.keys: List[Tuple[int, ...]] = []
        self.vals: List[np.ndarray] = []
        self.tags: List[Dict[str, Any]] = []
        self.max_items = int(max_items)
        self.history_size: List[int] = []
        self._unique_hashes: set = set()
        self.decay_rate = float(decay_rate)

        # RHCM attractors per output shape
        self.attractors: Dict[Tuple[int,int], Dict[str, Any]] = {}

        # Basins keyed by (subject, (R,C))
        self.basins: Dict[Tuple[str, Tuple[int,int]], List[Dict[str, Any]]] = defaultdict(list)

        # Analytics
        self.subject_depth: Dict[str, Counter] = defaultdict(lambda: Counter())

        # Meta-rule registry (xform chains)
        self.meta_rules: List[Dict[str, Any]] = []

        # Consolidation index
        self._pair_index: Dict[Tuple[Tuple[int,int], str, str], Dict[str, Any]] = {}

        # Maintenance
        self._last_decay_ts = time.time()

        # Provenance bonuses
        self._prov_bonus = _load_prov_bonus()

        # Counters
        self._n_add = 0; self._n_reinforce = 0; self._n_dedup = 0; self._n_get = 0

        # KEEL defaults (Memory-owned)
        self.keel_q_ll = float(keel_q_ll)
        self.compression_ratio = 0.0
        self.keel_ratio_history = deque(maxlen=256)

        # Kairos pulse — prefer injected instance, else global
        self.kairos = kairos if kairos is not None else (globals().get("kairos") if enable_kairos else None)
        self.kairos_flux_history = deque(maxlen=256)
        self._last_flux = 0.0

        # Recall scoring + pruning policy + admission control
        self.recall_scoring = recall_scoring or RecallScoring()
        self.prune_policy = prune_policy or PrunePolicy()
        self.admission_threshold = float(admission_threshold)

        meta_log("holo.init.strict",
                 items=self.max_items, decay=self.decay_rate,
                 has_phys=bool(_HOLO_HAS_PHYS), keel_q_ll=self.keel_q_ll,
                 kairos=bool(self.kairos is not None), prune=self.prune_policy.mode)

    # ---------------------------
    # Telemetry (unified fan-out if available)
    # ---------------------------
    def _telemetry(self, kind: str, **payload):
        # Prefer the unified _safe_emit if present; fallback to meta_log.
        try:
            _safe_emit(f"holo.{kind}", payload)  # type: ignore[name-defined]
        except Exception:
            try:
                meta_log(f"holo.{kind}", **payload)
            except Exception:
                pass

    # ---------------------------
    # Wiring verification (self-healing test harness)
    # ---------------------------
    def verify_wiring(self, strict: bool = True) -> Dict[str, Any]:
        report = {"ts": _now(), "passed": True, "checks": {}}
        # KEEL round-trip
        try:
            g = (np.arange(64, dtype=np.uint8).reshape(8,8) * 3) % 255
            blob, meta = keel_compress_grid(g, q_ll=self.keel_q_ll, deblock=True)
            rec = keel_decompress_grid(blob)
            km = keel_metrics(g, rec)
            ok = (rec.shape == g.shape) and (km.get("psnr", 0.0) >= 15.0)
            report["checks"]["keel"] = {"ok": bool(ok), "psnr": float(km.get("psnr", 0.0))}
            report["passed"] &= ok
        except Exception as e:
            report["checks"]["keel"] = {"ok": False, "error": str(e)}; report["passed"] = False
        # Kairos step (soft)
        try:
            if self.kairos is not None and hasattr(self.kairos, "step"):
                self.kairos.step(max(1, int(getattr(self.kairos, "phase_time", 0)) + 1))
                report["checks"]["kairos"] = {"ok": True}
            else:
                report["checks"]["kairos"] = {"ok": True, "note": "disabled"}
        except Exception as e:
            report["checks"]["kairos"] = {"ok": False, "error": str(e)}; report["passed"] = False
        # apply_ops identity sanity
        try:
            ident = np.eye(4, dtype=int)
            out = apply_ops(ident, [])
            ok = np.array_equal(out, ident)
            report["checks"]["apply_ops"] = {"ok": bool(ok)}; report["passed"] &= ok
        except Exception as e:
            report["checks"]["apply_ops"] = {"ok": False, "error": str(e)}; report["passed"] = False
        # invariants presence if enabled
        if _HOLO_HAS_PHYS:
            try:
                s = compute_invariants(np.zeros((4,4), dtype=int))
                ok = hasattr(s, "glyph_id")
                report["checks"]["invariants"] = {"ok": bool(ok)}; report["passed"] &= ok
            except Exception as e:
                report["checks"]["invariants"] = {"ok": False, "error": str(e)}; report["passed"] = False
        # persist
        try:
            os.makedirs("deployment", exist_ok=True)
            with open("deployment/holo_wiring_report.json", "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2)
        except Exception:
            pass
        if strict and not report["passed"]:
            raise RuntimeError(f"[HoloMemory.verify_wiring] failed: {report}")
        return report

    # =========================================================
    # Meta-rules (contract-aware)
    # =========================================================
    def _maybe_register_rule(self, tags: dict):
        ops = None
        if isinstance(tags, dict):
            ops = tags.get("ops") or tags.get("chain") or tags.get("xform_ops")
        if not ops:
            return

        # If a typed Rule is available, validate and normalize
        try:
            RuleT = globals().get("Rule", None)
            if RuleT is not None and hasattr(RuleT, "validate"):
                RuleT.validate({"kind": "kb_xform", "params": {"ops": ops}})
        except Exception:
            pass

        # Stable tuple signature (until full typed Rule adoption is complete)
        norm_ops = []
        for name, kw in ops:
            try:
                stable = tuple(sorted((k, json.dumps(v, sort_keys=True)) for k, v in (kw or {}).items()))
            except Exception:
                stable = tuple(sorted((k, str(v)) for k, v in (kw or {}).items()))
            norm_ops.append((name, stable))
        sig = tuple(norm_ops)

        for mr in self.meta_rules:
            if mr.get("sig") == sig:
                mr["count"] = int(mr.get("count", 0)) + 1
                mr["last_ts"] = time.time()
                mr["confidence"] = min(1.0, float(mr.get("confidence", 0.6)) + 0.02)
                self._telemetry("meta_rule_hit", sig_len=len(sig), count=int(mr["count"]), conf=float(mr["confidence"]))
                return
        self.meta_rules.append({"ops": ops, "sig": sig, "count": 1, "last_ts": time.time(), "confidence": float(tags.get("confidence", 0.6))})
        self._telemetry("meta_rule_register", ops_len=len(ops), confidence=float(tags.get("confidence", 0.6)))

    # =========================================================
    # Basin helper (strict) + glyph-aware dedup (JSON-safe)
    # =========================================================
    def _add_to_basin(self, inp: np.ndarray, out: np.ndarray, tag: dict):
        subject = (tag or {}).get("subject", "generic")
        depth = int((tag or {}).get("depth", 0))
        shape = tuple(inp.shape)
        entry = {
            "sig": self._sig(inp),
            "out": np.asarray(out, dtype=int).copy(),
            "ts": time.time(),
            "confidence": float((tag or {}).get("confidence", 0.5)),
            "pca_sig": tag.get("pca_sig", self._pca_projection(inp)),
            "tags": dict(tag or {})
        }

        # capacity guard across all basins
        total_items = sum(len(v) for v in self.basins.values())
        if total_items >= self.max_items:
            worst_key, worst_idx, worst_score = None, None, float('inf')
            for key, lst in self.basins.items():
                for i, e in enumerate(lst):
                    score = float(e["confidence"]) * self._temporal_weight(float(e["ts"]))
                    if score < worst_score:
                        worst_score = score; worst_key = key; worst_idx = i
            if worst_key is not None:
                self.basins[worst_key].pop(worst_idx)
                self._telemetry("evict_basin", basin=str(worst_key), index=int(worst_idx))

        # enforce per-basin soft-cap
        B = self.basins[(subject, shape)]
        if len(B) >= self.PER_BASIN_MAX:
            worst_idx, worst_score = None, float('inf')
            for i, e in enumerate(B):
                score = float(e["confidence"]) * self._temporal_weight(float(e["ts"]))
                if score < worst_score:
                    worst_idx, worst_score = i, score
            if worst_idx is not None:
                B.pop(worst_idx)
                self._telemetry("evict_basin_local", basin=str((subject, shape)), index=int(worst_idx))

        for e in B:
            if e["sig"] == entry["sig"]:
                # glyph-aware merge (lists, JSON-safe)
                gi_old = e["tags"].get("glyph_in"); gi_new = entry["tags"].get("glyph_in")
                go_old = e["tags"].get("glyph_out"); go_new = entry["tags"].get("glyph_out")
                if gi_new and gi_new != gi_old:
                    _merge_tag_list(e["tags"], "glyph_in_list", gi_new)
                    if gi_old: _merge_tag_list(e["tags"], "glyph_in_list", gi_old)
                if go_new and go_new != go_old:
                    _merge_tag_list(e["tags"], "glyph_out_list", go_new)
                    if go_old: _merge_tag_list(e["tags"], "glyph_out_list", go_old)
                e["confidence"] = min(1.0, e["confidence"] + 0.05)
                e["ts"] = time.time()
                self.subject_depth[subject][depth] += 1
                self._telemetry("add_basin_duplicate", subject=subject, shape=shape)
                return
        B.append(entry); self.subject_depth[subject][depth] += 1
        self._telemetry("add_basin_new", subject=subject, shape=shape)

    # =========================================================
    # Pair consolidation index
    # =========================================================
    def _index_pair(self, shape: Tuple[int,int], sig_in: Tuple[int,...], sig_out: Tuple[int,...], success: bool):
        key = (shape, self._sha_sig(sig_in), self._sha_sig(sig_out))
        rec = self._pair_index.get(key, {"count": 0, "last_ts": 0.0})
        if success:
            rec["count"] = int(rec.get("count", 0)) + 1
            rec["last_ts"] = time.time()
            self._pair_index[key] = rec
            A = self.attractors.get(shape)
            if A is not None:
                A["consolidation"] = min(1.0, float(A.get("consolidation", 0.0)) + self.CONSOLIDATION_BONUS)
                try:
                    # inverse update via RHCM section (no duplication)
                    n = max(2, min(shape))
                    A["proto"] = 0.5 * A["proto"] + 0.5 * rhcm.inverse(A["proto"], n)  # type: ignore[name-defined]
                except Exception:
                    pass
                self.attractors[shape] = A
                self._telemetry("consolidate_pair", shape=shape, count=rec["count"])

    # =========================================================
    # Admission scoring (global backpressure)
    # =========================================================
    def _admission_score(self, conf: float, origin: str, load: float) -> float:
        bonus = float(self._prov_bonus.get(origin, 0.0))
        # flux pressure: slightly raise bar when flux is wild
        try:
            flux = float(self.kairos.last_entropy_flux) if self.kairos is not None else 0.0
            flux_pen = 0.02 * np.tanh(abs(flux) / 20.0)
        except Exception:
            flux_pen = 0.0
        return float(conf * (1.0 + bonus) * (1.0 - min(0.9, load)) * (1.0 - flux_pen))

    def _provenance_bonus(self, origin: str) -> float:
        try:
            return float(self._prov_bonus.get(str(origin) or "generic", 0.0))
        except Exception:
            return 0.0

    # =========================================================
    # Add (public) — strict, with KEEL + Kairos + invariants (if enabled)
    # =========================================================
    def add(self, inp: np.ndarray, out: np.ndarray, meta: dict):
        sig = tuple(np.asarray(inp, dtype=int).ravel().tolist())
        h = hash(sig)

        # Curiosity taxonomy normalization if available
        try:
            if "normalize_curiosity_tags" in globals() and callable(globals()["normalize_curiosity_tags"]):
                meta = dict(globals()["normalize_curiosity_tags"](meta))  # type: ignore
        except Exception:
            pass

        # pca signature for retrieval
        pca_sig = self._pca_projection(inp)

        tag = dict(meta or {})
        tag.setdefault("confidence", 0.5)
        tag.setdefault("origin", _norm_origin(meta))
        tag["timestamp"] = time.time()
        tag["pca_sig"] = pca_sig

        # Kairos pulse (soft): step monotonic time if available
        try:
            if self.kairos is not None and hasattr(self.kairos, "step"):
                self.kairos.step(max(1, int(getattr(self.kairos, "phase_time", 0)) + 1))
                k_flux = abs(getattr(self.kairos, "last_entropy_flux", 0.0))
                self._last_flux = float(k_flux)
                self.kairos_flux_history.append(self._last_flux)
                tag["kairos_flux"] = float(k_flux)
        except Exception:
            pass

        # KEEL sampling (Memory-owned)
        _inp_u8  = np.clip(np.asarray(inp, dtype=np.int32), 0, 255).astype(np.uint8)
        _out_u8  = np.clip(np.asarray(out, dtype=np.int32), 0, 255).astype(np.uint8)
        blob_in,  meta_in  = keel_compress_grid(_inp_u8, q_ll=float(tag.get("keel_q_ll", self.keel_q_ll)), deblock=True)
        blob_out, meta_out = keel_compress_grid(_out_u8, q_ll=float(tag.get("keel_q_ll", self.keel_q_ll)), deblock=True)
        keel_ratio_in  = float((_inp_u8.size) / max(1, len(blob_in)))
        keel_ratio_out = float((_out_u8.size) / max(1, len(blob_out)))
        tag["keel_ratio_in"]  = keel_ratio_in
        tag["keel_ratio_out"] = keel_ratio_out
        rec_out = keel_decompress_grid(blob_out)
        km = keel_metrics(_out_u8, rec_out)
        tag["keel_psnr_out"] = float(km.get("psnr", 0.0))
        tag["keel_ssim_out"] = float(km.get("ssim_proxy", 0.0))
        self.compression_ratio = float((_inp_u8.size + _out_u8.size) / max(1, len(blob_in) + len(blob_out)))
        self.keel_ratio_history.append(self.compression_ratio)
        self._telemetry("keel.sample", rin=keel_ratio_in, rout=keel_ratio_out,
                        psnr=tag["keel_psnr_out"], ssim=tag["keel_ssim_out"])

        # Feed generic health back to Kairos (no Keel API dependency)
        try:
            if self.kairos is not None and hasattr(self.kairos, "update_system_health"):
                self.kairos.update_system_health(mem_gain=float(self.compression_ratio))
        except Exception:
            pass

        # Invariants (dual-path)
        if _HOLO_HAS_PHYS:
            try:
                sig_in  = compute_invariants(_grid(inp))
                sig_out = compute_invariants(_grid(out))
                tag["glyph_in"]  = getattr(sig_in, "glyph_id", None)
                tag["glyph_out"] = getattr(sig_out, "glyph_id", None)
                tag["epi_in"]    = float(getattr(sig_in, "epi", 0.0))
                tag["epi_out"]   = float(getattr(sig_out, "epi", 0.0))
                tag["binder_in"] = float(getattr(sig_in, "binder_last", getattr(sig_in, "binder", 0.0)))
                tag["binder_out"]= float(getattr(sig_out, "binder_last", getattr(sig_out, "binder", 0.0)))
            except Exception:
                pass
        else:
            # deterministic fallback invariant to keep structure present
            tag.setdefault("glyph_in",  None)
            tag.setdefault("glyph_out", None)
            tag.setdefault("epi_in",    0.0)
            tag.setdefault("epi_out",   0.0)
            tag.setdefault("binder_in", 0.0)
            tag.setdefault("binder_out",0.0)

        # RHCM attractor update (delegated to RHCM section — no local defs)
        shape = tuple(getattr(inp, "shape", (1, 1)))
        n = max(2, min(shape))
        try:
            seq = rhcm.feedback(n=n, depth=tag.get("rchm_depth", 1.618),
                                seed=tag.get("rchm_seed", 42),
                                iterations=int(tag.get("rchm_iter", 3)),
                                continuous=bool(tag.get("rchm_continuous", False)))  # type: ignore[name-defined]
        except Exception:
            seq = []

        try:
            if seq:
                proto = seq[-1]
            else:
                proto = rhcm.gen_cRHCM(n) if tag.get("rchm_continuous", False) else rhcm.gen_RHCM(n)  # type: ignore[name-defined]
        except Exception:
            proto = np.zeros((n, n))

        attract = self.attractors.get(shape, {"proto": proto, "last_ts": time.time(), "count": 0, "consolidation": 0.0})
        if "proto" in attract and getattr(attract["proto"], "shape", None) == getattr(proto, "shape", None):
            attract["proto"] = (1.0 - self.ATTR_ALPHA) * attract["proto"] + self.ATTR_ALPHA * proto
        else:
            attract["proto"] = proto
        attract["count"] = int(attract.get("count", 0)) + 1
        attract["last_ts"] = time.time()

        # feedback metrics via RHCM utilities
        try:
            if seq:
                std_list = rhcm.track_std(seq)          # type: ignore[name-defined]
                uniq_list = rhcm.track_unique(seq)      # type: ignore[name-defined]
                epi = rhcm.echo_persistence(seq, seq[-1])  # type: ignore[name-defined]
                attract["feedback_metrics"] = {"std": std_list[-1], "uniq": uniq_list[-1], "epi": epi}
                tag["rchm_std"] = float(std_list[-1]); tag["rchm_uniq"] = int(uniq_list[-1]); tag["rchm_epi"] = float(epi)
        except Exception:
            pass

        # Attractor viz budget: emit only if eigendrift exceeds threshold
        try:
            eigs = rhcm.eigenspectrum(attract["proto"])  # type: ignore[name-defined]
            eigs = np.asarray(eigs).real[:8]
            prev = np.asarray(attract.get("eigens", np.zeros_like(eigs)))
            drift = float(np.linalg.norm(eigs - prev) / (np.linalg.norm(prev) + 1e-6))
            attract["eigens"] = [float(x) for x in eigs]
            drift_thresh = float(os.getenv("HOLO_EIGENDRIFT_THRESH", "0.25"))
            if drift > drift_thresh:
                # lightweight PNG budget if a helper exists (no matplotlib requirement here)
                try:
                    if "save_attractor_png" in globals() and callable(globals()["save_attractor_png"]):
                        path = f"deployment/attractors/attr_{shape[0]}x{shape[1]}_{int(time.time())}.png"
                        os.makedirs(os.path.dirname(path), exist_ok=True)
                        globals()["save_attractor_png"](attract["proto"], path)  # type: ignore
                        self._telemetry("attractor_png", path=path, drift=drift)
                except Exception:
                    pass
        except Exception:
            pass

        self.attractors[shape] = attract
        self._telemetry("attractor_update", shape=shape, count=int(attract.get("count", 0)),
                        std=float(tag.get("rchm_std", 0.0)), uniq=int(tag.get("rchm_uniq", 0)), epi=float(tag.get("rchm_epi", 0.0)))

        # Admission control (global backpressure)
        load = min(1.0, (len(self.keys) + sum(len(v) for v in self.basins.values())) / float(max(1, self.max_items)))
        adm = self._admission_score(float(tag["confidence"]), tag["origin"], load)
        if adm < self.admission_threshold:
            self._add_to_basin(inp, out, tag)
            self._telemetry("admission_drop", load=float(load), threshold=float(self.admission_threshold), score=float(adm))
            return

        # Duplicate in legacy
        for i, k in enumerate(self.keys):
            if k == sig:
                self.update_confidence(i, success=True, bump=None)
                self._add_to_basin(inp, out, tag)
                self._maybe_register_rule(tag)
                self.tags.append(tag); self.history_size.append(len(self.keys)); self._unique_hashes.add(h)
                self._index_pair(shape, sig, tuple(np.asarray(out, dtype=int).ravel().tolist()), success=True)
                self._telemetry("add_duplicate", shape=shape, subject=tag.get("subject", "generic"))
                self._maybe_decay_maintenance(); self._n_reinforce += 1; self._n_dedup += 1
                return

        # Evict legacy if full (policy)
        if len(self.keys) >= self.max_items and self.tags:
            mode = self.prune_policy.mode
            if mode in ("default", "confidence"):
                worst_idx = min(range(len(self.tags)), key=lambda i: (self.tags[i].get("confidence", 0.0),
                                                                      -len(set(self.keys[i])) if i < len(self.keys) else 0))
            elif mode == "fifo":
                worst_idx = 0
            elif mode == "lru":
                worst_idx = min(range(len(self.tags)), key=lambda i: self.tags[i].get("timestamp", time.time()))
            else:
                worst_idx = min(range(len(self.tags)), key=lambda i: (self.tags[i].get("confidence", 0.0)))
            self.keys.pop(worst_idx); self.vals.pop(worst_idx); self.tags.pop(worst_idx)
            self._unique_hashes = {hash(k) for k in self.keys}
            self._telemetry("evict_legacy", index=int(worst_idx), mode=mode)

        # Persist legacy
        self.keys.append(sig)
        self.vals.append(np.asarray(out, dtype=int).copy())
        self.tags.append(tag)
        self._unique_hashes.add(h)
        self.history_size.append(len(self.keys))
        self._telemetry("add_legacy", shape=shape, subject=tag.get("subject", "generic"))

        # Basins + meta-reg + pair index
        self._add_to_basin(inp, out, tag)
        self._maybe_register_rule(tag)
        self._index_pair(shape, sig, tuple(np.asarray(out, dtype=int).ravel().tolist()), success=True)
        self._maybe_decay_maintenance()
        self._n_add += 1

    # =========================================================
    # Get (public)
    # =========================================================
    @staticmethod
    def _sig(grid: np.ndarray) -> Tuple[int, ...]:
        return tuple(np.asarray(grid, dtype=int).ravel().tolist())

    @staticmethod
    def _sha_sig(sig: Tuple[int, ...]) -> str:
        h = hashlib.sha1(); h.update(np.asarray(sig, dtype=np.int32).tobytes()); return h.hexdigest()[:20]

    @staticmethod
    def _sig_hash(sig: Tuple[int, ...]) -> int: return hash(sig)

    @staticmethod
    def _hamming_same_len(sig1: Tuple[int, ...], sig2: Tuple[int, ...]) -> float:
        if not sig1: return 1.0
        diff = sum(a != b for a, b in zip(sig1, sig2)); return diff / float(len(sig1))

    @staticmethod
    def _cross_len_distance(sig1: Tuple[int, ...], sig2: Tuple[int, ...]) -> float:
        L1, L2 = len(sig1), len(sig2)
        if L1 == 0 and L2 == 0: return 0.0
        if L1 == 0 or L2 == 0:  return 1.0 + abs(L1 - L2) / max(1, max(L1, L2))
        m = min(L1, L2)
        base = (sum(sig1[i] != sig2[i] for i in range(m)) / float(m)) if m else 1.0
        length_pen = abs(L1 - L2) / float(max(L1, L2))
        return base + 0.5 * length_pen

    @staticmethod
    def _pca_projection(grid: np.ndarray, n_components: int = 8) -> np.ndarray:
        flat = np.asarray(grid, dtype=float).ravel(); stride = max(1, len(flat) // n_components)
        return np.array([np.mean(flat[i:i+stride]) for i in range(0, len(flat), stride)][:n_components])

    @staticmethod
    def _pca_distance(v1: np.ndarray, v2: np.ndarray) -> float:
        v1 = np.asarray(v1); v2 = np.asarray(v2)
        if v1.shape != v2.shape:
            m = min(len(v1), len(v2)); v1, v2 = v1[:m], v2[:m]
        return float(np.linalg.norm(v1 - v2) / (np.linalg.norm(v1) + 1e-6))

    def _temporal_weight(self, ts: float) -> float:
        age = time.time() - ts
        return float(np.exp(-self.decay_rate * age))

    def _nearest_in_basin(self, basin_list, sig, pca_sig, topk):
        scored = []
        for i, e in enumerate(basin_list):
            k = e["sig"]
            d = self._hamming_same_len(sig, k) if len(k) == len(sig) else self._cross_len_distance(sig, k)
            pca_d = self._pca_distance(pca_sig, e.get("pca_sig", pca_sig))
            td = self._temporal_weight(e.get("ts", time.time()))
            combined = (0.5 * d + 0.5 * pca_d) * (1.0 / max(td, 1e-6))
            scored.append((combined, i))
        scored.sort(key=lambda t: t[0])
        return scored[:max(1, topk)]

    def _apply_meta_rules(self, inp):
        preds = []
        for mr in self.meta_rules:
            pred = apply_ops(inp, mr["ops"])  # strict: let exceptions bubble
            conf = float(mr.get("confidence", 0.5))
            preds.append((pred, mr, conf))
        preds.sort(key=lambda t: t[2], reverse=True)
        return preds

    def get(self, inp: np.ndarray, topk: int = 1, _meta_tau: float = 0.25) -> List[Tuple[np.ndarray, dict, float]]:
        results: List[Tuple[np.ndarray, dict, float]] = []
        if not (self.keys or self.basins or self.meta_rules):
            return results
        sig = self._sig(inp); pca_sig = self._pca_projection(inp); shape = tuple(inp.shape)

        # Exact legacy
        for i, k in enumerate(self.keys):
            if k == sig:
                self.update_confidence(i, success=True)
                self.tags[i]["distance"] = 0.0
                self._telemetry("get_hit_legacy_exact", shape=shape)
                self._maybe_decay_maintenance(); self._n_get += 1
                return [(self.vals[i], self.tags[i], 0.0)]

        # Basin scan
        cand: List[Tuple[np.ndarray, dict, float]] = []
        for (subject, shp), lst in self.basins.items():
            if shp != shape: continue
            for e in lst:
                if e["sig"] == sig:
                    e["confidence"] = min(1.0, e["confidence"] + 0.05)
                    e["ts"] = time.time()
                    meta = dict(e["tags"]); meta["subject"] = subject; meta["basin_shape"] = shp; meta["distance"] = 0.0
                    self._telemetry("get_hit_basin_exact", subject=subject, shape=shape)
                    self._maybe_decay_maintenance(); self._n_get += 1
                    return [(e["out"], meta, 0.0)]
            top = self._nearest_in_basin(lst, sig, pca_sig, max(1, topk))
            for score, idx in top:
                e = lst[idx]
                meta = dict(e["tags"]); meta["subject"] = subject; meta["basin_shape"] = shp; meta["distance"] = float(score)
                cand.append((e["out"], meta, float(score)))
        if cand: self._telemetry("get_candidates_basin", shape=shape, n=len(cand))

        # Legacy approximate
        scored_legacy: List[Tuple[float, int]] = []
        for i, k in enumerate(self.keys):
            d = self._hamming_same_len(sig, k) if len(k) == len(sig) else self._cross_len_distance(sig, k)
            pca_d = self._pca_distance(pca_sig, self.tags[i].get("pca_sig", pca_sig))
            td = self._temporal_weight(self.tags[i].get("timestamp", time.time()))
            score = (0.5 * d + 0.5 * pca_d) * (1.0 / max(td, 1e-6))
            scored_legacy.append((score, i))
        scored_legacy.sort(key=lambda t: t[0])
        for j in range(min(topk, len(scored_legacy))):
            sc, i = scored_legacy[j]
            meta = dict(self.tags[i]); meta["distance"] = float(sc); meta["subject"] = meta.get("subject", "generic"); meta["basin_shape"] = shape
            cand.append((self.vals[i], meta, float(sc)))
        if scored_legacy: self._telemetry("get_candidates_legacy", shape=shape, n=min(topk, len(scored_legacy)))

        # Meta rules if weak/none
        cand.sort(key=lambda t: t[2])
        best_dist = cand[0][2] if cand else 1.0
        if (not cand) or (best_dist > _meta_tau):
            meta_preds = self._apply_meta_rules(inp)
            meta_results = []
            for pred, mr, conf in meta_preds[:max(1, topk * 3)]:
                low_conf = max(0.35, min(0.95, float(conf)))
                self.add(inp, pred, {"subject": "generic", "depth": 1, "confidence": low_conf, "rule_kind": "kb_xform", "ops": mr["ops"]})
                meta_results.append((pred, {"rule_kind": "kb_xform", "ops": mr["ops"], "confidence": low_conf, "subject": "generic", "basin_shape": shape}, 1.0 - low_conf))
            if meta_results:
                meta_results.sort(key=lambda t: t[2])
                self._telemetry("get_meta_rule_used", shape=shape, n=len(meta_results))
                self._maybe_decay_maintenance(); self._n_get += 1
                return meta_results[:topk]

        if cand: self._telemetry("get_return_candidates", shape=shape, n=len(cand))
        else:    self._telemetry("get_empty", shape=shape)
        self._maybe_decay_maintenance(); self._n_get += 1
        return cand[:topk] if cand else []

    # =========================================================
    # Confidence dynamics (decay + consolidation with RHCM inverse)
    # =========================================================
    def update_confidence(self, idx: int, success: bool, bump: float = None):
        if not (0 <= idx < len(self.tags)):
            raise IndexError(f"[HoloMemory.update_confidence] idx out of range: {idx}")
        cur = float(self.tags[idx].get("confidence", 0.5))
        ts  = float(self.tags[idx].get("timestamp", time.time()))
        age = max(0.0, time.time() - ts)
        cur *= float(np.exp(-self.decay_rate * age))  # forgetting
        try:
            out_shape = tuple(getattr(self.vals[idx], "shape", ()))
            in_sig    = self.keys[idx]
            out_sig   = tuple(np.asarray(self.vals[idx], dtype=int).ravel().tolist())
            key = (out_shape, self._sha_sig(in_sig), self._sha_sig(out_sig))
            seen = self._pair_index.get(key, {}).get("count", 0)
            if seen > 0:
                boost = min(self.CONSOLIDATION_MAX_BOOST, self.CONSOLIDATION_BONUS * np.log1p(seen))
                cur = min(1.0, cur + boost)
                A = self.attractors.get(out_shape)
                if A is not None and "proto" in A:
                    try:
                        n = max(2, min(out_shape))
                        A["proto"] = 0.5 * A["proto"] + 0.5 * rhcm.inverse(A["proto"], n)  # type: ignore[name-defined]
                        self.attractors[out_shape] = A
                    except Exception:
                        pass
        except Exception:
            pass
        delta = bump if bump is not None else (0.1 if success else -0.1)
        cur = float(min(1.0, max(0.0, cur + delta)))
        self.tags[idx]["confidence"] = cur
        self.tags[idx]["timestamp"]  = time.time()
        self._telemetry("confidence_update", idx=int(idx), conf=float(cur), success=bool(success))

        # Confidence audit trail (rolling JSONL)
        try:
            p = os.path.join(_ultra_run_phase_dir("holo", "audit"), "confidence_audit.jsonl")
            os.makedirs(os.path.dirname(p), exist_ok=True)
            line = json.dumps({
                "t": time.time(), "idx": int(idx), "success": bool(success),
                "delta": float(delta), "conf": float(cur),
            }, ensure_ascii=False)
            with open(p, "a", encoding="utf-8") as f:
                f.write(line + "\n")
        except Exception:
            pass

    # =========================================================
    # Periodic maintenance (decay + prune + attractor drift)
    # =========================================================
    def _maybe_decay_maintenance(self):
        now = time.time()
        if (now - self._last_decay_ts) < self.DECAY_INTERVAL_SEC: return
        self._last_decay_ts = now

        # Legacy decay + prune
        keep_idx = []
        for i, t in enumerate(self.tags):
            conf = float(t.get("confidence", 0.5)) * self._temporal_weight(float(t.get("timestamp", now)))
            if conf >= self.MIN_CONF_FOR_KEEP or i == len(self.tags) - 1:
                keep_idx.append(i)
        if len(keep_idx) < len(self.tags):
            self.keys  = [self.keys[i] for i in keep_idx]
            self.vals  = [self.vals[i] for i in keep_idx]
            self.tags  = [self.tags[i] for i in keep_idx]
            self._unique_hashes = {hash(k) for k in self.keys}
            self._telemetry("prune_legacy", remain=len(self.tags))

        # Basin decay + prune
        for k, lst in list(self.basins.items()):
            new_lst = []
            for e in lst:
                conf = float(e.get("confidence", 0.5)) * self._temporal_weight(float(e.get("ts", now)))
                if conf >= self.MIN_CONF_FOR_KEEP:
                    e["confidence"] = conf; new_lst.append(e)
            self.basins[k] = new_lst[:self.PER_BASIN_MAX]
        self._telemetry("prune_basins", basins=len(self.basins))

        # Attractor slow inverse drift (delegated to RHCM inverse)
        for shape, A in list(self.attractors.items()):
            try:
                proto = A.get("proto")
                if proto is None: continue
                n = max(2, min(shape)); lam = self.ATTR_DECAY_LAMBDA
                A["proto"] = (1.0 - lam) * proto + lam * rhcm.inverse(proto, n)  # type: ignore[name-defined]
                A["last_ts"] = now; self.attractors[shape] = A
            except Exception:
                pass
        self._telemetry("attractor_regularize", n=len(self.attractors))

    # =========================================================
    # Reporting / Stats / Visuals
    # =========================================================
    def __len__(self) -> int: return len(self.keys)

    def clear(self):
        self.keys.clear(); self.vals.clear(); self.tags.clear()
        self.history_size.clear(); self._unique_hashes.clear()
        self.attractors.clear(); self.basins.clear()
        self.meta_rules.clear(); self.subject_depth.clear()
        self._pair_index.clear()
        self._telemetry("clear")

    def dump(self) -> List[dict]:
        out = []
        for i, (k, v, t) in enumerate(zip(self.keys, self.vals, self.tags)):
            out.append({"idx": i, "sig_len": len(k), "out_shape": list(v.shape), "tags": t, "source": "legacy"})
        bidx = len(out)
        for (subject, shape), lst in self.basins.items():
            for e in lst:
                out.append({"idx": bidx, "subject": subject, "shape": list(shape), "sig_len": len(e["sig"]),
                            "out_shape": list(e["out"].shape), "tags": e["tags"], "source": "basin"})
                bidx += 1
        self._telemetry("dump", n=len(out))
        return out

    def stats(self) -> dict:
        n_legacy = len(self.keys)
        lens_legacy = [len(k) for k in self.keys] if n_legacy else []
        modal_len = None
        if lens_legacy:
            cnt = Counter(lens_legacy); modal_len = max(cnt, key=cnt.get)
        n_basins_items = sum(len(v) for v in self.basins.values())
        rule_kinds = Counter(t.get("rule_kind") for t in self.tags if isinstance(t, dict))
        s = {
            "n_items": n_legacy,
            "max_items": self.max_items,
            "avg_sig_len": (sum(lens_legacy) / n_legacy) if n_legacy else 0,
            "modal_sig_len": modal_len,
            "rule_kinds": dict(rule_kinds),
            "unique_signatures": len(self._unique_hashes),
            "duplicate_pressure": n_legacy - len(self._unique_hashes),
            "attractors": {str(k): {"count": v.get("count", 0), "consolidation": float(v.get("consolidation", 0.0))} for k, v in self.attractors.items()},
            "basin_items": n_basins_items,
            "subjects": {s: dict(self.subject_depth[s]) for s in self.subject_depth},
            "meta_rules": len(self.meta_rules),
            "pair_index": len(self._pair_index),
            "compression_ratio": float(self.compression_ratio),
            "flux_mean": float(np.mean(self.kairos_flux_history)) if self.kairos_flux_history else 0.0,
            "keel_ratio_mean": float(np.mean(self.keel_ratio_history)) if self.keel_ratio_history else 0.0,
        }
        self._telemetry("stats", n_items=int(n_legacy), basin_items=int(n_basins_items), meta_rules=int(len(self.meta_rules)))
        return s

    def pretty_print_stats(self):
        s = self.stats()
        print("\n=== HoloMemory Stats (strict) ===")
        print(f" Items stored (legacy) : {s['n_items']}/{s['max_items']}")
        print(f" Basin items           : {s['basin_items']}")
        print(f" Unique sigs           : {s['unique_signatures']} (dup pressure={s['duplicate_pressure']})")
        print(f" Avg sig length        : {s['avg_sig_len']:.1f}")
        print(f" Modal sig length      : {s['modal_sig_len']}")
        print(f" Meta rules            : {s['meta_rules']}")
        print(f" Pair index keys       : {s['pair_index']}")
        print(" Rule kinds            :")
        if not s["rule_kinds"]:
            print("   (none)")
        else:
            for k, v in s["rule_kinds"].items():
                print(f"   {k}: {v}")
        if self.attractors:
            print(" Attractors            :")
            for k, v in self.attractors.items():
                print(f"   shape={k} count={v.get('count',0)} consolidation={v.get('consolidation',0.0):.2f}")
        print(" Subjects/Depth        :")
        if not s["subjects"]:
            print("   (none)")
        else:
            for subj, depths in s["subjects"].items():
                print(f"   {subj}: {depths}")
        print("========================\n")
        self._telemetry("pretty_print_stats")

    def plot_history(self, path: str = "holo_history.png"):
        if not self.history_size or not self.ENABLE_VIS_HOLO_HISTORY: return
        try:
            import matplotlib.pyplot as plt
        except Exception:
            return
        plt.figure(figsize=(6,4))
        plt.plot(self.history_size, label="HoloMemory (legacy array) size")
        plt.xlabel("Additions"); plt.ylabel("Items stored"); plt.title("HoloMemory Growth Over Time")
        plt.legend(); plt.tight_layout(); plt.savefig(path); plt.close()
        self._telemetry("plot_history", path=path)

    def plot_pca_map(self, path: str = "holo_pca.png"):
        if not self.tags or not self.ENABLE_VIS_HOLO_PCA: return
        pcs = [t.get("pca_sig") for t in self.tags if isinstance(t, dict) and "pca_sig" in t]
        if not pcs: return
        pcs = np.array(pcs)
        try:
            import matplotlib.pyplot as plt
        except Exception:
            return
        plt.figure(figsize=(6,6))
        plt.scatter(pcs[:,0], pcs[:,1], alpha=0.6, s=20)
        plt.title("HoloMemory PCA Diversity Map")
        plt.xlabel("PC1"); plt.ylabel("PC2")
        plt.tight_layout(); plt.savefig(path); plt.close()
        self._telemetry("plot_pca_map", path=path, n=len(pcs))
    
    def export_stats_csv(self, path: str = "holo_stats.csv"):
        s = self.stats()
        row = {
            "ts": time.time(),
            "n_items": s.get("n_items", 0),
            "max_items": s.get("max_items", 0),
            "avg_sig_len": s.get("avg_sig_len", 0.0),
            "modal_sig_len": s.get("modal_sig_len", ""),
            "basin_items": s.get("basin_items", 0),
            "unique_signatures": s.get("unique_signatures", 0),
            "duplicate_pressure": s.get("duplicate_pressure", 0),
            "meta_rules": s.get("meta_rules", 0),
            "pair_index": s.get("pair_index", 0),
            "compression_ratio": s.get("compression_ratio", 0.0),
            "flux_mean": s.get("flux_mean", 0.0),
            "keel_ratio_mean": s.get("keel_ratio_mean", 0.0),
        }
        try:
            os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
            write_header = not os.path.isfile(path)
            import csv
            with open(path, "a", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=list(row.keys()))
                if write_header:
                    w.writeheader()
                w.writerow(row)
            self._telemetry("export_stats_csv", path=path)
        except Exception:
            self._telemetry("export_stats_csv_fail", path=path)

# ==========================================================
# Geometry & Physics Utilities  (RHCM-aware • SSOT • Meta-ready)
# ==========================================================

# ---------- Global wiring (if provided by runtime) ----------
EXPLAIN = globals().get("EXPLAIN", None)
meta_log = globals().get("meta_log", lambda *a, **k: None)
kairos = globals().get("kairos", None)
get_meta_dials = globals().get("get_meta_dials", None)

# ---------- Env toggles / tunables ----------
GEOM_ENHANCED = True
GEOM_DEBUG    = bool(int(os.getenv("GEOM_DEBUG", "0")))
PHYS_ALLOW_LOCAL_TICK = bool(int(os.getenv("PHYS_ALLOW_LOCAL_TICK", "0")))  # 0 = Meta-only Kairos authority

# SSOT-backed telemetry downsample (falls back to env=1)
try:
    if callable(get_meta_dials):
        _d = get_meta_dials() or {}
        TEL_DS_N = int(max(1, int(_d.get("tel_downsample_n", 1))))
    else:
        TEL_DS_N = max(1, int(float(os.getenv("META_TEL_DOWNSAMPLE_N", "1"))))
except Exception:
    TEL_DS_N = 1

# ==========================================================
# Small memoizers (array → (shape, hash) keys) with LRU trim
# ==========================================================
_GEOM_MEMO: Dict[str, Dict[Any, Any]] = {}

def _akey(arr: np.ndarray, *extra) -> Tuple[Tuple[int, int], str, Tuple[Any, ...]]:
    a = np.asarray(arr)
    a_c = np.ascontiguousarray(a)
    h = hashlib.sha1(a_c.tobytes()).hexdigest()
    return tuple(map(int, a.shape)), f"{a.dtype.str}|{h}", tuple(extra)


def _memo_get(ns: str, key):
    try: return _GEOM_MEMO.get(ns, {}).get(key)
    except Exception: return None


def _memo_put(ns: str, key, val, cap=2048):
    try:
        bucket = _GEOM_MEMO.setdefault(ns, {})
        if len(bucket) >= cap:
            # FIFO trim 1/4
            for k in list(bucket.keys())[: max(1, cap // 4)]:
                bucket.pop(k, None)
        bucket[key] = val
    except Exception:
        pass

# ==========================================================
# Basic stats / masks
# ==========================================================

def mode_color(x: np.ndarray) -> int:
    vals, cnts = np.unique(x, return_counts=True)
    return int(vals[np.argmax(cnts)]) if vals.size else 0


def _bbox(mask: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
    if not np.any(mask):
        return None
    rs = np.where(mask.any(axis=1))[0]
    cs = np.where(mask.any(axis=0))[0]
    return int(rs.min()), int(rs.max()), int(cs.min()), int(cs.max())

# ==========================================================
# Canonical translations / components
# ==========================================================

def translate_to_top_left(x: np.ndarray) -> np.ndarray:
    bg = mode_color(x)
    mask = (x != bg)
    bb = _bbox(mask)
    if bb is None:
        return x.copy()
    r0, r1, c0, c1 = bb
    crop = x[r0:r1+1, c0:c1+1]
    y = np.full_like(x, bg); h, w = crop.shape
    y[:h, :w] = crop
    return y


def cc_labels(mask: np.ndarray) -> Tuple[np.ndarray, List[int]]:
    h, w = mask.shape
    lab = np.zeros_like(mask, dtype=int)
    label, sizes = 0, []
    for i in range(h):
        for j in range(w):
            if mask[i, j] and lab[i, j] == 0:
                label += 1
                stack = [(i, j)]
                lab[i, j] = label
                cnt = 0
                while stack:
                    r, c = stack.pop(); cnt += 1
                    for dr, dc in ((1,0),(-1,0),(0,1),(0,-1)):
                        rr, cc = r+dr, c+dc
                        if 0 <= rr < h and 0 <= cc < w and mask[rr, cc] and lab[rr, cc] == 0:
                            lab[rr, cc] = label
                            stack.append((rr, cc))
                sizes.append(cnt)
    return lab, sizes


def keep_largest_component(x: np.ndarray) -> np.ndarray:
    bg = mode_color(x)
    mask = (x != bg).astype(np.uint8)
    labels, sizes = cc_labels(mask)
    if not sizes:
        return x.copy()
    k = int(np.argmax(sizes) + 1)
    y = np.full_like(x, bg)
    y[labels == k] = x[labels == k]
    return y

# ==========================================================
# Simple RHCM-lite (analytic trace; heavy RHCM is sandbox-only)
# ==========================================================

def shannon_entropy01(x: np.ndarray) -> float:
    p1 = float(np.mean(x))
    if p1 <= 0.0 or p1 >= 1.0: return 0.0
    p0 = 1.0 - p1
    return float(-(p0*math.log(p0 + 1e-12) + p1*math.log(p1 + 1e-12)))


def majority_3x3(a: np.ndarray) -> np.ndarray:
    pad = np.pad(a, 1, mode='constant')
    out = np.zeros_like(a)
    for i in range(a.shape[0]):
        for j in range(a.shape[1]):
            win = pad[i:i+3, j:j+3]
            out[i, j] = 1 if win.sum() >= 5 else 0
    return out


def morph_close(x: np.ndarray) -> np.ndarray:
    k = np.array([[0,1,0],[1,1,1],[0,1,0]], dtype=int)
    def dilate(a):
        pad = np.pad(a, 1, mode='constant'); out = np.zeros_like(a)
        for i in range(a.shape[0]):
            for j in range(a.shape[1]):
                win = pad[i:i+3, j:j+3]; out[i, j] = 1 if np.any(win & k) else a[i, j]
        return out
    def erode(a):
        pad = np.pad(a, 1, mode='constant'); out = np.zeros_like(a)
        for i in range(a.shape[0]):
            for j in range(a.shape[1]):
                win = pad[i:i+3, j:j+3]; out[i, j] = 1 if np.all(win | (1-k)) else 0
        return out
    return erode(dilate(x))


def boundary_flip_score(a: np.ndarray) -> float:
    h = (a[:, 1:] ^ a[:, :-1]).mean() if a.shape[1] > 1 else 0.0
    v = (a[1:, :] ^ a[:-1, :]).mean() if a.shape[0] > 1 else 0.0
    return float(h + v)

def score_from_trace(tr: Dict[str, Any]) -> float:
    return (-float(tr["entropy_slope"])) + 0.7*float(tr["epi_tail"]) - 0.5*float(tr["sym_dev"])

# ==========================================================
# Palette-aware helpers
# ==========================================================

def _greedy_palette_map(a: np.ndarray, b: np.ndarray) -> Dict[int, int]:
    mapping = {}
    ua = np.unique(a); ub = np.unique(b)
    for ca in ua:
        mask = (a == ca)
        vals, cnts = np.unique(b[mask], return_counts=True)
        mapping[int(ca)] = int(vals[np.argmax(cnts)]) if len(vals) else int(ub[0])
    return mapping


def recolor_by_map(x: np.ndarray, mapping: Dict[int, int]) -> np.ndarray:
    y = x.copy()
    for k, v in mapping.items():
        y[x == k] = v
    return y


def boundary_weighted_error(a: np.ndarray, b: np.ndarray) -> float:
    mism = (a != b).astype(np.uint8)
    edge = np.zeros_like(mism, dtype=np.uint8)
    edge[:, 0] = 1; edge[:, -1] = 1; edge[0, :] = 1; edge[-1, :] = 1
    w = 1.0 + edge
    return float((mism * w).sum() / max(1.0, w.sum()))


def error_field(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
    return (y_pred != y_true).astype(np.uint8)

# ==========================================================
# Discrete geometric ops
# ==========================================================

def rot90(x):  return np.rot90(x, 1)

def rot180(x): return np.rot90(x, 2)

def rot270(x): return np.rot90(x, 3)

def flip_h(x): return np.flip(x, axis=1)

def flip_v(x): return np.flip(x, axis=0)

def transpose(x): return x.T

def flip_diag_main(x: np.ndarray) -> np.ndarray:  return x.T

def flip_diag_anti(x: np.ndarray) -> np.ndarray:  return np.flipud(x.T)


def translate_to_center(x: np.ndarray) -> np.ndarray:
    bg = mode_color(x)
    mask = (x != bg); bb = _bbox(mask)
    if bb is None: return x.copy()
    r0, r1, c0, c1 = bb; H, W = x.shape
    crop = x[r0:r1+1, c0:c1+1]
    # centers
    src_r = (r0 + r1) / 2.0; src_c = (c0 + c1) / 2.0
    tgt_r = (H - 1) / 2.0;   tgt_c = (W - 1) / 2.0
    dr = int(round(tgt_r - src_r)); dc = int(round(tgt_c - src_c))
    out = np.full_like(x, bg)
    nr0 = max(0, r0 + dr); nc0 = max(0, c0 + dc)
    nr1 = min(H - 1, r1 + dr); nc1 = min(W - 1, c1 + dc)
    sr0 = r0 + (nr0 - (r0 + dr)); sc0 = c0 + (nc0 - (c0 + dc))
    sr1 = r1 - ((r1 + dr) - nr1); sc1 = c1 - ((c1 + dc) - nc1)
    if sr0 <= sr1 and sc0 <= sc1 and nr0 <= nr1 and nc0 <= nc1:
        out[nr0:nr1+1, nc0:nc1+1] = x[sr0:sr1+1, sc0:sc1+1]
    return out


def unwrap_rings(x: np.ndarray) -> np.ndarray:
    H, W = x.shape; bg = mode_color(x); layers = []
    num_layers = min(H, W) // 2
    for r in range(num_layers):
        top, left = r, r; bottom, right = H - 1 - r, W - 1 - r
        if top > bottom or left > right: break
        perim = []
        for c in range(left, right + 1): perim.append(x[top, c])
        for rr in range(top + 1, bottom): perim.append(x[rr, right])
        if bottom > top:
            for c in range(right, left - 1, -1): perim.append(x[bottom, c])
        if left < right:
            for rr in range(bottom - 1, top, -1): perim.append(x[rr, left])
        if perim: layers.append(np.array(perim, dtype=x.dtype)[None, :])
    if not layers: return x.copy()
    maxw = max(row.shape[1] for row in layers)
    out = np.full((len(layers), maxw), bg, dtype=x.dtype)
    for i, row in enumerate(layers): out[i, :row.shape[1]] = row
    return out


def block_scale_up(x: np.ndarray, sy: int, sx: int) -> np.ndarray:
    return np.repeat(np.repeat(x, sy, axis=0), sx, axis=1)


def block_scale_down(x: np.ndarray, sy: int, sx: int) -> Optional[np.ndarray]:
    h, w = x.shape
    if (h % sy != 0) or (w % sx != 0):
        _safe_emit("geo.scale_down_mismatch", {"h": h, "w": w, "sy": sy, "sx": sx})
        return None
    out = np.zeros((h//sy, w//sx), dtype=x.dtype)
    for i in range(0, h, sy):
        for j in range(0, w, sx):
            block = x[i:i+sy, j:j+sx]
            vals, cnts = np.unique(block, return_counts=True)
            out[i//sy, j//sx] = vals[np.argmax(cnts)]
    return out


def scale_to_shape(x: np.ndarray, tgt_shape: Tuple[int, int]) -> Optional[np.ndarray]:
    if not tgt_shape or tuple(x.shape) == tuple(tgt_shape): return x
    try:
        th, tw = int(tgt_shape[0]), int(tgt_shape[1])
        sh, sw = x.shape
        yy = np.zeros((th, tw), dtype=x.dtype)
        for i in range(th):
            si = min(sh - 1, round(i * (sh - 1) / max(1, th - 1)))
            for j in range(tw):
                sj = min(sw - 1, round(j * (sw - 1) / max(1, tw - 1)))
                yy[i, j] = x[si, sj]
        return yy
    except Exception:
        return None

# ==========================================================
# Operation composition (NOTE: palette mapping preserves np.unique order)
# ==========================================================
BASE_HYPOS: Dict[str, Callable[[np.ndarray], np.ndarray]] = {
    "id": lambda x: x,
    "rot90": rot90, "rot180": rot180, "rot270": rot270,
    "flip_h": flip_h, "flip_v": flip_v, "transpose": transpose,
    "to_topleft": translate_to_top_left,
    "flip_diag_main": flip_diag_main, "flip_diag_anti": flip_diag_anti,
    "to_center": translate_to_center,
    "unwrap_rings": unwrap_rings,
    "keep_largest": keep_largest_component,
}


def compose_ops(x: np.ndarray, ops: List[Tuple[str, Any]]) -> np.ndarray:
    def _palette_of(arr: np.ndarray) -> Tuple[int, ...]:
        return tuple(int(v) for v in np.unique(arr))
    def recolor_to_palette(arr: np.ndarray, pal: Tuple[int, ...]) -> np.ndarray:
        cur = _palette_of(arr)
        m = {c: p for c, p in zip(cur, pal[:len(cur)])}
        vv = np.vectorize(lambda v: int(m.get(int(v), int(v))))
        return vv(arr).astype(int)

    y = x
    for name, param in ops:
        if name in BASE_HYPOS:
            y = BASE_HYPOS[name](y)
        elif name == "scale_to":
            tgt = tuple(param) if param is not None else y.shape
            z = scale_to_shape(y, tgt)
            if z is not None: y = z
        elif name == "recolor_to":
            pal = tuple(int(v) for v in param) if param is not None else _palette_of(y)
            y = recolor_to_palette(y, pal)
        # unknown => no-op
    return y

# ==========================================================
# Geometry + RHCM-driven selection (analytic score only here)
# ==========================================================

def geometric_rhcm_filter(
    train_pairs: List[Tuple[np.ndarray, np.ndarray]],
    top_k: int = 3, steps: int = 24
) -> List[Tuple[List[Tuple[str, Any]], float]]:
    if not train_pairs: return []
    shapes = [p[1].shape for p in train_pairs if p[1] is not None]
    if not shapes: return []
    vals, cnts = np.unique(np.array(shapes, dtype=object), return_counts=True)
    target_shape = tuple(vals[np.argmax(cnts)])

    base_norms = ["id", "to_topleft", "keep_largest"]
    rots = ["id", "rot90", "rot180", "rot270", "transpose"]
    flips = ["id", "flip_h", "flip_v"]

    cand_chains: List[List[Tuple[str, Any]]] = []
    for n1 in base_norms:
        for r in rots:
            for f in flips:
                chain = []
                if n1 != "id": chain.append((n1, None))
                if r  != "id": chain.append((r, None))
                if f  != "id": chain.append((f, None))
                chain.append(("scale_to", target_shape))
                cand_chains.append(chain)

    scored: List[Tuple[List[Tuple[str, Any]], float]] = []
    for chain in cand_chains:
        per_pair_scores = []
        consistent = True
        for Xin, Xout in train_pairs:
            try:
                Y = compose_ops(Xin, chain)
                if Y.shape != Xout.shape:
                    consistent = False; break
                mapping = _greedy_palette_map(Y, Xout)
                Yr = recolor_by_map(Y, mapping)
                bw = boundary_weighted_error(Yr, Xout)
                E  = error_field(Yr, Xout)
                tr = rhcm_collapse_trace(E, steps=steps)
                s  = (1.0 - bw) + 0.35 * score_from_trace(tr)
                per_pair_scores.append(float(s))
            except Exception:
                consistent = False; break
        if consistent and per_pair_scores:
            scored.append((chain, float(np.mean(per_pair_scores))))

    scored.sort(key=lambda z: z[1], reverse=True)
    _safe_emit("geo.filter_ranked", {"n": len(scored), "top_score": float(scored[0][1]) if scored else 0.0})
    return scored[:max(1, top_k)]


def geom_filter_suggest_ops(
    train_pairs: List[Tuple[np.ndarray, np.ndarray]],
    top_k: int = 3, steps: int = 24
) -> Dict[str, Any]:
    try:
        ranked = geometric_rhcm_filter(train_pairs, top_k=top_k, steps=steps)
        if not ranked: return {"ops": [], "score": 0.0}
        best_ops, best_score = ranked[0]
        return {"ops": best_ops, "score": float(best_score)}
    except Exception:
        return {"ops": [], "score": 0.0}

# ===========================================
# Interpreter  (webbed: Kairos + Ultra + Holo + Sandbox)
# ===========================================
def interpret_predictions(rows: List[Dict[str, Any]],
                          out_csv: str = "interpretation.csv",
                          stats_only: bool = False,
                          solver: Optional[Any] = None):
    

    # --- derive run_id + phase for filename scoping/sanitization -------------
    def _get_run_phase():
        run_id = None
        phase = None
        try:
            if solver is not None:
                run_id = getattr(solver, "run_id", None)
                phase = getattr(solver, "phase", None)
                if run_id is None and hasattr(solver, "meta"):
                    run_id = getattr(getattr(solver, "meta", None), "run_id", None)
                if phase is None and hasattr(solver, "meta"):
                    phase = getattr(getattr(solver, "meta", None), "phase", None)
        except Exception:
            pass
        try:
            if run_id is None:
                run_id = os.getenv("RUN_ID", "na")
            if phase is None:
                phase = os.getenv("PHASE", "na")
        except Exception:
            run_id, phase = "na", "na"
        return str(run_id), str(phase)

    run_id, phase = _get_run_phase()

    def _scoped_path(pth: str) -> str:
        # If template tokens present, format; else scope into runs/<run_id>/
        try:
            if "{run_id}" in pth or "{phase}" in pth:
                return pth.format(run_id=run_id, phase=phase)
        except Exception:
            pass
        return os.path.join("runs", run_id, pth)

    # scope default outputs
    out_csv = _scoped_path(out_csv)
    by_task_csv = _scoped_path("interpretation_by_task.csv")

    # headers (append forensics: hybrid margin + latency)
    headers = [
        "task_id", "test_index", "rule_kind", "sim_score",
        "ok_shape", "correct",
        "hybrid_margin", "latency_ms"
    ]

    # --- helpers --------------------------------------------------------------
    def _safe_makedirs(path: str):
        d = os.path.dirname(path)
        if d:
            try:
                os.makedirs(d, exist_ok=True)
            except Exception:
                pass

    def _telemetry(topic: str, **kw):
        # Meta logger
        try:
            if "meta_log" in globals() and callable(globals().get("meta_log")):
                globals()["meta_log"](topic, **kw)  # type: ignore
        except Exception:
            pass
        # Ultra + Kairos
        try:
            if globals().get("ultra"):
                globals()["ultra"].observe(topic.replace(".", "_"), **kw)
            if globals().get("kairos"):
                globals()["kairos"].step(time_step=int(kw.get("time_step_hint", 1)))
        except Exception:
            pass
        # EXPLAIN mirror
        try:
            ex = globals().get("EXPLAIN")
            if ex is not None and hasattr(ex, "log"):
                ex.log(topic, {"run_id": run_id, "phase": phase, **kw})
        except Exception:
            pass
        # Holo echo
        try:
            if solver and hasattr(solver, "holo") and solver.holo is not None:
                solver.holo.add(topic, kw, {"schema": schema_version, "run_id": run_id, "phase": phase})
        except Exception:
            pass

    # ------------------- Main CSV export -------------------
    csv_rows: List[List[Any]] = []
    for r in rows:
        # collect optional forensics
        margin = r.get("hybrid_margin", r.get("margin", r.get("hybrid_delta", 0.0)))
        latency = r.get("latency_ms", r.get("latency", 0.0))
        try:
            margin = float(margin)
        except Exception:
            margin = 0.0
        try:
            latency = float(latency)
        except Exception:
            latency = 0.0

        csv_rows.append([
            r.get("task_id", ""),
            r.get("test_index", ""),
            r.get("rule_kind", ""),
            round(float(r.get("sim_score", 0.0)), 6),
            bool(r.get("ok_shape", False)),
            bool(r.get("correct", False)),
            round(margin, 6),
            round(latency, 3),
        ])

    try:
        _atomic_write_csv(out_csv, csv_rows, headers)
        try:
            if "logger" in globals():
                logger.info(f"Interpreter wrote → {out_csv}")  # type: ignore
        except Exception:
            pass
        _telemetry("interpreter.write", file=out_csv, n=len(rows))
    except Exception as e:
        try:
            if "logger" in globals():
                logger.warning(f"Interpreter CSV write failed: {e}")  # type: ignore
        except Exception:
            pass
        _telemetry("interpreter.write_error", error=str(e))

    # ------------------- Holo sync -------------------
    try:
        if solver and hasattr(solver, "holo") and solver.holo is not None:
            solver.holo.add("interpreter_csv", out_csv, {"count": len(rows), "headers": headers})
    except Exception:
        pass

    # ------------------- Aggregate per task/kind -------------------
    by_task = defaultdict(lambda: {"n": 0, "ok": 0})
    by_kind = defaultdict(lambda: {"ok": 0, "fail": 0})
    failures = []
    for r in rows:
        tid = r.get("task_id", "")
        is_ok = bool(r.get("correct"))
        by_task[tid]["n"] += 1
        if is_ok:
            by_task[tid]["ok"] += 1
        knd = r.get("rule_kind", "")
        if is_ok:
            by_kind[knd]["ok"] += 1
        else:
            by_kind[knd]["fail"] += 1
            failures.append(r)

    # Failure taxonomy tagging
    for r in failures:
        if not bool(r.get("ok_shape", False)):
            r["failure_type"] = "shape_mismatch"
        else:
            r["failure_type"] = "content_mismatch"

    # Always emit per-task CSV (even when stats_only=False)
    try:
        task_rows = []
        for tid, v in by_task.items():
            acc = (v["ok"] / v["n"]) if v["n"] > 0 else 0.0
            task_rows.append([tid, v["n"], v["ok"], round(acc, 6)])
        _atomic_write_csv(by_task_csv, task_rows, ["task_id", "n", "ok", "acc"])
        # Telemetry mirrors
        accuracies = []
        for tid, v in by_task.items():
            acc = (v["ok"] / v["n"]) if v["n"] > 0 else 0.0
            accuracies.append(acc)
            try:
                if solver and hasattr(solver, "holo") and solver.holo is not None:
                    solver.holo.add("task_accuracy", acc, {"task_id": tid, "ok": v["ok"], "n": v["n"]})
            except Exception:
                pass
            try:
                if getattr(solver, "ultra", None):
                    solver.ultra.observe("task_accuracy", task_id=tid, acc=acc)
            except Exception:
                pass
        div = float(np.std(accuracies)) if accuracies else 0.0
        _telemetry("interpreter.task_summary", n=len(by_task), divergence=div, entropy_delta=0.002 * len(rows))
        try:
            if "_update_op_rank" in globals():
                _update_op_rank("interpreter_feedback", reward=(float(np.mean(accuracies)) - 0.5) if accuracies else -0.5)  # type: ignore
        except Exception:
            pass
    except Exception as e:
        try:
            if "logger" in globals():
                logger.warning(f"interpretation_by_task export failed: {e}")  # type: ignore
        except Exception:
            pass
        _telemetry("interpreter.task_summary_error", error=str(e))

    # ------------------- Stats & plots + analytics -------------------
    bundle_files = [out_csv, by_task_csv]
    if stats_only:
        # Console print
        print("\n=== Accuracy by rule_kind ===")
        for k, v in by_kind.items():
            tot = v["ok"] + v["fail"]
            rate = (v["ok"] / tot) if tot > 0 else 0.0
            print(f" {k:12s}: {v['ok']}✔  {v['fail']}✘   (acc={rate:.2f})")
            try:
                if solver and hasattr(solver, "holo") and solver.holo is not None:
                    solver.holo.add("rule_kind_accuracy", rate, {
                        "rule_kind": k, "ok": v["ok"], "fail": v["fail"], "total": tot
                    })
            except Exception:
                pass
        print("============================\n")

        # Headless-safe plotting
        try:
            import matplotlib
            try:
                matplotlib.use("Agg")
            except Exception:
                pass
            import matplotlib.pyplot as plt  # type: ignore

            labs = list(by_kind.keys())
            okv = [v["ok"] for v in by_kind.values()]
            flv = [v["fail"] for v in by_kind.values()]
            acc = [(ok / (ok + fl) if (ok + fl) > 0 else 0.0) for ok, fl in zip(okv, flv)]

            # Stacked bar (OK/FAIL)
            if labs:
                plt.figure(figsize=(max(6, len(labs) * 0.6), 4))
                idx = np.arange(len(labs))
                plt.bar(idx, okv, label="OK")
                plt.bar(idx, flv, bottom=okv, label="FAIL")
                plt.xticks(idx, labs, rotation=90)
                plt.ylim(0, max([ok + fl for ok, fl in zip(okv, flv)] + [1]))
                plt.title(f"Counts by rule_kind (OK stacked over FAIL) — run:{run_id}")
                plt.legend()
                plt.tight_layout()
                stacked_path = _scoped_path("interpretation_stacked.png")
                plt.savefig(stacked_path)
                plt.close()
                bundle_files.append(stacked_path)
                _telemetry("interpreter.stacked_plot", kinds=len(labs))

            # Accuracy bar
            if labs:
                plt.figure(figsize=(6, 4))
                plt.bar(labs, acc)
                plt.title(f"Accuracy per rule_kind — run:{run_id}")
                plt.ylim(0, 1)
                plt.tight_layout()
                acc_bar_path = _scoped_path("interpretation_acc.png")
                plt.savefig(acc_bar_path)
                plt.close()
                bundle_files.append(acc_bar_path)
                _telemetry("interpreter.acc_plot", kinds=len(labs))

            # Simple accuracy heatmap (per-kind)
            if labs:
                plt.figure(figsize=(max(4, len(labs) * 0.3), 2.2))
                A = np.array([acc], dtype=float)
                plt.imshow(A, aspect="auto", vmin=0.0, vmax=1.0)
                plt.yticks([0], ["acc"])
                plt.xticks(range(len(labs)), labs, rotation=90)
                plt.colorbar(fraction=0.046, pad=0.04)
                plt.tight_layout()
                heat_path = _scoped_path("interpretation_acc_heatmap.png")
                plt.savefig(heat_path)
                plt.close()
                bundle_files.append(heat_path)
                _telemetry("interpreter.acc_heatmap", kinds=len(labs))
        except Exception as e:
            try:
                if "logger" in globals():
                    logger.warning(f"Plotting failed: {e}")  # type: ignore
            except Exception:
                pass
            _telemetry("interpreter.plot_error", error=str(e))

        # Top-N leaderboard CSV (by accuracy then by count)
        try:
            leaderboard = []
            for k, v in by_kind.items():
                tot = v["ok"] + v["fail"]
                acc_v = v["ok"] / tot if tot else 0.0
                leaderboard.append([k, tot, v["ok"], v["fail"], round(acc_v, 6)])
            leaderboard.sort(key=lambda x: (x[4], x[1]), reverse=True)
            lb_path = _scoped_path("interpretation_leaderboard.csv")
            _atomic_write_csv(lb_path, leaderboard,
                              ["rule_kind", "total", "ok", "fail", "acc"])
            bundle_files.append(lb_path)
            _telemetry("interpreter.leaderboard", n=len(leaderboard))
        except Exception as e:
            _telemetry("interpreter.leaderboard_error", error=str(e))

        # ROC-like sweep (threshold vs accuracy) if sim_score available
        try:
            sims = [float(r.get("sim_score", 0.0)) for r in rows if "sim_score" in r]
            if sims:
                import matplotlib
                try:
                    matplotlib.use("Agg")
                except Exception:
                    pass
                import matplotlib.pyplot as plt  # type: ignore
                ths = np.linspace(0.0, 1.0, 21).tolist()
                rates = []
                for t in ths:
                    ok = 0; tot = 0
                    for rr in rows:
                        s = float(rr.get("sim_score", 0.0))
                        pred_ok = s >= t
                        tot += 1
                        ok += 1 if (pred_ok == bool(rr.get("correct"))) else 0
                    rates.append(ok / tot if tot else 0.0)
                plt.figure(figsize=(6, 4))
                plt.plot(ths, rates, marker="o")
                plt.title(f"Threshold sweep vs correctness agreement — run:{run_id}")
                plt.xlabel("threshold on sim_score"); plt.ylabel("agreement rate")
                plt.tight_layout()
                ts_path = _scoped_path("interpretation_threshold_sweep.png")
                plt.savefig(ts_path); plt.close()
                bundle_files.append(ts_path)
                _telemetry("interpreter.threshold_sweep", points=len(ths))
        except Exception as e:
            _telemetry("interpreter.threshold_error", error=str(e))

        # Outlier dump (hardest failures)
        try:
            failures_sorted = sorted(
                [rr for rr in failures if "sim_score" in rr],
                key=lambda rr: float(rr.get("sim_score", 0.0))
            )
            path = _scoped_path("interpretation_outliers.jsonl")
            _safe_makedirs(path)
            tmp = f"{path}.tmp"
            with open(tmp, "w", encoding="utf-8") as f:
                for rr in failures_sorted[:200]:
                    f.write(json.dumps(rr) + "\n")
            os.replace(tmp, path)
            bundle_files.append(path)
            _telemetry("interpreter.outliers", n=min(200, len(failures_sorted)))
        except Exception as e:
            _telemetry("interpreter.outliers_error", error=str(e))

    # ------------------- Artifact bundle + rolling state -------------------
    try:
        sidecar = {
            "schema_version": schema_version,
            "rows": len(rows),
            "tasks": len(by_task),
            "kinds": len(by_kind),
            "generated_at": time.time(),
            "run_id": run_id,
            "phase": phase,
            "files": bundle_files,
            "stats_only": bool(stats_only),
        }
        meta_path = _scoped_path("interpretation_meta.json")
        _safe_makedirs(meta_path)
        with open(meta_path + ".tmp", "w", encoding="utf-8") as f:
            json.dump(sidecar, f, indent=2)
        os.replace(meta_path + ".tmp", meta_path)
        bundle_files.append(meta_path)

        import zipfile
        zip_path = _scoped_path("interpretation_bundle.zip")
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as z:
            for fp in bundle_files:
                if os.path.isfile(fp):
                    z.write(fp)
        _telemetry("interpreter.bundle", zip=zip_path, n_files=len(bundle_files))
    except Exception as e:
        _telemetry("interpreter.bundle_error", error=str(e))

    # Rolling error budget
    try:
        errs = sum(1 for rr in rows if not bool(rr.get("correct")))
        total = len(rows)
        err_rate = (errs / total) if total else 0.0
        state = {"n": total, "errors": errs, "err_rate": err_rate, "t": time.time(), "run_id": run_id, "phase": phase}
        st_path = _scoped_path(".interpretation.state")
        with open(st_path + ".tmp", "w", encoding="utf-8") as f:
            json.dump(state, f)
        os.replace(st_path + ".tmp", st_path)
        _telemetry("interpreter.state_updated", err_rate=err_rate)
    except Exception as e:
        _telemetry("interpreter.state_error", error=str(e))

    # ------------------- Global phase sync -------------------
    try:
        if getattr(solver, "kairos", None):
            solver.kairos.step(time_step=1)
        if getattr(solver, "ultra", None):
            solver.ultra.observe("interpreter_complete", rows=len(rows))
        # Mirror basic summary to EXPLAIN one more time
        try:
            ex = globals().get("EXPLAIN")
            if ex is not None and hasattr(ex, "log"):
                ex.log("interpreter.summary", {"run_id": run_id, "phase": phase, "rows": len(rows)})
        except Exception:
            pass
    except Exception as e:
        try:
            if "logger" in globals():
                logger.warning(f"Global phase sync failed: {e}")  # type: ignore
        except Exception:
            pass
        try:
            if "meta_log" in globals() and callable(globals().get("meta_log")):
                meta_log("global_phase_sync_error", error=str(e))  # type: ignore
        except Exception:
            pass

    return {
        "n_rows": len(rows),
        "out_csv": out_csv,
        "by_task_csv": by_task_csv,
        "stats_only": bool(stats_only),
        "bundle": _scoped_path("interpretation_bundle.zip"),
        "run_id": run_id,
        "phase": phase,
    }


# ----------------------
# Commit replay helper
# ----------------------
def commit_replay(commit_id: str, journal_path: str = "commit_journal.jsonl") -> Optional[dict]:   
    try:
        with open(journal_path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    rec = json.loads(line)
                    if rec.get("commit_id") == commit_id:
                        return rec
                except Exception:
                    continue
    except Exception:
        pass
    return None

# ==========================================================
# RHCM Collapse Scoring Adapter + Candidate Selector (wired)
# ==========================================================

# Tunables (via env):
#   SCRAMBLER_COLLAPSE_WEIGHTS="alpha,beta,gamma"   (default "1.0,0.8,0.5")
#   SCRAMBLER_DEPTH="1.618"                         (default 1.618)
#   SCRAMBLER_ITERS="6"                             (default 6)
#   SCRAMBLER_CONTINUOUS="0"                        (0→quantized RHCM, 1→continuous cRHCM)
#   SCRAMBLER_TOPK="2"                              (default 2)



# ---------- env helpers ----------
def _env_float(name: str, default: float) -> float:
    try:
        return float(os.getenv(name, str(default)))
    except Exception:
        return float(default)

def _env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, str(default)))
    except Exception:
        return int(default)

def _env_tuple3(name: str, default: Tuple[float, float, float]) -> Tuple[float, float, float]:
    raw = os.getenv(name)
    if not raw:
        return default
    try:
        parts = [float(x.strip()) for x in raw.split(",")]
        if len(parts) >= 3:
            return (parts[0], parts[1], parts[2])
        if len(parts) == 2:
            return (parts[0], parts[1], default[2])
        if len(parts) == 1:
            return (parts[0], default[1], default[2])
    except Exception:
        pass
    return default

# ---------- grid → entropy surface Ψ ----------
def _grid_to_entropy_matrix(grid: np.ndarray, n: int = 32) -> np.ndarray:    
    try:
        g = np.asarray(grid, dtype=int)
        # map -1 to a separate bucket at the end to avoid skewing
        vals = g[g != -1].ravel()
        if vals.size == 0:
            base = np.zeros_like(g, dtype=float)
        else:
            uniq = np.unique(vals)
            ranks = {v: i for i, v in enumerate(uniq)}
            base = np.where(g == -1, len(uniq), np.vectorize(lambda x: ranks.get(int(x), 0))(g))
            base = base.astype(float)
            base /= max(1.0, base.max())

        R, C = base.shape
        if R == n and C == n:
            return base
        # nearest-neighbor resize without external libs
        rr = np.linspace(0, R - 1, num=n).astype(int)
        cc = np.linspace(0, C - 1, num=n).astype(int)
        return base[np.ix_(rr, cc)]
    except Exception:
        return np.zeros((n, n), dtype=float)

# ---------- RHCM sequence driven by Ψ ----------
def _rhcm_sequence_from_entropy(psi: np.ndarray,
                                iterations: int = 6,
                                depth: float = 1.618,
                                continuous: bool = False) -> List[np.ndarray]:    
    seq: List[np.ndarray] = []
    try:
        n = int(psi.shape[0])
        Ψ = psi.copy()
        for _ in range(max(1, iterations)):
            if continuous:
                M = generate_cRHCM(n, depth, Ψ)     # expects Ψ
                Minv = -M                            # continuous inverse
            else:
                M = generate_RHCM(n, depth, Ψ)       # expects Ψ
                Minv = generate_RHCM_inverse(M, n)   # your discrete inverse
            Ψ = M - Minv
            seq.append(Ψ.copy())
    except Exception:
        # graceful fallback: return a singleton sequence
        seq = [psi.copy()]
    return seq

# ---------- metrics on sequence ----------
def _entropy_like(x: np.ndarray, bins: int = 16) -> float:    
    try:
        arr = np.asarray(x, dtype=float)
        hist, _ = np.histogram(arr, bins=bins)
        p = hist.astype(float) / max(1.0, hist.sum())
        # avoid log(0)
        return float(-(p * np.log2(p + 1e-12)).sum())
    except Exception:
        return 0.0

def _slope(y: List[float]) -> float:    
    try:
        T = len(y)
        if T <= 1:
            return 0.0
        xs = np.arange(T, dtype=float)
        # slope = cov(x,y)/var(x)
        vx = float(((xs - xs.mean()) ** 2).sum()) or 1.0
        slope = float(((xs - xs.mean()) * (np.array(y) - np.mean(y))).sum() / vx)
        return slope
    except Exception:
        return 0.0

def _symmetry_deviation(x: np.ndarray) -> float:    
    try:
        arr = np.asarray(x, dtype=float)
        rng = float(arr.max() - arr.min()) or 1.0
        dev_lr = float(np.mean(np.abs(arr - np.fliplr(arr)))) / rng
        dev_ud = float(np.mean(np.abs(arr - np.flipud(arr)))) / rng
        return float(min(1.0, 0.5 * (dev_lr + dev_ud)))
    except Exception:
        return 0.5

# ---------- global cache for traces ----------
_RHCM_TRACE_CACHE: Dict[str, Dict[str, float]] = {}

def _hash_grid(g: np.ndarray) -> str:
    try:
        h = hashlib.sha1()
        # small downsample in hash to be robust to tiny jitters
        miniature = _grid_to_entropy_matrix(g, n=16)
        h.update(np.ascontiguousarray(miniature).tobytes())
        return h.hexdigest()
    except Exception:
        return hex(abs(hash(str(g.shape))))[2:]

# ===========================================
# Scrambler (deterministic, collapse-scored, namespaced, telemetry-safe)
# ===========================================


# --- env knobs (tunable without code changes) ---
_SCR_TRIALS = int(os.getenv("SCRAMBLER_TRIALS", "8"))        # how many permutations to try
_SCR_TOPK   = int(os.getenv("SCRAMBLER_TOPK", "2"))          # top-K kept before tie-break
# alpha,beta,gamma (for logging only; scoring uses score_candidate_via_collapse)
try:
    _a,_b,_g = [float(x) for x in os.getenv("SCRAMBLER_COLLAPSE_WEIGHTS", "1.0,0.8,0.5").split(",")]
except Exception:
    _a,_b,_g = 1.0, 0.8, 0.5

# Optional: size thresholds
_SCR_GPU_MIN_ELEMS = int(os.getenv("SCR_GPU_MIN_ELEMS", "200000"))  # only push to GPU for large grids

# --- safe meta logger shim (unified) ---
def _safe_meta_log(topic: str, **payload):
    try:
        ml = globals().get("_meta_log") or globals().get("meta_log")
        if callable(ml):
            ml(topic, **payload)
    except Exception:
        pass

class Scrambler:    
    def __init__(self, seed: int = 1337, solver: Optional[Any] = None):
        self.seed = int(seed)
        # Per-namespace caches (ns_key -> ...)
        self.perm_cache: Dict[str, Dict[int, int]] = {}
        self.inv_cache: Dict[str, Dict[int, int]] = {}
        self._lut_cache: Dict[str, np.ndarray] = {}
        self._inv_lut_cache: Dict[str, np.ndarray] = {}
        self._quarantine: Set[str] = set()
        self.solver = solver

    # ---------------------- key utilities ----------------------
    def _ns_key(self, key: str) -> str:        
        try:
            run_id = getattr(self.solver, "run_id", None)
            phase  = getattr(self.solver, "phase", None)
            task   = getattr(self.solver, "current_task_id", None)
            base   = f"{key}|seed={self.seed}|run={run_id}|phase={phase}|task={task}"
        except Exception:
            base   = f"{key}|seed={self.seed}"
        return hashlib.sha1(base.encode("utf-8")).hexdigest()

    def _emit_diag(self, topic: str, **payload):
        _safe_meta_log(topic, **payload)
        # Global taps (best-effort, headless-safe)
        try:
            k = globals().get("kairos"); u = globals().get("ultra"); h = globals().get("holo")
            if k is not None and hasattr(k, "step"): k.step(time_step=1)
            if u is not None and hasattr(u, "observe"): u.observe(topic, **payload)
            if h is not None and hasattr(h, "add"): h.add(None, None, {"subject": topic, **payload})
        except Exception:
            pass

    # ---------------------- RNG helpers ----------------------
    def _rng_for_ns(self, ns_key: str) -> random.Random:
        # seed RNG deterministically per ns_key
        return random.Random(ns_key)

    # ---------------------- permutation generation ----------------------
    def _perm_candidates(self, ns_key: str, trials: int, mode: str, rng: random.Random) -> List[List[int]]:
        digits = list(range(10))
        conf_pairs = [(1,7), (2,5), (3,8), (4,6)]
        cands: List[List[int]] = []
        for _ in range(max(1, trials)):
            p = digits[:]
            rng.shuffle(p)
            if mode == "adversarial" and rng.random() < 0.7:
                a, b = conf_pairs[rng.randrange(len(conf_pairs))]
                ia, ib = p.index(a), p.index(b)
                p[ia], p[ib] = p[ib], p[ia]
            cands.append(p)
        # Bias add: try to minimize fixed points on one additional candidate
        q = digits[:]
        rng.shuffle(q)
        for i in range(10):
            if q[i] == i:
                j = (i + 1) % 10
                q[i], q[j] = q[j], q[i]
        cands.append(q)
        # Deduplicate
        uniq = []
        seen = set()
        for p in cands:
            t = tuple(p)
            if t not in seen:
                uniq.append(p)
                seen.add(t)
        return uniq

    # ---------------------- LUT helpers (CPU/GPU) ----------------------
    def _ensure_luts(self, ns_key: str):
        if ns_key in self._lut_cache and ns_key in self._inv_lut_cache:
            return
        mp = self.perm_cache.get(ns_key)
        inv = self.inv_cache.get(ns_key)
        if mp is None or inv is None:
            return
        lut = np.arange(256, dtype=int)
        inv_lut = np.arange(256, dtype=int)
        for k, v in mp.items():
            if 0 <= k < 256:
                lut[k] = v
        for k, v in inv.items():
            if 0 <= k < 256:
                inv_lut[k] = v
        # Sentinel roundtrip sanity (non-fatal)
        try:
            s = mp.get(-1, None)
            if s is not None and 0 <= s < 256:
                if inv_lut[lut[s]] != s:
                    self._quarantine.add(ns_key)
                    self._emit_diag("scrambler.sentinel_roundtrip_fail", ns_key=ns_key, sentinel=int(s))
        except Exception:
            pass
        self._lut_cache[ns_key] = lut
        self._inv_lut_cache[ns_key] = inv_lut

    def _to_gpu_if_available(self, arr: np.ndarray) -> Tuple[Any, bool]:
        if arr.size < _SCR_GPU_MIN_ELEMS:
            return arr, False
        try:
            import cupy as cp  # type: ignore
            return cp.asarray(arr), True
        except Exception:
            return arr, False

    # ---------------------- collapse-based scoring ----------------------
    def _grid_sample(self, grid: np.ndarray, ns_key: str, max_elems: int = 4096) -> np.ndarray:        
        g = np.asarray(grid, dtype=int)
        R, C = g.shape
        if R * C <= max_elems:
            return g
        # stride sample (deterministic by ns_key)
        rng = self._rng_for_ns(ns_key)
        sR = max(1, int(min(R, max(2, R // 32))))
        sC = max(1, int(min(C, max(2, C // 32))))
        r0 = rng.randrange(0, min(sR, R)) if R > sR else 0
        c0 = rng.randrange(0, min(sC, C)) if C > sC else 0
        return g[r0::sR, c0::sC]

    def _apply_mapping_to_sample(self, mapping: Dict[int,int], sample: np.ndarray) -> np.ndarray:
        lut = np.arange(256, dtype=int)
        for k, v in mapping.items():
            if 0 <= k < 256:
                lut[k] = v
        try:
            out = lut[sample]
        except Exception:
            vec = np.vectorize(lambda x: mapping.get(int(x), int(x)))
            out = vec(sample)
        # enforce sentinel on mask
        if -1 in mapping:
            out[sample == -1] = mapping[-1]
        return out

    @staticmethod
    def _estimate_cycles(p: List[int]) -> int:        
        n = len(p); seen = [False]*n; cycles = 0
        for i in range(n):
            if not seen[i]:
                j = i
                while not seen[j]:
                    seen[j] = True
                    j = p[j]
                cycles += 1
        return cycles

    def _score_candidates(self, ns_key: str, grid: np.ndarray, cands: List[List[int]]) -> List[Dict[str, Any]]:        
        sample = self._grid_sample(grid, ns_key)
        rows: List[Dict[str, Any]] = []

        scorer = globals().get("score_candidate_via_collapse")
        tracer = globals().get("rhcm_collapse_trace")

        for p in cands:
            # build mapping with reserved sentinel
            mp = {i: p[i] for i in range(10)}
            used = set(mp.values())
            sentinel = next((v for v in range(255, -1, -1) if v not in used), 255)
            mp[-1] = sentinel

            arr = self._apply_mapping_to_sample(mp, sample)

            if callable(scorer):
                try:
                    V = float(scorer(arr))
                    dS = EPI = sigma = 0.0
                    if callable(tracer):
                        try:
                            tr = tracer(arr)
                            dS   = float(tr.get("entropy_slope", 0.0))
                            EPI  = float(tr.get("epi_tail", 0.0))
                            sigma= float(tr.get("sym_dev", 0.0))
                        except Exception:
                            pass
                    rows.append({
                        "mp": mp, "perm": p,
                        "V": V, "Vn": V,
                        "dS": dS, "EPI": EPI, "sigma": sigma,
                        "fixed": sum(1 for i,v in enumerate(p) if i==v)
                    })
                    continue
                except Exception:
                    pass

            # --- fallback (no scorer available) ---
            fixed = sum(1 for i,v in enumerate(p) if i == v)
            inv_cycles = self._estimate_cycles(p)
            rows.append({
                "mp": mp, "perm": p,
                "V": float(-(fixed) - 0.05 * inv_cycles),
                "Vn": float(-(fixed) - 0.05 * inv_cycles),
                "dS": 0.0, "EPI": 0.0, "sigma": 0.0,
                "fixed": int(fixed), "tb": self._rng_for_ns(ns_key).random()
            })

        # Sort by score; tie-break by fewer fixed-points


# ==========================================================
# COLLAPSE INVARIANTS PHYSICS HARNESS (hardened + unified)
# ==========================================================
try:
    _COLLAPSE_INVARIANTS_INSTALLED
except NameError:
    _COLLAPSE_INVARIANTS_INSTALLED = True
   
    def _envi(key: str, dflt: int) -> int:
        try: return int(float(os.getenv(key, str(dflt))))
        except Exception: return dflt

    CINV_MAXC_DEFAULT = _envi("CINV_MAXC_DEFAULT", 10)
    CINV_SMOOTH_DIAL  = _envf("CINV_SMOOTH_DIAL", 0.02)
    CINV_PRIOR_BLEND  = _envf("CINV_PRIOR_BLEND", 0.10)
    CINV_TOL_ENT      = _envf("CINV_TOL_ENT", 0.02)
    CINV_TOL_EPI      = _envf("CINV_TOL_EPI", 0.10)
    CINV_TOL_BIND     = _envf("CINV_TOL_BIND", 0.08)
    CINV_DIAL_KFLUX   = _envf("CINV_DIAL_KFLUX", 0.05)
    CINV_DIAL_KEEL    = _envf("CINV_DIAL_KEEL",  0.03)
    CINV_DIAL_BIND    = _envf("CINV_DIAL_BIND",  0.04)

    # ---------- Kairos local tick (optional; monotonic) ----------
    _local_kairos_t = 0
    def _kairos_step_once():
        try:
            if PHYS_ALLOW_LOCAL_TICK and "kairos" in globals() and getattr(kairos, "step", None):
                global _local_kairos_t
                _local_kairos_t += 1
                kairos.step(_local_kairos_t)  # deterministic, no wall-clock
        except Exception:
            pass

    # ---------- Core helpers ----------
    def _to_np(g: Any) -> np.ndarray:
        arr = g if isinstance(g, np.ndarray) else np.array(g, dtype=int)
        if arr.ndim != 2:
            raise ValueError("Grid must be 2D.")
        return arr

    def _infer_maxc(arr: np.ndarray, fallback: int = CINV_MAXC_DEFAULT) -> int:
        try: return int(max(1, int(arr.max()) + 1))
        except Exception: return int(max(1, fallback))

    def _color_hist(arr: np.ndarray, maxc: int) -> np.ndarray:
        a = np.clip(arr, 0, maxc - 1)
        h = np.bincount(a.ravel(), minlength=maxc)[:maxc].astype(float)
        s = h.sum()
        if s > 0: h /= s
        return h

    def _shannon_entropy(arr: np.ndarray, maxc: int) -> float:
        p = _color_hist(arr, maxc=maxc)
        p = p[(p > 0) & np.isfinite(p)]
        if p.size == 0: return 0.0
        return float(-np.sum(p * np.log(p + 1e-12)))

    def _autocorr_1(y: np.ndarray) -> float:
        if y.size < 2: return 0.0
        ym = y - y.mean()
        num = float(np.dot(ym[1:], ym[:-1]))
        den = float(np.sum(ym[:-1] ** 2) + 1e-12)
        return float(num / den)


    def _collapse_step(arr: np.ndarray, *, maxc: Optional[int] = None) -> np.ndarray:
        H, W = arr.shape
        mc = int(max(1, _infer_maxc(arr) if maxc is None else maxc))

        # palette prior
        prior = _color_hist(arr, maxc=mc)  # sums to 1 when present
        prior = np.asarray(prior, dtype=float)

        # onehot planes
        onehot = np.zeros((mc, H, W), dtype=float)
        for c in range(mc):
            onehot[c] = (arr == c).astype(float)

        # Kairos smoothing dial (context only; no step here)
        k_mod = 1.0
        try:
            if "kairos" in globals():
                k_flux = float(getattr(kairos, "last_entropy_flux", 0.0))
                k_mod = 1.0 + CINV_SMOOTH_DIAL * float(np.tanh(k_flux / 10.0))
        except Exception:
            pass

        # 4-neighbor consensus (vectorized by roll)
        smooth = np.zeros_like(onehot)
        for c in range(mc):
            ch = onehot[c]
            up = np.roll(ch, -1, axis=0)
            dn = np.roll(ch,  1, axis=0)
            lf = np.roll(ch,  1, axis=1)
            rt = np.roll(ch, -1, axis=1)
            base = (ch + up + dn + lf + rt) / 5.0

            # palette-aware prior blend
            if CINV_PRIOR_BLEND > 0.0:
                base = (1.0 - CINV_PRIOR_BLEND) * base + CINV_PRIOR_BLEND * prior[c]

            smooth[c] = base * k_mod

        idx = np.argmax(smooth, axis=0)
        return idx.astype(int)

    @dataclass
    class CollapseSignatures:
        entropy_trace: List[float]
        entropy_slope: float
        epi: float
        binder_trace: List[float]
        binder_last: float
        glyph_id: str

    def compute_invariants(grid: Any, steps: int = 24, maxc: int = CINV_MAXC_DEFAULT) -> CollapseSignatures:
        arr = _to_np(grid)
        steps = int(max(1, steps))
        maxc = int(max(1, maxc))

        # Memoization (speeds up repeat calls)
        mkey = _akey(arr, ("inv", steps, maxc))
        hit = _memo_get("inv", mkey)
        if hit is not None:
            return hit

        # Local deterministic tick (optional)
        _kairos_step_once()

        ent, bind = [], []
        x = arr.copy()
        for _ in range(steps):
            ent.append(_shannon_entropy(x, maxc=maxc))
            bind.append(_binder_like(x, maxc=maxc))
            x = _collapse_step(x, maxc=maxc)

        ent = np.array(ent, dtype=float)
        t = np.arange(len(ent))
        denom = float(np.sum((t - t.mean())**2) + 1e-12)
        slope = float(np.sum((t - t.mean()) * (ent - ent.mean())) / denom)
        epi = _autocorr_1(ent)

        # include key Kairos/KEEL scalars to lock context
        try:
            k_state = getattr(kairos, "symbolic_state", "Ω₀")
            k_flux  = float(getattr(kairos, "last_entropy_flux", 0.0))
            keel_r  = float(getattr(kairos, "keel_ratio_avg", 1.0))
            keel_ps = float(getattr(kairos, "keel_psnr_avg", 0.0))
            keel_ss = float(getattr(kairos, "keel_ssim_avg", 0.0))
        except Exception:
            k_state, k_flux, keel_r, keel_ps, keel_ss = "Ω₀", 0.0, 1.0, 0.0, 0.0

        def _glyph_hash(obj: dict) -> str:
            try:
                s = json.dumps(obj, sort_keys=True, separators=(",", ":"))
            except Exception:
                s = repr(obj)
            return hashlib.sha256(s.encode("utf-8")).hexdigest()[:16]

        glyph = _glyph_hash({
            "ent": np.round(ent, 6).tolist(),
            "bind": np.round(np.array(bind, dtype=float), 6).tolist(),
            "kairos_state": k_state,
            "kairos_flux": k_flux,
            "keel_r": keel_r,
            "keel_psnr": keel_ps,
            "keel_ssim": keel_ss,
        })

        sig = CollapseSignatures(
            entropy_trace=list(map(float, ent)),
            entropy_slope=float(slope),
            epi=float(epi),
            binder_trace=list(map(float, bind)),
            binder_last=float(bind[-1] if bind else 0.0),
            glyph_id=glyph,
        )

        _safe_emit("collapse.invariants", {
            "slope": sig.entropy_slope, "epi": sig.epi, "binder": sig.binder_last,
            "glyph": sig.glyph_id, "kairos_flux": k_flux, "keel_ratio": keel_r,
            "keel_psnr": keel_ps, "keel_ssim": keel_ss
        })

        _memo_put("inv", mkey, sig)
        return sig

    @dataclass
    class PairScore:
        stable: bool
        entropy_delta: float
        epi_pair: float
        binder_delta: float
        glyph_in: str
        glyph_out: str
        reason: str

    def _score_from_sigs(si: CollapseSignatures, so: CollapseSignatures,
                         ent_tol: float, epi_tol: float, binder_tol: float) -> PairScore:
        # dynamic tolerance modulation (Kairos/KEEL)
        try:
            k_flux = float(getattr(kairos, "last_entropy_flux", 0.0))
            keel_r = float(getattr(kairos, "keel_ratio_avg", 1.0))
        except Exception:
            k_flux, keel_r = 0.0, 1.0

        ent_tol_dyn    = float(ent_tol    * (1.0 + CINV_DIAL_KFLUX * np.tanh(k_flux / 20.0)))
        epi_tol_dyn    = float(epi_tol    * (1.0 + CINV_DIAL_KEEL  * np.tanh((keel_r - 1.0))))
        binder_tol_dyn = float(binder_tol * (1.0 + CINV_DIAL_BIND  * np.tanh(k_flux / 25.0)))

        d_ent   = float(so.entropy_trace[-1] - si.entropy_trace[-1])
        d_bind  = float(so.binder_last - si.binder_last)
        epi_gap = float(abs(so.epi - si.epi))
        stable  = (d_ent <= ent_tol_dyn) and (epi_gap <= epi_tol_dyn) and (abs(d_bind) <= binder_tol_dyn)

        reason = []
        if not stable:
            if d_ent > ent_tol_dyn:   reason.append(f"entropy↑({d_ent:+.3f})>{ent_tol_dyn:.3f}")
            if epi_gap > epi_tol_dyn: reason.append(f"epi_gap({epi_gap:.3f})>{epi_tol_dyn:.3f}")
            if abs(d_bind) > binder_tol_dyn: reason.append(f"binderΔ({d_bind:+.3f})>{binder_tol_dyn:.3f}")

        payload = {
            "stable": bool(stable), "d_ent": d_ent, "epi_gap": epi_gap, "d_bind": d_bind,
            "ent_tol": float(ent_tol_dyn), "epi_tol": float(epi_tol_dyn), "binder_tol": float(binder_tol_dyn),
            "glyph_in": si.glyph_id, "glyph_out": so.glyph_id
        }
        _safe_emit("collapse.score_pair", payload)

        return PairScore(
            stable=stable,
            entropy_delta=d_ent,
            epi_pair=epi_gap,
            binder_delta=d_bind,
            glyph_in=si.glyph_id,
            glyph_out=so.glyph_id,
            reason="; ".join(reason) if reason else "within tolerances",
        )

    def score_pair(grid_in: Any, grid_out: Any,
                   ent_tol: float = CINV_TOL_ENT, epi_tol: float = CINV_TOL_EPI, binder_tol: float = CINV_TOL_BIND) -> PairScore:
        si = compute_invariants(grid_in)
        so = compute_invariants(grid_out)
        return _score_from_sigs(si, so, ent_tol, epi_tol, binder_tol)

    @dataclass
    class TaskReport:
        pair_scores: List[PairScore]
        mean_entropy_slope_in: float
        mean_entropy_slope_out: float
        binder_crossing_hint: bool
        note: str

    def analyze_task(task: Dict[str, Any],
                     ent_tol: float = CINV_TOL_ENT, epi_tol: float = CINV_TOL_EPI, binder_tol: float = CINV_TOL_BIND) -> TaskReport:
        train = task.get("train", [])
        if not train:
            raise ValueError("Task has no train pairs.")
        scores: List[PairScore] = []
        ent_slopes_in: List[float] = []
        ent_slopes_out: List[float] = []

        for pair in train:
            gi = _to_np(pair["input"]) if isinstance(pair, dict) else _to_np(pair[0])
            go = _to_np(pair["output"]) if isinstance(pair, dict) else _to_np(pair[1])
            si = compute_invariants(gi)
            so = compute_invariants(go)
            ps = _score_from_sigs(si, so, ent_tol, epi_tol, binder_tol)
            scores.append(ps)
            ent_slopes_in.append(si.entropy_slope)
            ent_slopes_out.append(so.entropy_slope)

        bsigns = [np.sign(ps.binder_delta) for ps in scores]
        crossing_hint = (len(set(map(int, bsigns))) > 1) if bsigns else False

        rep = TaskReport(
            pair_scores=scores,
            mean_entropy_slope_in=float(np.mean(ent_slopes_in)) if ent_slopes_in else 0.0,
            mean_entropy_slope_out=float(np.mean(ent_slopes_out)) if ent_slopes_out else 0.0,
            binder_crossing_hint=bool(crossing_hint),
            note="Use glyph_in/out to enforce glyph-constrained search; use 'stable' to early-exit."
        )

        _safe_emit("collapse.task_report", {
            "pair_scores": len(scores),
            "slope_in": rep.mean_entropy_slope_in,
            "slope_out": rep.mean_entropy_slope_out,
            "crossing": rep.binder_crossing_hint
        })

        # optional local deterministic tick
        _kairos_step_once()
        try:
            st = getattr(kairos, "get_state", lambda: {})()
            _safe_emit("collapse.analysis_pulse", st if isinstance(st, dict) else {})
        except Exception:
            pass

        return rep

    def glyph_constrained_candidates(candidates: List[np.ndarray],
                                     target_glyph: str,
                                     max_keep: int = 20,
                                     steps: int = 24,
                                     maxc: int = CINV_MAXC_DEFAULT) -> List[np.ndarray]:
        out: List[np.ndarray] = []
        for c in candidates:
            try:
                g = compute_invariants(c, steps=steps, maxc=maxc).glyph_id
                if g == target_glyph:
                    out.append(c)
                    if len(out) >= max_keep:
                        break
            except Exception:
                pass
        _safe_emit("collapse.glyph_filter", {"target": target_glyph, "kept": len(out), "scanned": len(candidates)})
        return out

    def early_exit_if_stable(current_grid: np.ndarray,
                             ref_entropy: float,
                             ent_tol: float = CINV_TOL_ENT,
                             steps: int = 12,
                             maxc: int = CINV_MAXC_DEFAULT) -> bool:
        sig = compute_invariants(current_grid, steps=steps, maxc=maxc)
        ok = (abs(sig.entropy_trace[-1] - float(ref_entropy)) <= float(ent_tol))
        _safe_emit("collapse.early_exit", {"ok": bool(ok), "ent": float(sig.entropy_trace[-1]), "ref": float(ref_entropy)})
        return ok

    # Pretty report (utility)
    def _pretty_report(rep: 'TaskReport') -> str:
        lines = ["== ARC Collapse Report =="]
        for i, ps in enumerate(rep.pair_scores, 1):
            lines.append(
                f" Pair {i}: stable={ps.stable} | ΔS={ps.entropy_delta:+.3f} | "
                f"EPI_gap={ps.epi_pair:.3f} | BinderΔ={ps.binder_delta:+.3f} | "
                f"glyph_in={ps.glyph_in} | glyph_out={ps.glyph_out} | {ps.reason}"
            )
        lines.append(f" mean entropy slope (in)  = {rep.mean_entropy_slope_in:+.5f}")
        lines.append(f" mean entropy slope (out) = {rep.mean_entropy_slope_out:+.5f}")
        lines.append(f" binder crossing hint     = {rep.binder_crossing_hint}")
        lines.append(f" note: {rep.note}")
        return "\n".join(lines)
        # ===== At the very end of COLLAPSE INVARIANTS section =====
    def _register_invariants_api_to_globals():
        api = {
            "compute_invariants": compute_invariants,
            "score_pair": score_pair,
            "glyph_constrained_candidates": glyph_constrained_candidates,
            "early_exit_if_stable": early_exit_if_stable,
            # optional: public dataclasses for type hints / external use
            "CollapseSignatures": CollapseSignatures,
            "PairScore": PairScore,
            "TaskReport": TaskReport,
        }
        for name, obj in api.items():
            # don't overwrite if a caller already provided a stub/override
            if globals().get(name) is None:
                globals()[name] = obj
        globals().setdefault("INVARIANTS_API_SCHEMA", "cinv/1")

    _register_invariants_api_to_globals()


# ==========================================================
# Invariant Scorer (unified, Meta-compatible comps keys)
# ==========================================================

def meta_comps_from_pairscore(ps: "PairScore") -> Dict[str, Any]:
    return {
        "entropy_delta": float(ps.entropy_delta),
        "epi_gap": float(ps.epi_pair),
        "binder_delta": float(ps.binder_delta),
        "stable": bool(ps.stable),
        # legacy keys for existing consumers
        "dH": float(ps.entropy_delta),
        "epi": float(ps.epi_pair),
        "binder": float(ps.binder_delta),
    }


class InvariantScorer:
    def __init__(self, logger=None, steps: int = 24, maxc: int = CINV_MAXC_DEFAULT):
        self.logger = logger
        self.steps = int(steps)
        self.maxc = int(maxc)

    def score_pair(self, x: np.ndarray, y: np.ndarray) -> dict:
        ps = score_pair(x, y)
        comps = meta_comps_from_pairscore(ps)  # Canonical + legacy keys
        penalty = abs(ps.entropy_delta) + abs(ps.binder_delta) + abs(ps.epi_pair)
        score_scalar = (1.0 if ps.stable else 0.0) - penalty
        metrics = {
            **comps,
            "score": float(score_scalar),
            "fits": bool(ps.stable),
            "glyph_in": ps.glyph_in,
            "glyph_out": ps.glyph_out,
            "reason": ps.reason,
        }
        _safe_emit("invariants.metrics", metrics)
        return metrics

    def compute_invariants(self, grid: np.ndarray) -> dict:
        sig = compute_invariants(grid, steps=self.steps, maxc=self.maxc)
        out = {
            "entropy_slope": sig.entropy_slope,
            "epi": sig.epi,
            "binder": sig.binder_last,
            "glyph_id": sig.glyph_id,
            "entropy_trace": sig.entropy_trace,
            "binder_trace": sig.binder_trace,
        }
        _safe_emit("invariants.signatures", {
            "glyph": sig.glyph_id,
            "slope": sig.entropy_slope,
            "epi": sig.epi,
            "binder": sig.binder_last
        })
        return out

    def analyze_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        try:
            rep = analyze_task(task)  # uses optimized harness
            out = {
                "mean_entropy_slope_in": rep.mean_entropy_slope_in,
                "mean_entropy_slope_out": rep.mean_entropy_slope_out,
                "binder_crossing_hint": rep.binder_crossing_hint,
                "note": rep.note,
            }
            _safe_emit("invariants.task_analysis", {
                "slope_in": out["mean_entropy_slope_in"],
                "slope_out": out["mean_entropy_slope_out"],
                "binder_cross": out["binder_crossing_hint"]
            })
            return out
        except Exception as e:
            if self.logger:
                try: self.logger.error(f"[InvariantScorer] analyze_task failed: {e}")
                except Exception: pass
            return {}


def invariants_score_pair(x: np.ndarray, y: np.ndarray) -> dict:
    return InvariantScorer().score_pair(x, y)



# ==========================================================
# Origin normalization
# ==========================================================

def _norm_origin(meta: Optional[Dict[str, Any]]) -> str:
    allow = {
        "solver", "sandbox", "meta", "human", "generator", "kb_xform", "episodic",
        "direct_replay", "xform_proxy", "inference", "none"
    }
    m = meta or {}
    o = str(m.get("origin", "")).strip().lower()
    if o in allow: return o
    d = str(m.get("discovery", "")).strip().lower()
    if d in allow: return d
    return "none"

# =========================================================
# KEEL + provenance + signatures (clean, monolith-safe)
# =========================================================

# --- minimal dtype/grid helpers ---

def _as_u8(x: np.ndarray) -> np.ndarray:
    return x if x.dtype == np.uint8 else np.clip(x, 0, 255).astype(np.uint8, copy=False)

# --- KEEL codec wrappers (grid <-> bytes) ---
# expects globals: keel_encode, keel_decode, psnr, ssim_proxy

def keel_compress_grid(grid_u8: np.ndarray, q_ll: float = 3.0, deblock: bool = True) -> Tuple[bytes, Dict[str, Any]]:
    if grid_u8.ndim != 2:
        raise AssertionError("KEEL expects single-channel grids")
    if "keel_encode" not in globals():
        _safe_emit("keel.missing", {"what": "keel_encode"})
        raise RuntimeError("keel_encode not available")
    blob, meta = keel_encode(_as_u8(grid_u8), q_ll=q_ll, deblock=deblock, sym_agent=None)  # type: ignore[name-defined]
    return blob, meta


def keel_decompress_grid(blob: bytes) -> np.ndarray:
    if "keel_decode" not in globals():
        _safe_emit("keel.missing", {"what": "keel_decode"})
        raise RuntimeError("keel_decode not available")
    return keel_decode(blob)  # type: ignore[name-defined]


def keel_metrics(a: np.ndarray, b: np.ndarray) -> Dict[str, float]:
    if ("psnr" not in globals()) or ("ssim_proxy" not in globals()):
        _safe_emit("keel.metrics_missing", {"psnr": ("psnr" in globals()), "ssim_proxy": ("ssim_proxy" in globals())})
        return {}
    a8, b8 = _as_u8(a), _as_u8(b)
    if a8.shape != b8.shape:
        meta_log("keel.metrics.shape_mismatch", a_shape=tuple(map(int, a8.shape)), b_shape=tuple(map(int, b8.shape)))
        raise ValueError(f"keel_metrics requires matching shapes, got {a8.shape} vs {b8.shape}")
    out = {
        "psnr": float(psnr(a8, b8)),            # type: ignore[name-defined]
        "ssim_proxy": float(ssim_proxy(a8, b8)) # type: ignore[name-defined]
    }
    # Optional extra metrics (env-gated)
    try:
        if str(os.getenv("HOLO_EXTRA_METRICS", "0")).strip().lower() in ("1", "true"):
            diff = a8.astype(np.float32) - b8.astype(np.float32)
            out["mae"]  = float(np.mean(np.abs(diff)))
            out["rmse"] = float(np.sqrt(np.mean(diff**2)))
    except Exception:
        pass
    return out

# --- physics/invariant fields (compute_invariants must exist) ---

def _phys_fields(g: np.ndarray) -> Dict[str, Any]:
    try:
        inv = compute_invariants(g)
        return {
            "glyph": getattr(inv, "glyph_id", None),
            "epi": float(getattr(inv, "epi", 0.0)),
            "binder": float(getattr(inv, "binder_last", getattr(inv, "binder", 0.0))),
            "entropy_slope": float(getattr(inv, "entropy_slope", 0.0)),
        }
    except Exception:
        return {}

# --- grid signatures (stable hashes; JSON-safe) ---

def _sha1_grid(g: np.ndarray) -> str:
    g_c = np.ascontiguousarray(g)
    header = f"{g_c.dtype.str}|{g_c.shape}|".encode("utf-8")
    h = hashlib.sha1(); h.update(header); h.update(g_c.tobytes())
    return h.hexdigest()


def _blake2b16_grid(g: np.ndarray) -> str:
    g_c = np.ascontiguousarray(g)
    header = f"{g_c.dtype.str}|{g_c.shape}|".encode("utf-8")
    h = hashlib.blake2b(digest_size=8); h.update(header); h.update(g_c.tobytes())
    return h.hexdigest()


def grid_signature(g: np.ndarray) -> Dict[str, str]:
    g8 = _as_u8(g)
    return {"sha1": _sha1_grid(g8), "b2b16": _blake2b16_grid(g8)}


def _pair_sig(inp: np.ndarray, out: np.ndarray) -> str:
    return f"{_sha1_grid(_as_u8(inp))}::{_sha1_grid(_as_u8(out))}"

# --- HybridSimilarity shim (composite invariant-aware similarity) ---
# returns both heuristic fields and canonical Meta comps keys

def invariant_composite(a: np.ndarray, b: np.ndarray) -> Dict[str, float]:
    ps = score_pair(a, b)  # type: ignore[name-defined]
    return {
        "score": float((1.0 if getattr(ps, "stable", False) else 0.0)
                       - (abs(getattr(ps, "entropy_delta", 0.0))
                          + abs(getattr(ps, "binder_delta", 0.0))
                          + abs(getattr(ps, "epi_pair", 0.0)))),
        "epi": float(getattr(ps, "epi_pair", 0.0)),
        "binder": float(getattr(ps, "binder_delta", 0.0)),
        "entropy": float(1.0 - abs(getattr(ps, "entropy_delta", 0.0))),
        "symmetry": float(1.0 - min(1.0, abs(getattr(ps, "binder_delta", 0.0)) + abs(getattr(ps, "epi_pair", 0.0)))),
        # canonical fields for Meta firewall:
        "entropy_delta": float(ps.entropy_delta),
        "epi_gap": float(ps.epi_pair),
        "binder_delta": float(ps.binder_delta),
        "stable": bool(ps.stable),
    }

# --- (optional) batched invariants helper ---

def compute_invariants_batch(grids: List[np.ndarray], steps: int = 24, maxc: int = 256) -> List["CollapseSignatures"]:
    out = []
    for g in grids:
        try:
            out.append(compute_invariants(np.asarray(g, dtype=int), steps=steps, maxc=maxc))
        except Exception:
            out.append(compute_invariants(np.asarray(g, dtype=int)))
    return out

# ===========================================
# Hybrid Similarity (physics, invariants, ranking)
# ===========================================
def _fallback_boundary_flip_score(err_field: np.ndarray) -> float:
    try:
        if err_field is None:
            return 1.0
        e = np.array(err_field, dtype=float)
        e = np.abs(e - np.flip(e, axis=1)) if e.ndim == 2 else np.abs(e)
        m = float(np.mean(e)) if e.size else 1.0
        return float(max(0.0, min(1.0, m)))
    except Exception:
        return 1.0

class HybridSimilarity:
    def __init__(self,
                 meta: Optional[Any]=None,
                 kb: Optional[Any]=None,
                 sandbox: Optional[Any]=None,
                 ultra: Optional[Any]=None,
                 kairos: Optional[Any]=None,
                 holo: Optional[Any]=None,
                 encoder: Optional[Any]=None):
        self.meta    = meta
        self.kb      = kb
        self.sandbox = sandbox
        self.ultra   = ultra
        self.kairos  = kairos
        self.holo    = holo
        self.encoder = encoder

        self.history: deque = deque(maxlen=512)
        self.weights: Dict[str, float] = {"epi": 0.5, "binder": 0.2, "entropy": 0.2, "symmetry": 0.1}
        try:
            self._bonus_map = _load_prov_bonus()
        except Exception:
            self._bonus_map = {"none": 0.0}
        self._emit = _ModuleEmitter(meta=meta, module_name="HybridSimilarity")
        self._cal  = SigmoidCalibrator()
        self._global_cal = globals().get("HYBRID_GLOBAL_CAL", None)

    def set_context(self, *, meta=None, kb=None, sandbox=None, ultra=None, kairos=None, holo=None, encoder=None):
        if meta    is not None: self.meta = meta
        if kb      is not None: self.kb = kb
        if sandbox is not None: self.sandbox = sandbox
        if ultra   is not None: self.ultra = ultra
        if kairos  is not None: self.kairos = kairos
        if holo    is not None: self.holo = holo
        if encoder is not None: self.encoder = encoder
        return self

    def _holo_echo(self, tag: str, payload: dict):
        try:
            if self.holo is None or not hasattr(self.holo, "_telemetry"):
                return
            self.holo._telemetry(f"hybrid.{tag}", **payload)
        except Exception:
            pass

    def _provenance_bonus(self, source: str) -> float:
        try:
            if self.meta is not None and hasattr(self.meta, "_provenance_bonus"):
                return float(self.meta._provenance_bonus(source))
        except Exception:
            pass
        return float(self._bonus_map.get(str(source) or "none", 0.0))

    def _kb_glyph_penalty(self, a: np.ndarray, b: np.ndarray) -> float:
        try:
            kb = self.kb or getattr(self.meta, "kb", None)
            if kb is None:
                return 0.0
            inv_fn = globals().get("compute_invariants")
            if not callable(inv_fn):
                return 0.0
            gi = inv_fn(b)
            glyph = getattr(gi, "glyph_id", None)
            idx = getattr(kb, "idx_failures_by_glyph", {}) or {}
            val = idx.get(glyph, 0.0)
            if isinstance(val, (tuple, list)) and len(val) == 2:
                fail, seen = float(val[0]), float(val[1])
                rate = (fail / max(1.0, seen))
            else:
                rate = float(val) * 0.01
            return float(min(0.05, max(0.0, rate)))
        except Exception:
            return 0.0

    def _kairos_weight_nudge(self):
        try:
            k = self.kairos or getattr(self.meta, "kairos", None)
            flux = float(getattr(k, "last_entropy_flux", 0.0)) if k else 0.0
            d = 0.02 * math.tanh(flux / 50.0)
            self.weights["epi"]      = float(max(0.0, min(1.0, self.weights["epi"] + d)))
            self.weights["entropy"]  = float(max(0.0, min(1.0, self.weights["entropy"] - d)))
            s = sum(self.weights.values())
            if s > 0:
                for kk in self.weights:
                    self.weights[kk] /= s
        except Exception:
            pass

    def composite(self, a: np.ndarray, b: np.ndarray) -> Dict[str, float]:
        sp = globals().get("score_pair")
        ef_fn = globals().get("error_field")
        bf_fn = globals().get("boundary_flip_score")

        dH = epi = bnd = 0.0; stable = False
        if callable(sp):
            try:
                ps = sp(a, b)
                dH   = float(getattr(ps, "entropy_delta", 0.0))
                epi  = float(getattr(ps, "epi_pair", 0.0))
                bnd  = float(getattr(ps, "binder_delta", 0.0))
                stable = bool(getattr(ps, "stable", False))
            except Exception:
                pass

        sym_sim = 0.0
        try:
            if callable(ef_fn):
                ef = ef_fn(a, b)
                if callable(bf_fn):
                    sym_dev = float(bf_fn(ef))
                else:
                    sym_dev = _fallback_boundary_flip_score(ef)
                sym_sim = float(max(0.0, min(1.0, 1.0 - sym_dev)))
        except Exception:
            pass

        self._kairos_weight_nudge()

        epi_sim     = float(1.0 - abs(epi))
        binder_sim  = float(1.0 - abs(bnd))
        entropy_sim = float(1.0 - abs(dH))
        score = (
            self.weights["epi"]      * epi_sim +
            self.weights["binder"]   * binder_sim +
            self.weights["entropy"]  * entropy_sim +
            self.weights["symmetry"] * sym_sim
        )
        score = float(max(0.0, score - self._kb_glyph_penalty(a, b)))
        p_conf = float(self._cal.predict(score))
        try:
            if callable(self._global_cal):
                p_conf = float(self._global_cal(p_conf))
        except Exception:
            pass

        rec = {"score": score, "p_conf": p_conf, "epi": epi_sim, "binder": binder_sim,
               "entropy": entropy_sim, "symmetry": sym_sim, "stable": bool(stable)}
        self._emit.emit("hybrid.composite", **rec)
        try:
            if self.ultra is not None and hasattr(self.ultra, "observe"):
                self.ultra.observe("hybrid_composite", **rec)
        except Exception:
            pass
        try:
            if self.sandbox is not None and hasattr(self.sandbox, "trace_feedback"):
                self.sandbox.trace_feedback({"source": "hybrid", **rec})
        except Exception:
            pass
        self._holo_echo("composite", rec)
        return rec

    def nudge_after_outcome(self, comps: Dict[str, float], success: bool):
        base = float(comps.get("score", 0.0))
        lr = 0.02 if success else -0.02
        for k in list(self.weights.keys()):
            v = float(comps.get({"epi": "epi", "binder": "binder",
                                 "entropy": "entropy", "symmetry": "symmetry"}[k], 0.0))
            self.weights[k] = float(max(0.0, min(1.0, self.weights[k] + lr * v)))
        s = sum(self.weights.values())
        if s > 0:
            for k in self.weights:
                self.weights[k] /= s
        self.history.append({"score": base, "success": bool(success)})
        payload = {"success": bool(success), "base": float(base),
                   "w_epi": float(self.weights.get("epi", 0.0)),
                   "w_binder": float(self.weights.get("binder", 0.0)),
                   "w_entropy": float(self.weights.get("entropy", 0.0)),
                   "w_symmetry": float(self.weights.get("symmetry", 0.0))}
        self._emit.emit("hybrid.nudge", **payload)
        try:
            if self.encoder is not None and hasattr(self.encoder, "record_feedback"):
                self.encoder.record_feedback(label="hybrid", memory_layer="general", success=bool(success))
        except Exception:
            pass

    def rank_candidates(
        self,
        candidates: List[Tuple[np.ndarray, Dict[str, Any]]],
        base: Optional[np.ndarray] = None
    ) -> List[Tuple[np.ndarray, float, float]]:
        scored: List[Tuple[np.ndarray, float, float]] = []
        _blender = HybridConfidenceBlender(meta=self.meta)
        for grid, meta in (candidates or []):
            if base is not None:
                m = _blender.blended_score(self, base, grid)
                s = float(m.get("blended", 0.0))
                p = float(m.get("p_conf", 0.0))
            else:
                s = float((meta or {}).get("score", 0.0))
                p = float(self._cal.predict(s))
            s += self._provenance_bonus((meta or {}).get("source", "none"))
            try:
                s -= self._kb_glyph_penalty(base if base is not None else grid, grid)
            except Exception:
                pass
            scored.append((grid, s, p))
        scored.sort(key=lambda t: (-t[2], -t[1]))
        self._emit.emit("hybrid.rank", n=int(len(scored)), top=float(scored[0][2] if scored else 0.0))
        return scored

def trace_scatter(self, results: List[Dict[str, Any]], prefix="hybrid_trace"):
    # Plotting is optional; guard imports and filesystem
    try:
        import matplotlib.pyplot as plt
        xs = [float(r.get("sim", r.get("score", 0.0))) for r in results]
        ys = [float(r.get("score", 0.0)) for r in results]
        colors = ["green" if r.get("stable", False) else "red" for r in results]
        path = os.path.join("exports", f"{prefix}.png")
        os.makedirs(os.path.dirname(path), exist_ok=True)
        plt.figure(figsize=(6, 4))
        plt.scatter(xs, ys, alpha=0.6, c=colors)
        plt.title("Hybrid Blender Trace"); plt.xlabel("Similarity"); plt.ylabel("Blended/Score")
        plt.tight_layout(); plt.savefig(path); plt.close()
        self._emit.emit("hybrid.trace", path=path, n=int(len(results)))
        self._holo_echo("trace", {"path": path, "n": int(len(results))})
    except Exception:
        pass

# ---- op priority ----
if "_op_priority" not in globals():
    _OP_PRIORITIES = {
        "rot": 0.70, "flip_lr": 0.65, "flip_ud": 0.65, "flip_diag": 0.60,
        "trans": 0.55, "pad_to": 0.55, "crop_to": 0.55, "mass_pad": 0.58,
        "entropy_crop": 0.60, "recolor": 0.50, "hist_recolor": 0.50, "symmetry": 0.52,
    }
    def _op_priority(name: str) -> float:
        return float(_OP_PRIORITIES.get(str(name), 0.5))

# ---- contextual op prior ----
if "_get_op_prior" not in globals():
    def _get_op_prior(name: str, arr=None) -> float:
        return 0.0

# ---- entropy persistence ----
if "_entropy_persistence_index" not in globals():
    def _entropy_persistence_index(inp, out) -> float:
        try:
            Hin = _entropy(inp); Hou = _entropy(out)
            m = max(Hin, Hou, 1e-12)
            return float(max(0.0, min(1.0, 1.0 - abs(Hin - Hou) / m)))
        except Exception:
            return 0.0

# ============================================================
# Hybrid Confidence Blender (Integrated with Invariants Harness)
# ============================================================
class HybridConfidenceBlender:
    def __init__(self, alpha=0.70, beta=0.10, gamma=0.12, delta=0.08,
                 meta: Optional[Any]=None,
                 logger: Optional[Any]=None):
        self.alpha = float(alpha)
        self.beta  = float(beta)
        self.gamma = float(gamma)
        self.delta = float(delta)
        self.meta  = meta
        self.log   = logger if logger is not None else (globals().get("EXPLAIN", None))
        self._emit = _ModuleEmitter(meta=meta, module_name="HybridConfidenceBlender")
        self._cal  = SigmoidCalibrator()
        # Optional global calibrator hook for coherence with Meta/Solver
        self._global_cal = globals().get("HYBRID_GLOBAL_CAL", None)

    def _apply_cal(self, x: float) -> float:
        try:
            if callable(self._global_cal):
                return float(self._global_cal(x))
        except Exception:
            pass
        return float(self._cal.predict(x))

    def blended_score(self, sim: Optional[Any], inp: np.ndarray,
                      pred: np.ndarray, ref: Optional[np.ndarray] = None) -> Dict[str, float]:
        # similarity component (ensure composite(inp, pred) ordering)
        try:
            if sim is None:
                s_sim = float((inp == pred).mean()) if inp.shape == pred.shape else 0.0
                comps = {"score": s_sim}
            else:
                comps = sim.composite(inp, pred)
                s_sim = float(comps.get("score", 0.0))
        except Exception:
            s_sim, comps = 0.0, {"score": 0.0}
        # physics via unified harness
        dH = comps.get("entropy", None)
        epi = comps.get("epi", None)
        bnd = comps.get("binder", None)
        stable = bool(comps.get("stable", False))
        try:
            if dH is None or epi is None or bnd is None:
                sp = globals().get("score_pair")
                if callable(sp):
                    ps = sp(inp, pred)
                    dH   = float(getattr(ps, "entropy_delta", 0.0)) if dH is None else float(dH)
                    epi  = float(getattr(ps, "epi_pair", 0.0))      if epi is None else float(epi)
                    bnd  = float(getattr(ps, "binder_delta", 0.0))  if bnd is None else float(bnd)
                    stable = bool(getattr(ps, "stable", stable))
        except Exception:
            dH = 0.0 if dH is None else float(dH)
            epi = 0.0 if epi is None else float(epi)
            bnd = 0.0 if bnd is None else float(bnd)

        blended = (
            self.alpha * s_sim +
            self.beta  * (1.0 - abs(float(dH or 0.0))) +
            self.gamma * (1.0 - abs(float(epi or 0.0))) +
            self.delta * (1.0 - abs(float(bnd or 0.0)))
        )
        p_conf = self._apply_cal(blended)

        # === Kairos micro-nudge on final blended confidence (±0.02 bounded) ===
        try:
            k = (self.meta.kairos if (self.meta is not None and hasattr(self.meta, "kairos")) else globals().get("kairos", None))
            flux = float(getattr(k, "last_entropy_flux", 0.0)) if k is not None else 0.0
        except Exception:
            flux = 0.0
        blend_delta = float(0.02 * math.tanh(flux / 50.0))
        p_conf_nudged = float(max(0.0, min(1.0, p_conf + blend_delta)))
        # Emit trace for auditability
        self._emit.emit("hybrid.blend_delta", delta=blend_delta, p_in=p_conf, p_out=p_conf_nudged, flux=flux)

        metrics = {
            "sim": float(s_sim),
            "entropy_delta": float(dH or 0.0),
            "epi_gap": float(epi or 0.0),
            "binder_delta": float(bnd or 0.0),
            "blended": float(blended),
            "p_conf": float(p_conf_nudged),
            "stable": bool(stable),
            "hybrid_degraded": bool(sim is None and not callable(globals().get("score_pair")))
        }
        try:
            if self.log is not None and hasattr(self.log, "log"):
                self.log.log("hybrid.blended_score", {"ok": True, **metrics})
        except Exception:
            pass
        self._emit.emit("hybrid.blended_score", flux_hint=float(abs(metrics["entropy_delta"])), **metrics)
        try:
            ul = globals().get("ultra")
            if ul is not None and hasattr(ul, "observe"):
                ul.observe("hybrid_blended_score", **metrics)
        except Exception:
            pass
        return metrics

    def blended_score_batch(self, sim: Optional[Any], inp: np.ndarray,
                            cands: List[np.ndarray]) -> List[Dict[str, float]]:
        out: List[Dict[str, float]] = []
        for c in cands or []:
            out.append(self.blended_score(sim, inp, c))
        return out

    def rerank(self, sim: Optional[Any], inp: np.ndarray,
               candidates: List[np.ndarray], ref: Optional[np.ndarray] = None) -> Tuple[np.ndarray, Dict[str, float]]:
        if not candidates:
            m = self.blended_score(sim, inp, inp, ref=ref)
            self._emit.emit("hybrid.rescore", n_candidates=0,
                            best_blended=float(m.get("blended", 0.0)),
                            best_p=float(m.get("p_conf", 0.0)))
            try:
                ex = globals().get("EXPLAIN")
                if ex and hasattr(ex, "log"):
                    ex.log("hybrid.pick", {"n": 0, "choice": "identity", **m})
            except Exception:
                pass
            # Also emit via module emitter for trace visibility
            self._emit.emit("hybrid.pick", n=0, choice="identity", **m)
            return inp, m

        best_idx, best, best_m = -1, None, {"blended": -1e9, "p_conf": -1.0}
        scores = []
        for i, c in enumerate(candidates):
            m = self.blended_score(sim, inp, c, ref=ref)
            scores.append({"i": i, **m})
            key = (m.get("p_conf", 0.0), m.get("blended", 0.0))
            if key > (best_m.get("p_conf", -1.0), best_m.get("blended", -1e9)):
                best_idx, best, best_m = i, c, m

        self._emit.emit("hybrid.rescore", n_candidates=len(candidates),
                        best_blended=float(best_m.get("blended", 0.0)),
                        best_p=float(best_m.get("p_conf", 0.0)))
        try:
            ex = globals().get("EXPLAIN")
            if ex and hasattr(ex, "log"):
                ex.log("hybrid.pick", {"best_idx": best_idx, "scores": scores[:12]})
            ul = globals().get("ultra")
            if ul and hasattr(ul, "observe"):
                ul.observe("hybrid_pick", best_idx=best_idx, best_p=best_m.get("p_conf", 0.0))
        except Exception:
            pass
        # Emit explicit pick trace event (downsampled) for auditability
        self._emit.emit("hybrid.pick", best_idx=best_idx, best_p=float(best_m.get("p_conf", 0.0)))
        return (best if best is not None else candidates[0]), best_m

# ============================================================
# Auto-Attach: Hybrid Hooks into Solver (Resilient, Idempotent)
# ============================================================
def attach_hybrid_to_solver(solver, **_ignored):
    try:
        import numpy as np  # noqa: F401
    except Exception:
        class _NPStub:
            def array(self, *_, **__): return []
            ndarray = list
        np = _NPStub()  # type: ignore

    import functools as _functools
    from functools import wraps as _wraps

    EXPLAIN = globals().get("EXPLAIN", None)
    if EXPLAIN is None or not hasattr(EXPLAIN, "log"):
        class _NoopExplain:
            def log(self, *_, **__): pass
        EXPLAIN = _NoopExplain()

    HybridSimilarity = globals().get("HybridSimilarity", None)
    HybridConfidenceBlender = globals().get("HybridConfidenceBlender", None)
    compute_invariants = globals().get("compute_invariants", None)
    glyph_constrained_candidates = globals().get("glyph_constrained_candidates", None)

    if HybridSimilarity is None or HybridConfidenceBlender is None:
        return False

    if not callable(globals().get("_to_np", None)):
        def _to_np(x):
            try:
                return x if isinstance(x, np.ndarray) else np.array(x, dtype=int)
            except Exception:
                return np.array([], dtype=int)
    else:
        _to_np = globals()["_to_np"]

    if not callable(glyph_constrained_candidates):
        def glyph_constrained_candidates(cands, ref_glyph, max_keep=None):
            return cands

    def _safe_log(evt: str, payload: dict):
        try:
            if EXPLAIN is not None and hasattr(EXPLAIN, "log"):
                EXPLAIN.log(evt, payload)
        except Exception:
            pass

    def _ultra_observe(where: str, payload: dict):
        try:
            ultra = getattr(solver, "ultra", None)
            if ultra and hasattr(ultra, "observe"):
                ultra.observe("hybrid", {"where": where, **payload})
        except Exception:
            pass

    def _maybe_glyph_gate(cands: list, ref_grid) -> list:
        if not cands:
            return cands
        try:
            ref_glyph = None
            if callable(compute_invariants) and ref_grid is not None:
                try:
                    inv = compute_invariants(ref_grid)
                    ref_glyph = getattr(inv, "glyph_id", None)
                except Exception:
                    ref_glyph = None
            if ref_glyph:
                return glyph_constrained_candidates(cands, ref_glyph, max_keep=min(len(cands), 64)) or cands
        except Exception:
            pass
        return cands

    try:
        sim = getattr(solver, "sim", None)
        if sim is None or not isinstance(sim, HybridSimilarity):
            sim = HybridSimilarity(meta=getattr(solver, "meta", None),
                                   kb=getattr(solver, "kb", None),
                                   sandbox=getattr(solver, "sandbox", None),
                                   ultra=getattr(solver, "ultra", None),
                                   kairos=getattr(solver, "kairos", None),
                                   holo=getattr(solver, "holo", None),
                                   encoder=getattr(getattr(solver, "ml", None), "encoder", None))
            setattr(solver, "sim", sim)
        blender = getattr(solver, "blender", None)
        if blender is None or not isinstance(blender, HybridConfidenceBlender):
            blender = HybridConfidenceBlender(meta=getattr(solver, "meta", None))
            setattr(solver, "blender", blender)
    except Exception:
        return False

    if getattr(solver, "_hybrid_wrapped", False):
        return True

    if hasattr(solver, "choose_best_prediction") and not hasattr(solver, "_hybrid_wrapped_cbp"):
        solver._hybrid_wrapped_cbp = solver.choose_best_prediction

        @_wraps(solver.choose_best_prediction)
        def _cbp_hybrid(inp, candidates, ref=None, *a, **kw):
            try:
                cands = list(candidates or [])
                cands = _maybe_glyph_gate(cands, ref)
                if not cands:
                    return solver._hybrid_wrapped_cbp(inp, candidates, ref, *a, **kw)
                best, metrics = blender.rerank(sim, _to_np(inp), cands, ref=_to_np(ref) if ref is not None else None)
                _safe_log("hybrid.choice", {"where": "choose_best_prediction",
                                            "n": int(len(cands)),
                                            "p_conf": float(metrics.get("p_conf", 0.0)),
                                            "blended": float(metrics.get("blended", 0.0))})
                _ultra_observe("choose_best_prediction", {"n": int(len(cands)), "p_conf": float(metrics.get("p_conf", 0.0))})
                return (best, metrics)
            except Exception as e:
                _safe_log("hybrid.choice.error", {"where": "choose_best_prediction", "err": str(e)})
                return solver._hybrid_wrapped_cbp(inp, candidates, ref, *a, **(kw or {}))
        solver.choose_best_prediction = _cbp_hybrid

    if hasattr(solver, "predict_one") and not hasattr(solver, "_hybrid_wrapped_predict_one"):
        solver._hybrid_wrapped_predict_one = solver.predict_one

        @_wraps(solver.predict_one)
        def _predict_one_hybrid(x, *a, **kw):
            res = solver._hybrid_wrapped_predict_one(x, *a, **kw)
            try:
                pred, meta = (res if isinstance(res, tuple) else (res, {}))
                inp = x.get("input") if isinstance(x, dict) else x
                m = blender.blended_score(sim, _to_np(inp), _to_np(pred))
                meta = dict(meta); meta["hybrid"] = m
                _safe_log("hybrid.pick", {"where": "predict_one",
                                          "p_conf": float(m.get("p_conf", 0.0)),
                                          "blended": float(m.get("blended", 0.0))})
                _ultra_observe("predict_one", {"p_conf": float(m.get("p_conf", 0.0))})
                return (pred, meta)
            except Exception as e:
                _safe_log("hybrid.pick.error", {"where": "predict_one", "err": str(e)})
                return res
        solver.predict_one = _predict_one_hybrid

    if hasattr(solver, "solve_task") and not hasattr(solver, "_hybrid_wrapped_solve_task"):
        solver._hybrid_wrapped_solve_task = solver.solve_task

        @_wraps(solver.solve_task)
        def _solve_task_hybrid(task, *a, **kw):
            out = solver._hybrid_wrapped_solve_task(task, *a, **kw)
            try:
                if not isinstance(out, dict):
                    return out
                cands = out.get("candidates")
                if not cands:
                    if "prediction" in out:
                        inp = None
                        try:
                            if isinstance(task, dict) and task.get("test"):
                                inp = task.get("test", [{}])[0].get("input")
                        except Exception:
                            inp = None
                        if inp is not None:
                            m = blender.blended_score(sim, _to_np(inp), _to_np(out["prediction"]))
                            out.setdefault("meta", {})["hybrid"] = m
                            _safe_log("hybrid.pick", {"where": "solve_task.annotate", "p_conf": float(m.get("p_conf", 0.0))})
                            _ultra_observe("solve_task.annotate", {"p_conf": float(m.get("p_conf", 0.0))})
                    return out

                ref = out.get("ref")
                cands = _maybe_glyph_gate(list(cands), ref)
                out["candidates"] = cands
                try:
                    inp = task.get("test", [{}])[0].get("input") if isinstance(task, dict) and task.get("test") else (cands[0] if cands else None)
                except Exception:
                    inp = cands[0] if cands else None
                if inp is not None and cands:
                    best, m = blender.rerank(sim, _to_np(inp), cands, ref=_to_np(ref) if ref is not None else None)
                    out["prediction"] = best
                    out.setdefault("meta", {})["hybrid"] = m
                    _safe_log("hybrid.pick", {"where": "solve_task", "n": int(len(cands)),
                                              "p_conf": float(m.get("p_conf", 0.0)),
                                              "blended": float(m.get("blended", 0.0))})
                    _ultra_observe("solve_task", {"n": int(len(cands)), "p_conf": float(m.get("p_conf", 0.0))})
                return out
            except Exception as e:
                _safe_log("hybrid.pick.error", {"where": "solve_task", "err": str(e)})
                return out
        solver.solve_task = _solve_task_hybrid

    if hasattr(solver, "learn_from_misses") and not hasattr(solver, "_hybrid_wrapped_lfm"):
        solver._hybrid_wrapped_lfm = solver.learn_from_misses

        @_wraps(solver.learn_from_misses)
        def _lfm_hybrid(misses):
            _safe_log("solver.replay", {"where": "learn_from_misses", "n": int(len(misses or []))})
            _ultra_observe("learn_from_misses", {"n": int(len(misses or []))})
            try:
                return solver._hybrid_wrapped_lfm(misses)
            except Exception as e:
                _safe_log("solver.replay.error", {"err": str(e)})
                return solver._hybrid_wrapped_lfm(misses)
        solver.learn_from_misses = _lfm_hybrid

    setattr(solver, "_hybrid_wrapped", True)
    _safe_log("hybrid.attach", {"ok": True})
    return True


# ===========================================
# Curiosity Engine (Unified • Telemetry • Parallel • Provenance)
# ===========================================

class CuriosityEngine:
    def __init__(self,
                 explorer,
                 rulebase,
                 sandbox=None,
                 *,
                 solver=None,
                 max_depth: int = 3,
                 verbose: bool = False,
                 init_budget: int = 50,
                 export_prefix: str = "curiosity",
                 training_mode: bool = False,
                 logger=None,
                 allow_parallel: bool = True):
        # external graph
        self.explorer = explorer
        self.rulebase = rulebase
        self.sandbox  = sandbox
        self.solver   = solver

        # toggles (obey process/global if available)
        self.toggles = getattr(solver, "toggles", globals().get("SOLVER_TOGGLES", None))

        # env
        self.max_depth      = int(max_depth)
        self.verbose        = bool(verbose)
        self.export_prefix  = str(export_prefix)
        self.training_mode  = bool(training_mode)
        self.log            = logger if logger is not None else globals().get("EXPLAIN", None)
        self.allow_parallel = bool(allow_parallel)

        # budget + novelty
        self.curiosity_budget = int(init_budget)
        self.fail_streak      = 0
        self.diversity_bias   = 0.15
        self.len_penalty      = 0.05
        self.min_improve      = 0.0

        self.novelty_log: Set[Tuple] = set()
        self._glyph_pairs_seen: Set[Tuple[str, str]] = set()

        # telemetry caches
        self._discover_events: List[Dict[str, Any]] = []
        self._chain_len_hist, self._depth_hist = Counter(), Counter()
        self._growth_points: List[Tuple[float, int]] = []

        # totals
        self.total_novel = 0

        # physics (reuse global instance/ctor)
        self._phys = None
        try:
            self._phys = globals().get("PhysicsHeuristics", None)
            if callable(self._phys):
                self._phys = self._phys(logger=self.log)
        except Exception:
            self._phys = None

        # hook references (do not redefine)
        self._telemetry     = globals().get("EvaluationTelemetry", None)  # optional unified emitter
        self._commit_xform  = globals().get("commit_xform", None)
        self._RuleRecord    = globals().get("RuleRecord", None)
        self._Rule          = globals().get("Rule", None)
        self._apply_ops     = globals().get("apply_ops", None) or globals().get("sandbox_apply_ops", None)
        self._compute_inv   = globals().get("compute_invariants", None)
        self._score_pair    = globals().get("score_pair", None)
        self._creativity    = globals().get("creativity_features", None)
        self._narrate_chain = globals().get("_narrate_chain", None)

        # rule boundary normalization/validation (globals must provide these)
        self._normalize_rule          = globals().get("normalize_rule", None)
        self._validate_rule_contract  = globals().get("validate_rule_contract", None)

        # io helpers
        self._ensure_dir     = globals().get("_ensure_dir", None) or (lambda d: os.makedirs(d, exist_ok=True))
        self._atomic_write   = globals().get("_atomic_write", None)   # may be None; we fallback inline
        self._prov_header    = globals().get("_provenance_header_dict", None)
        self._holo_snapshot  = globals().get("_holo_snapshot", None)
        self._zip_dir        = globals().get("_zip_dir", None)
        self._compress_keel  = globals().get("_compress_with_keel", None)  # external artifacts only
        self._save_card      = globals().get("save_card_triptych", None)
        self._hash_ops       = getattr(self.rulebase, "hash_ops", None)

        # other globals
        self._kairos = getattr(solver, "kairos", globals().get("kairos", None))
        self._holo   = getattr(solver, "holo",   globals().get("holo", None))

        # meta logger
        self._meta_log = globals().get("meta_log", None)
        self._log_safe = globals().get("_log_safe", None)

        # announce
        self._emit("curiosity.init",
                   max_depth=self.max_depth, init_budget=self.curiosity_budget,
                   training_mode=self.training_mode, allow_parallel=self.allow_parallel)

    # -------------- telemetry --------------
    def _emit(self, topic: str, **payload):
        rec = {"module": "CuriosityEngine", "event": topic, "time": time.time(), **payload}
        # enrich with phase/epoch if available
        try:
            if self._kairos:
                epoch = getattr(self._kairos, "current_epoch", None)
                phase = None
                try:
                    get_state = getattr(self._kairos, "get_state", None)
                    if callable(get_state):
                        st = get_state() or {}
                        phase = st.get("phase", None)
                except Exception:
                    phase = None
                rec["epoch"] = epoch
                rec["phase"] = phase
        except Exception:
            pass
        # unified telemetry if available
        try:
            if self._telemetry:
                self._telemetry.emit(stage="curiosity", **rec)  # type: ignore
        except Exception:
            pass
        # classic fallbacks
        try:
            if self._meta_log:
                self._meta_log(topic, **rec)
        except Exception:
            pass
        try:
            kb = getattr(self.rulebase, "kb", None)
            if kb is not None:
                if hasattr(kb, "push_meta_stats"):
                    kb.push_meta_stats(rec)
                elif hasattr(kb, "narrations"):
                    kb.narrations.append(f"[Curiosity] {topic}: {payload}")
        except Exception:
            pass
        try:
            if self.log is not None and hasattr(self.log, "log"):
                self.log.log(topic, rec)
        except Exception:
            pass

    def _emit_curio(self, kind: str, value: Any, **meta):
        """Normalized curiosity/creativity tag emission (taxonomy bridge)."""
        tag = {"kind": str(kind), "value": value, "ts": time.time(), **meta}
        self._emit("curiosity.tag_emit", **tag)
        try:
            rb_meta = getattr(self.rulebase, "meta", None)
            if rb_meta is not None and hasattr(rb_meta, "record_tag"):
                rb_meta.record_tag("curiosity", tag)
        except Exception:
            pass

    def _err(self, stage: str, fn: Optional[str], e: Exception):
        if self._log_safe:
            try:
                self._log_safe(f"curiosity::{stage}", e, meta={"fn": fn})
            except Exception:
                pass
        if self._meta_log:
            try:
                self._meta_log("curiosity.error", stage=stage, fn=(fn or stage), err=str(e))
            except Exception:
                pass
        try:
            if "logger" in globals() and globals()["logger"]:
                globals()["logger"].exception(f"[Curiosity::{stage}] {fn or ''}: {e}")
        except Exception:
            pass

    # -------------- budget/fail --------------
    def should_explore(self) -> bool:
        return self.curiosity_budget > 0

    def spend_curiosity(self, amt: int = 1, grid=None, context: str = "", success: bool = False):
        self.curiosity_budget = (self.curiosity_budget + 1) if success else max(0, self.curiosity_budget - amt)
        cfeat, csig = {}, {}
        try:
            if grid is not None and self._creativity and hasattr(self.rulebase, "meta") and \
               hasattr(self.rulebase.meta, "evaluate_creativity"):
                cfeat = self._creativity(grid)
                csig = self.rulebase.meta.evaluate_creativity(cfeat, context=context)
        except Exception:
            pass
        self._emit("curiosity.spend",
                   amt=int(amt), remaining=int(self.curiosity_budget),
                   context=str(context), creativity=csig, success=bool(success))
        # normalized tags
        self._emit_curio("budget.remaining", int(self.curiosity_budget), context=context, success=bool(success))
        if csig:
            self._emit_curio("creativity.signature", csig, context=context)

    def reset_fail_streak(self):
        self.fail_streak = 0

    def bump_fail_streak(self) -> bool:
        self.fail_streak += 1
        if self.fail_streak >= 5:
            self._emit("curiosity.fail_streak", streak=self.fail_streak)
            self._emit_curio("fail.streak", int(self.fail_streak))
        return self.fail_streak >= 5

    # -------------- helpers --------------
    def _flag(self, name: str, default=True) -> bool:
        try:
            return bool(getattr(self.toggles, name))
        except Exception:
            return default

    def _make_stub(self, inp, out, task_id=None):
        try:
            if isinstance(inp, np.ndarray) and inp.size > 0:
                shape, src = inp.shape, "input_shape"
            elif self.training_mode and isinstance(out, np.ndarray) and out.size > 0:
                shape, src = out.shape, "gold_shape"
            else:
                shape, src = (1, 1), "fallback"
        except Exception:
            shape, src = (1, 1), "fallback"
        stub_inp, stub_out = np.zeros(shape, int), np.zeros(shape, int)
        self._emit("curiosity.stub", task_id=task_id, source=src, shape=shape, training_mode=self.training_mode)
        self._emit_curio("stub.used", True, source=src)
        return stub_inp, stub_out

    def _hash_chain(self, chain: List[Tuple[str, Dict[str, Any]]]):
        if callable(self._hash_ops):
            try:
                return self._hash_ops(chain)
            except Exception:
                pass
        def norm(v):
            if isinstance(v, dict): return tuple(sorted((k, norm(val)) for k, val in v.items()))
            if isinstance(v, (list, tuple)): return tuple(norm(x) for x in v)
            return v
        return ("chain", tuple((op[0], tuple(sorted((k, norm(v)) for k, v in (op[1] or {}).items()))) for op in chain))

    def _diversity_score(self, chain_hash):
        return 1.0 - float(chain_hash in self.novelty_log)

    def _inv_metrics(self, a: np.ndarray, b: np.ndarray, pred: Optional[np.ndarray] = None) -> Dict[str, float]:
        out = {"epi": 0.0, "binder": 0.0, "entropy_delta": 0.0, "epi_gap": 0.0, "binder_delta": 0.0}
        if not self._compute_inv:
            return out
        try:
            si = self._compute_inv(a)
            sb = self._compute_inv(b)
            out["epi"] = float(getattr(sb, "epi", 0.0))
            out["binder"] = float(getattr(sb, "binder_last", 0.0))
            et_i = getattr(si, "entropy_trace", []) or []
            et_o = getattr(sb, "entropy_trace", []) or []
            if et_i and et_o:
                out["entropy_delta"] = float(et_o[-1] - et_i[-1])
            out["binder_delta"] = float(getattr(sb, "binder_last", 0.0) - getattr(si, "binder_last", 0.0))
            if pred is not None:
                sp = self._compute_inv(pred)
                out["epi_gap"] = float(abs(getattr(sp, "epi", 0.0) - getattr(sb, "epi", 0.0)))
        except Exception:
            pass
        return out

    def _phys_metrics(self, a: np.ndarray, b: np.ndarray) -> Dict[str, Any]:
        if not self._phys:
            return {"stable": False, "entropy_delta": 0.0, "epi_pair": 0.0, "binder_delta": 0.0}
        try:
            if self._score_pair:  # prefer shared score_pair for parity
                ps = self._score_pair(a, b)
            else:
                ps = self._phys.score_pair(a, b)
            return {
                "stable": bool(getattr(ps, "stable", False)),
                "entropy_delta": float(getattr(ps, "entropy_delta", 0.0)),
                "epi_pair": float(getattr(ps, "epi_pair", 0.0)),
                "binder_delta": float(getattr(ps, "binder_delta", 0.0)),
            }
        except Exception:
            return {"stable": False, "entropy_delta": 0.0, "epi_pair": 0.0, "binder_delta": 0.0}

    def _confidence_from_chain(self, chain, inv=None, phys=None):
        L = max(1, len(chain))
        val = 0.95 - self.len_penalty * (L - 1)
        if inv:
            val += (float(inv.get("epi", 0.0)) + float(inv.get("binder", 0.0))) / 4.0
        if phys and phys.get("stable"):
            val += 0.05
        return float(max(0.55, min(0.98, val)))

    def _discover_chain(self, inp: np.ndarray, out: np.ndarray, depth: int, task_id=None):
        chain = None
        try:
            if hasattr(self.explorer, "_dfs"):
                chain = self.explorer._dfs(inp, out, [], depth)
            elif hasattr(self.explorer, "discover_chain"):
                chain = self.explorer.discover_chain(inp, out, max_depth=depth, task_id=task_id)
        except Exception as e:
            self._emit("curiosity.discover_failed", depth=depth, error=str(e))
            chain = None
        return chain

    def _passes_phys_geom(self, prev: np.ndarray, nxt: np.ndarray, out: np.ndarray) -> bool:
        # Prefer user hook if provided; else use invariant/geometry guards
        try:
            filt = globals().get("_geom_phys_filter", None)
            if callable(filt):
                ok = bool(filt(prev, nxt, out))
            else:
                invc = globals().get("invariant_composite", None)
                ef   = globals().get("error_field", None)
                bws  = globals().get("boundary_weighted_error", None)
                ok = True
                if callable(invc):
                    comps = invc(nxt, out)
                    ok = ok and bool(comps.get("stable", True))
                if callable(ef) and callable(bws):
                    try:
                        # boundary-weighted error of mismatches must be modest (loose guard)
                        efield = ef(nxt, out)
                        ok = ok and (float(bws(efield, np.zeros_like(out))) <= 0.5)
                    except Exception:
                        pass
            self._emit("curiosity.geom_firewall", ok=bool(ok))
            return bool(ok)
        except Exception:
            return True

    # -------------- core --------------
    def propose(self, state_grid: np.ndarray, k: int = 5) -> list:       
        proposals = []
        # Holo seeds (duck-type)
        try:
            h = self._holo or globals().get("holo", None)
            get_fn = getattr(h, "get", None)
            if callable(get_fn):
                hits = get_fn(state_grid, topk=min(2, k))
                for pred, meta_hit, dist in (hits or []):
                    proposals.append(("holo_seed", pred, 1.0 - float(dist)))
                if hits:
                    self._emit("curiosity.holo_seed", n=len(hits), shape=tuple(state_grid.shape))
                    self._emit_curio("holo.seed_hits", int(len(hits)), shape=tuple(state_grid.shape))
        except Exception as e:
            self._emit("curiosity.holo_seed_fail", error=str(e))
        # heuristic extension (safe fallback)
        more = []
        try:
            fn = globals().get("curiosity_heuristics", None)
            more = fn(state_grid, k=max(0, k - len(proposals))) if callable(fn) else []
        except Exception as e:
            self._emit("curiosity.heuristics_fail", error=str(e))
        proposals.extend(more)

        # geometry/RHCM analytic seeds (train-pair driven)
        try:
            gf = globals().get("geom_filter_suggest_ops", None)
            if callable(gf):
                # Resolve train pairs from rulebase (robust duck-typing)
                pairs = []
                try:
                    get_recent = getattr(self.rulebase, "get_recent_train_pairs", None)
                    if callable(get_recent):
                        pairs = get_recent(limit=6) or []
                    else:
                        recs = getattr(self.rulebase, "records", None) or []
                        for r in recs:
                            try:
                                gi = getattr(r, "input_grid", None)
                                go = getattr(r, "output_grid", None)
                                if gi is not None and go is not None:
                                    pairs.append((gi, go))
                                    if len(pairs) >= 6:
                                        break
                            except Exception:
                                continue
                except Exception:
                    pairs = []
                # Ensure tuple-of-arrays
                pairs = [(p[0], p[1]) for p in pairs if isinstance(p, (tuple, list)) and len(p) >= 2]
                rec = gf(pairs, top_k=max(0, k - len(proposals))) if pairs else []
                for ops, score in (rec or []):
                    try:
                        comp = globals().get("compose_ops", None)
                        if callable(comp):
                            pred = comp(state_grid, ops)
                            proposals.append(("geo_chain", pred, float(score)))
                    except Exception:
                        continue
                if rec:
                    self._emit("curiosity.geo_seed", n=len(rec))
        except Exception:
            pass

        return proposals[:k]

    def on_success(self, inp: np.ndarray, out: np.ndarray, score: float):
        """Curiosity acknowledges a success; reinforce HoloMemory (duck-typed)."""
        try:
            h = self._holo or globals().get("holo", None)
            add_fn = getattr(h, "add", None)
            if callable(add_fn):
                add_fn(inp, out, {"subject":"curiosity",
                                  "confidence": float(min(1.0, 0.8 + 0.2*np.tanh(score))),
                                  "rule_kind":"curiosity"})  # normalized rule_kind
                self._emit_curio("holo.reinforce", float(score))
        except Exception as e:
            self._emit("curiosity.holo_add_fail", error=str(e))

    def explore(self, inp: np.ndarray, out: np.ndarray, budget=5, task_id=None):
        # inputs
        inp_ok = isinstance(inp, np.ndarray) and inp.size > 0
        out_ok = isinstance(out, np.ndarray) and out.size > 0
        if not (inp_ok and out_ok):
            inp, out = self._make_stub(inp, out, task_id)
        if not self.should_explore():
            self._emit("curiosity.halt", reason="budget_exhausted")
            self._emit_curio("budget.halt", True, reason="exhausted")
            return 0

        # kairos tick
        try:
            if self._kairos and hasattr(self._kairos, "step"):
                self._kairos.step(self.total_novel)
                if self._telemetry:
                    self._telemetry.emit(stage="curiosity", event="kairos_step",
                                         epoch=getattr(self._kairos, "current_epoch", None))
        except Exception:
            pass       

        def cross_scale_project(grid: np.ndarray, target_shape: Tuple[int, int]) -> np.ndarray:
            # Prefer global scale_to_shape if present; else fallback to local NN majority pooling
            try:
                scale_to = globals().get("scale_to_shape", None)
                if callable(scale_to):
                    z = scale_to(grid, target_shape)
                    if z is not None:
                        return z
            except Exception:
                pass
            try:
                th, tw = target_shape
                gh, gw = grid.shape[:2]
                if (gh, gw) == (th, tw):
                    return grid
                outg = np.zeros((th, tw), dtype=grid.dtype)
                for i in range(th):
                    for j in range(tw):
                        y0 = int(round(i * gh / th))
                        y1 = int(round((i + 1) * gh / th))
                        x0 = int(round(j * gw / tw))
                        x1 = int(round((j + 1) * gw / tw))
                        y0, x0 = max(0, y0), max(0, x0)
                        y1, x1 = max(y0+1, y1), max(x0+1, x1)
                        window = grid[y0:y1, x0:x1]
                        vals, counts = np.unique(window, return_counts=True)
                        outg[i, j] = vals[int(np.argmax(counts))] if len(vals) else 0
                return outg
            except Exception:
                return grid

        # glyph ids for telemetry/context
        try:
            g_in  = getattr(self._compute_inv(inp), "glyph_id", "na") if self._compute_inv else "na"
            g_out = getattr(self._compute_inv(out), "glyph_id", "na") if self._compute_inv else "na"
        except Exception:
            g_in, g_out = "na", "na"
        glyph_pair = (g_in, g_out)
        self._emit("curiosity.glyph_pair", glyph_in=g_in, glyph_out=g_out)

        # evaluator compatible with Meta.holo_recall_then_evaluate
        # Includes optional cross-scale projection of candidates before scoring
        def evaluator(e_inp, pred):
            target_shape = getattr(out, "shape", None)
            cand = pred
            comps = {}

            if target_shape and hasattr(pred, "shape") and pred.shape != target_shape:
                cand = cross_scale_project(pred, target_shape)

            # Prefer shared score_pair
            if self._score_pair:
                try:
                    s = float(self._score_pair(cand, out))
                except Exception:
                    s = 0.0
            else:
                inv = self._inv_metrics(e_inp, out, pred=cand)
                phys = self._phys_metrics(e_inp, out)
                # lightweight normalized score blend as fallback
                s = 0.5
                try:
                    s = 0.5 \
                        + 0.25 * (1.0 - abs(float(inv.get("entropy_delta", 0.0)))) \
                        + 0.25 * (1.0 - float(inv.get("epi_gap", 0.0)))
                except Exception:
                    pass
                comps.update(inv)
                comps.update(phys)

            # ensure firewall fields exist
            comps.setdefault("entropy_delta", 0.0)
            comps.setdefault("epi_gap", 0.0)
            comps.setdefault("binder_delta", 0.0)
            comps.setdefault("stable", True)
            return _clamp(s), comps

        # Recall-first via Meta (graceful absence handling)
        meta = getattr(self.rulebase, "meta", None)
        holo = self._holo or globals().get("holo", None)
        admit = None
        tent = None
        try:
            admit = float(getattr(meta, "confident_thresh", None)) if meta else None
            tent = float(getattr(meta, "tentative_thresh", None)) if meta else None
        except Exception:
            admit, tent = None, None

        if meta is not None and holo is not None and hasattr(meta, "holo_recall_then_evaluate"):
            topk = 3
            near_pass_eff = None
            accepted = False
            eff_for_tel = None
            raw_for_tel = None

            try:
                res = meta.holo_recall_then_evaluate(
                    inp, evaluator, topk=topk, admit_thresh=admit,
                    origin="curiosity", subject=task_id, task_id=task_id, train_index=None
                )
                if res is not None:
                    # Accepted by Meta (it also committed with rule_kind="recall_commit")
                    _pred, _m, eff = res
                    accepted = True
                    eff_for_tel = float(eff)
                    try:
                        raw_for_tel, _c = evaluator(inp, _pred)
                    except Exception:
                        raw_for_tel = None
                    self._emit("curiosity.recall_eval",
                               accepted=True, score_eff=eff_for_tel, score_raw=raw_for_tel,
                               firewalled=False, subject=task_id or "generic", task_id=task_id, train_index=None,
                               admit_thresh=admit, topk=topk, glyph_in=g_in, glyph_out=g_out)
                    self._emit_curio("recall.accept", float(eff_for_tel if eff_for_tel is not None else 0.0),
                                     glyph_in=g_in, glyph_out=g_out)
                    self.spend_curiosity(1, grid=inp, context="recall_accept", success=True)
                    return 1  # Accepted via recall (counts as 1 outcome)
                else:
                    try:
                        get_fn = getattr(holo, "get", None)
                        best_eff = None
                        if callable(get_fn):
                            hits = get_fn(inp, topk=1) or []
                            for p, m, _d in hits:
                                r, c = evaluator(inp, p)
                                best_eff = r if best_eff is None else max(best_eff, r)
                        near_pass_eff = best_eff
                    except Exception:
                        near_pass_eff = None
                    self._emit("curiosity.recall_eval",
                               accepted=False, score_eff=near_pass_eff, score_raw=near_pass_eff,
                               firewalled=None, subject=task_id or "generic", task_id=task_id, train_index=None,
                               admit_thresh=admit, topk=topk, glyph_in=g_in, glyph_out=g_out)
                    if near_pass_eff is not None:
                        self._emit_curio("recall.near_miss", float(near_pass_eff),
                                         admit=admit, tent=tent, glyph_in=g_in, glyph_out=g_out)
            except Exception as e:
                self._emit("curiosity.recall_eval_error", error=str(e))
                accepted = False

            # Fewer expensive proposals on near-pass
            if not accepted and near_pass_eff is not None and tent is not None and admit is not None:
                if float(near_pass_eff) >= float(tent) and float(near_pass_eff) < float(admit):
                    omega = None
                    try:
                        get_state = getattr(self._kairos, "get_state", None)
                        if callable(get_state):
                            st = get_state()
                            omega = st.get("omega_state") or st.get("omega") or None
                    except Exception:
                        omega = None
                    self._emit("curiosity.recall_near_miss",
                               eff=near_pass_eff, admit=admit, tent=tent, omega=omega)
                    self._emit_curio("recall.near_miss.flag", True, eff=near_pass_eff, admit=admit, tent=tent, omega=omega)
                    local_max_depth = max(2, min(self.max_depth, 2 if omega in (None, "Ω1", "Ω2") else 3))
                    budget = max(0, int(budget) - 1)
                else:
                    local_max_depth = self.max_depth
            else:
                local_max_depth = self.max_depth
        else:
            self._emit("curiosity.recall_skip",
                       reason="meta_or_holo_missing",
                       has_meta=bool(meta is not None), has_holo=bool(holo is not None))
            local_max_depth = self.max_depth

        # glyph-pair heuristic for novelty penalty
        seen_this_pair_before = glyph_pair in self._glyph_pairs_seen

        # depth candidates (rank best per depth)
        found_new, per_depth_best = 0, {}

        def _try_depth(d: int):
            chain = self._discover_chain(inp, out, d, task_id=task_id)
            if not chain:
                return None
            ch = self._hash_chain(chain)
            if ch in self.novelty_log:
                return None
            rank = self._diversity_score(ch) + (1 - self.len_penalty * (len(chain) - 1))
            if seen_this_pair_before:
                rank -= 0.10
            return (d, rank, chain, ch)

        depths = list(range(2, int(local_max_depth) + 1))
        if self.allow_parallel and len(depths) > 1:
            max_workers = None
            try:
                rec = getattr(self._kairos, "recommend_threads", None)
                max_workers = rec("curiosity", total_tasks=len(depths)) if callable(rec) else None
            except Exception:
                max_workers = None
            max_workers = int(max_workers or min(8, (os.cpu_count() or 4)))
            if len(depths) == 1:
                max_workers = 1
            try:
                from concurrent.futures import ThreadPoolExecutor, as_completed
                with ThreadPoolExecutor(max_workers=max_workers) as ex:
                    futs = {ex.submit(_try_depth, d): d for d in depths}
                    for fut in as_completed(futs):
                        try:
                            res = fut.result()
                        except Exception as e:
                            self._emit("curiosity.depth_worker_error", error=str(e), depth=int(futs[fut]))
                            continue
                        if not res:
                            continue
                        d, rank, chain, ch = res
                        if d not in per_depth_best or rank > per_depth_best[d][0]:
                            per_depth_best[d] = (rank, chain, ch)
            except Exception as e:
                # fallback serial if executors unavailable
                self._emit("curiosity.parallel_unavailable", error=str(e))
                for d in depths:
                    res = _try_depth(d)
                    if not res:
                        continue
                    d, rank, chain, ch = res
                    if d not in per_depth_best or rank > per_depth_best[d][0]:
                        per_depth_best[d] = (rank, chain, ch)
        else:
            for d in depths:
                res = _try_depth(d)
                if not res:
                    continue
                d, rank, chain, ch = res
                if d not in per_depth_best or rank > per_depth_best[d][0]:
                    per_depth_best[d] = (rank, chain, ch)

        # sandbox rescue (single-shot) if nothing found
        if not per_depth_best and self.sandbox and hasattr(self.sandbox, "discover_chain"):
            try:
                chain = self.sandbox.discover_chain(inp, out, task_id=task_id)
                if chain:
                    ch = self._hash_chain(chain)
                    if ch not in self.novelty_log:
                        self.novelty_log.add(ch)
                    self._emit("curiosity.sandbox_rescue", task_id=task_id, chain_len=len(chain))
                    self._emit_curio("rescue.triggered", True, task_id=task_id)
                    per_depth_best[local_max_depth] = (1.0, chain, ch)
            except Exception as e:
                self._emit("curiosity.sandbox_failed", error=str(e))

        # commit best per depth
        for d, (_rank, chain, ch) in per_depth_best.items():
            if found_new >= budget:
                break
            self.novelty_log.add(ch)

            # predict + metrics
            inv_det = {}; phys_det = {}; pred = None
            try:
                if callable(self._apply_ops):
                    pred = self._apply_ops(inp, chain)
                inv_det = self._inv_metrics(inp, out, pred=pred)
                phys_det = self._phys_metrics(inp, out)
            except Exception as e:
                self._emit("curiosity.metric_error", error=str(e))

            conf = self._confidence_from_chain(chain, inv_det, phys_det)

            # creativity delta (attach for explainability if available)
            creative_delta = {}
            if self._creativity:
                try:
                    creative_delta = self._creativity(pred if isinstance(pred, np.ndarray) else inp)
                except Exception:
                    creative_delta = {}

            # provenance
            prov = {}
            if self._prov_header:
                try:
                    prov = self._prov_header({"kind": "curiosity", "task_id": task_id or "na"})
                except Exception:
                    prov = {"run_id": uuid.uuid4().hex[:8], "ts": time.time()}

            meta = {
                "discovery": "curiosity",
                "depth": d,
                "chain_len": len(chain),
                "timestamp": time.time(),
                "task_id": task_id,
                "glyph_in": g_in,
                "glyph_out": g_out,
                "confidence": float(conf),
                "provenance": prov,
                **{f"inv_{k}": v for k, v in inv_det.items()},
                **{f"phys_{k}": v for k, v in phys_det.items()},
                "creative_delta": creative_delta
            }

            # rule boundary: normalize + validate contract (no-throw, telemetry)
            rule_like = ("xform", {"ops": chain})
            try:
                if callable(self._normalize_rule):
                    rule_like = self._normalize_rule(rule_like)
                if callable(self._validate_rule_contract):
                    self._validate_rule_contract(rule_like)
                self._emit("curiosity.rule_validated", ok=True, kind=rule_like[0], chain_len=len(chain))
            except Exception as e:
                self._emit("curiosity.rule_validated", ok=False, error=str(e), chain_len=len(chain))

            # commit via global commit_xform → kb/rulebase
            committed = False
            try:
                if callable(self._commit_xform):
                    self._commit_xform(inp, out, chain, self.solver or self.rulebase,
                                       meta_extra=meta, confidence=conf, source="curiosity")
                    committed = True
                    self._emit("curiosity.commit_success", depth=d, chain_len=len(chain), confidence=float(conf))
            except Exception as e:
                self._emit("curiosity.commit_xform_failed", error=str(e))

            if not committed:
                try:
                    if self._RuleRecord and self._Rule:
                        rec = self._RuleRecord(inp, out, self._Rule("xform", {"ops": chain}), meta)
                        if hasattr(self.rulebase, "add"):
                            self.rulebase.add(rec)
                        kb = getattr(self.rulebase, "kb", None)
                        if kb is not None and hasattr(kb, "remember_xform"):
                            kb.remember_xform(inp, out, chain, confidence=conf)
                        self._emit("curiosity.commit_fallback", depth=d, chain_len=len(chain), confidence=float(conf))
                except Exception as e:
                    self._emit("curiosity.rulebase_error", error=str(e))

            # counters
            found_new += 1
            self.total_novel += 1
            self._depth_hist[d] += 1
            self._chain_len_hist[len(chain)] += 1
            self._glyph_pairs_seen.add(glyph_pair)

            # narration
            try:
                narr = self._narrate_chain(chain, []) if self._narrate_chain else f"ops={[(op[0], op[1]) for op in chain]}"
            except Exception:
                narr = f"ops={[(op[0], op[1]) for op in chain]}"

            # event (+ tag snapshot)
            ev = {
                "task_id": task_id,
                "depth": d,
                "chain_len": len(chain),
                "hash": str(ch),
                "confidence": conf,
                "ts": meta["timestamp"],
                "narration": narr,
                "glyph_in": g_in,
                "glyph_out": g_out,
                **{f"inv_{k}": v for k, v in inv_det.items()},
                **{f"phys_{k}": v for k, v in phys_det.items()},
            }
            self._discover_events.append(ev)
            self._growth_points.append((meta["timestamp"], self.total_novel))
            self._emit("curiosity.discovery", depth=d, conf=conf, chain_len=len(chain),
                       narration=narr, glyph_in=g_in, glyph_out=g_out)

            # normalized tags for analytics
            self._emit_curio("discovery.depth", int(d))
            self._emit_curio("discovery.chain_len", int(len(chain)))
            self._emit_curio("discovery.confidence", float(conf))
            for k, v in inv_det.items():
                self._emit_curio(f"inv.{k}", float(v) if isinstance(v, (int, float)) else v)
            for k, v in phys_det.items():
                self._emit_curio(f"phys.{k}", float(v) if isinstance(v, (int, float)) else v)

            # budget reinforcement
            self.spend_curiosity(1, grid=inp, context="explore", success=True)

        if self.verbose:
            try:
                print(f"[Curiosity] New rules: {found_new} | Total: {self.total_novel}")
            except Exception:
                pass

        self._export_all()
        return found_new

    # -------------- exports --------------
    def _atomic_dump_csv_dicts(self, path: str, rows: List[Dict[str, Any]]):
        # atomic write
        try:
            d = os.path.dirname(path)
            if d: os.makedirs(d, exist_ok=True)
            fn_tmp = path + ".tmp"
            with open(fn_tmp, "w", newline="", encoding="utf-8") as f:
                if rows:
                    fieldnames = sorted({k for r in rows for k in r})
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    f.write("# schema:curiosity_stats.v1\n")
                    writer.writeheader()
                    for r in rows:
                        writer.writerow(r)
                else:
                    w = csv.writer(f); w.writerow(["empty"])
            os.replace(fn_tmp, path)
        except Exception as e:
            self._err("csv_write", path, e)

    def _export_stats(self, prefix):
        try:
            if self.toggles and not self._flag("CSV_EXPORTS"):
                return
            rows = self._discover_events
            self._atomic_dump_csv_dicts(f"{prefix}_stats.csv", rows)

            if self.toggles and not self._flag("VIS_EXPORTS"):
                self._emit("curiosity.export_stats", n=len(rows))
                return

            # simple histograms (headless-safe)
            if plt is not None:
                if self._depth_hist:
                    xs, ys = zip(*sorted(self._depth_hist.items()))
                    plt.figure(figsize=(6,4))
                    plt.bar(xs, ys)
                    plt.title("Curiosity Discoveries by Depth")
                    plt.tight_layout()
                    plt.savefig(f"{prefix}_depth.png")
                    plt.close()
                if self._chain_len_hist:
                    xs, ys = zip(*sorted(self._chain_len_hist.items()))
                    plt.figure(figsize=(6,4))
                    plt.bar(xs, ys)
                    plt.title("Curiosity Discoveries by Chain Length")
                    plt.tight_layout()
                    plt.savefig(f"{prefix}_chain_len.png")
                    plt.close()
            self._emit("curiosity.export_stats", n=len(rows))
        except Exception as e:
            self._emit("curiosity.export_failed", error=str(e))

    def _export_cards(self, prefix, max_cards=6):
        # obey toggles
        if self.toggles and not self._flag("VIS_EXPORTS"):
            return
        try:
            cards_dir = os.path.join(os.path.dirname(prefix), "cards")
            self._ensure_dir(cards_dir)
            if not self._save_card or not callable(self._apply_ops):
                self._emit("curiosity.export_cards_skipped", reason="save_card/apply_ops_missing")
                return

            recs = getattr(self.rulebase, "records", None)
            if not isinstance(recs, list):
                self._emit("curiosity.export_cards_skipped", reason="records_missing")
                return

            count = 0
            for rec in recs:
                if count >= max_cards:
                    break
                if not hasattr(rec, "rule") or getattr(rec.rule, "kind", "") != "xform":
                    continue
                ops = (rec.rule.payload or {}).get("ops", [])
                inp, out = rec.input_grid, rec.output_grid
                if inp is None or out is None:
                    continue
                pred = self._apply_ops(inp, ops)
                self._save_card(inp, pred, out, os.path.join(cards_dir, f"card_{count}.png"),
                                title=f"ops len={len(ops)}")
                count += 1
            self._emit("curiosity.export_cards", n=count)
        except Exception as e:
            self._emit("curiosity.card_export_failed", error=str(e))

    def _export_all(self):
        if self.toggles and (not self._flag("CSV_EXPORTS") and not self._flag("VIS_EXPORTS")):
            return
        exp_dir = os.path.join("exports", "curiosity")
        self._ensure_dir(exp_dir)
        prefix = os.path.join(exp_dir, self.export_prefix)

        # pre snapshot
        try:
            if self._holo and hasattr(self._holo, "snapshot"):
                self._holo.snapshot(f"csv_pre_{os.path.basename(prefix)}")
        except Exception:
            pass

        self._export_stats(prefix)
        self._export_cards(prefix)

        # compression policy (optional KEEL for external artifacts ≥ 1MB)
        try:
            csv_fn = f"{prefix}_stats.csv"
            if os.path.isfile(csv_fn):
                sz = os.path.getsize(csv_fn)
                if sz >= 1_000_000 and self._compress_keel:
                    self._compress_keel(csv_fn)
                    self._emit("curiosity.keel_pack", file=csv_fn, size=int(sz))
        except Exception as e:
            self._err("compress", prefix, e)

        # post snapshot
        try:
            if self._holo and hasattr(self._holo, "snapshot"):
                self._holo.snapshot(f"csv_post_{os.path.basename(prefix)}")
        except Exception:
            pass

# ===========================================
# Meta-Layer 
# ===========================================


def _require_meta_globals():
    missing = []
    for name in ("meta_log", "EXPLAIN", "KairosPulseManager"):
        if name not in globals():
            missing.append(name)
    if missing:
        raise RuntimeError(f"[MetaLayer] Missing required globals: {missing}")    
    if "_META_HAS_PHYS" in globals() and globals().get("_META_HAS_PHYS"):
        if "compute_invariants" not in globals():
            raise RuntimeError("[MetaLayer] _META_HAS_PHYS=True requires global compute_invariants(grid)")


# ===========================================
# Meta-Layer (strict)
# ===========================================
class MetaLayer:
    TELEMETRY_FULL: bool = True

    def __init__(
        self,
        sim: 'HybridSimilarity',
        kb: Optional['SymbolicKB'] = None,
        *,
        enable_kairos: bool = True,
        kairos_config: Optional[Dict[str, Any]] = None,
        ultra: Optional[Any] = None,
        encoder: Optional[Any] = None,
    ):
        _require_meta_globals()

        # Core wiring
        self.sim = sim
        self.kb  = kb or SymbolicKB()  # type: ignore

        # Optional external systems (explicitly set; attachers can override)
        self.ultra: Optional[Any] = ultra
        self.encoder: Optional[Any] = encoder
        self.rulebase: Optional[Any] = None
        self.rulegen: Optional[Any] = None
        self.blender: Optional[Any] = None

        # Kairos (deterministic stepping)
        self.kairos = None
        if enable_kairos:
            cfg = dict(kairos_config or {})
            n = int(cfg.pop("n", 30))
            amp = float(cfg.pop("pulse_amplitude", 0.3))
            self.kairos = KairosPulseManager(n=n, pulse_amplitude=amp, **cfg)  # type: ignore

        # Holo/Sandbox/Curiosity wiring is attached via dedicated methods
        self.sandbox: Optional['SandboxExplorer'] = None
        self.holo: Optional['HoloMemory'] = None
        self.curiosity: Optional['CuriosityEngine'] = None

        # Ring buffers / logs
        self.history: deque = deque(maxlen=512)
        self.telemetry: List[Dict[str, Any]] = []
        self._telemetry_cap = 20_000

        # SSOT dials
        _dials = get_meta_dials()
        self._tel_downsample_n = int(_dials["tel_downsample_n"])
        self._emit_strict = int(_dials["emit_strict"])
        self._fw_dh = float(_dials["firewall_dh"])
        self._fw_epi = float(_dials["firewall_epi"])
        self._fw_bind = float(_dials["firewall_bind"])
        self._tel_emit_ctr = 0

        # Modes / thresholds
        self.mode = "NORMAL"
        self.ma_success = 0.5
        self.beta = 0.1
        self.confident_thresh = 0.85
        self.tentative_thresh = 0.50
        self._thresh_floor_c  = 0.70
        self._thresh_floor_t  = 0.35
        self._thresh_ceiling  = 0.95

        # Internal state
        self._mode_log: List[Tuple[float, str]] = []
        self._archetype_counts: List[Tuple[float, int]] = []
        self._archetype_entropy: List[Tuple[float, float]] = []
        self._symbolic_timeline: Dict[str, List[Tuple[float, int]]] = defaultdict(list)
        self._rolling_window = 100
        self._ev_nodes: List[Dict[str, Any]] = []
        self._ev_edges: List[Tuple[int, int, str]] = []
        self._ev_last_id: Optional[int] = None
        self._shape_stats: Dict[str, Dict[str, Any]] = defaultdict(lambda: {
            "counts": defaultdict(int),
            "success": defaultdict(int),
            "fail": defaultdict(int),
            "last_ts": 0.0,
        })
        self._record_counter = 0
        self._observe_counter = 0
        self._pulse_ct = 0
        self._last_pulse_ts = time.time()

        # Provenance bonuses (assumes helpers exist in monolith)
        try:
            self._prov_bonus = _load_prov_bonus()  # type: ignore
        except Exception:
            self._prov_bonus = {}

        # Symbolic ML controller is instantiated lazily after attachments
        self.ml = None  # delayed init, see _ensure_ml()

        # Phase
        self.phase = "init"
        self._emit("meta.init", kairos=bool(self.kairos is not None), ultra=bool(self.ultra is not None))

        # Curiosity budget knobs (kept from original)
        self.curiosity_budget   = 0
        self._curiosity_spent   = 0
        self._curiosity_max     = 64
        self._curiosity_step_up = 3
        self._curiosity_step_dn = 1

    # Internal utilities
    # -------------------------------------------
    def _ensure_ml(self):
        """Instantiate SymbolicMLController once we have at least kb; patch peers on attach."""
        if self.ml is None:
            try:
                self.ml = SymbolicMLController(meta=self, holo=self.holo, sandbox=self.sandbox, kb=self.kb)  # type: ignore
            except Exception:
                self.ml = None
        else:
            try:
                self.ml.meta = self  # type: ignore
                self.ml.holo = self.holo  # type: ignore
                self.ml.sandbox = self.sandbox  # type: ignore
                self.ml.kb = self.kb  # type: ignore
            except Exception:
                pass

    def _canon_rule_kind(self, rk: Optional[str]) -> str:
        rk = (rk or "").strip().lower()
        if rk in ("sandbox", "rescue_shape", "recall_commit", "inference"):
            return rk
        return "inference"

    # Internal emit (fail-fast by env)
    def _emit(self, topic: str, **payload):
        try:
            meta_log(topic, **payload)  # type: ignore
            EXPLAIN.log(topic, payload)  # type: ignore
            if self.holo is not None and hasattr(self.holo, "_telemetry"):
                self.holo._telemetry(topic, **payload)  # type: ignore
            if self.ultra is not None and hasattr(self.ultra, "observe"):
                # strip heavy payloads
                self.ultra.observe(topic, **{k: v for k, v in payload.items() if k not in ("grid", "inp", "out")})  # type: ignore
        except Exception:
            if self._emit_strict:
                raise
    
    # Attachments (runtime)
    # -------------------------------------------
    def attach_holomemory(self, holo: 'HoloMemory'):
        self.holo = holo
        self._ensure_ml()
        self._emit("meta.attach.holo", ok=True)

    def attach_curiosity(self, cur: 'CuriosityEngine'):
        self.curiosity = cur
        self._emit("meta.attach.curiosity", ok=True)

    def attach_sandbox(self, sandbox: 'SandboxExplorer'):
        self.sandbox = sandbox
        self._ensure_ml()
        self._emit("meta.attach.sandbox", ok=True)

    def attach_kb(self, kb: 'SymbolicKB'):
        self.kb = kb
        self._ensure_ml()
        self._emit("meta.attach.kb", ok=True)

    def attach_similarity(self, sim: 'HybridSimilarity'):
        self.sim = sim
        self._emit("meta.attach.similarity", ok=True)
        return self

    # Original surface attachments preserved
    def attach_rulebase(self, rulebase):
        self.rulebase = rulebase
        self._emit("meta.attach_rulebase", ok=True)
        return self

    def attach_rule_generator(self, rulegen):
        self.rulegen = rulegen
        self._emit("meta.attach_rulegen", ok=True)
        return self

    def attach_symbolic_ml(self, encoder):
        self.encoder = encoder
        self._emit("meta.attach_encoder", ok=True)
        return self

    def attach_blender(self, blender):
        self.blender = blender
        self._emit("meta.attach_blender", ok=True)
        return self

    def attach_ultra(self, ultra: Any):
        self.ultra = ultra
        self._emit("meta.attach.ultra", ok=True)
        return self

    def attach_encoder(self, encoder: Any):
        self.encoder = encoder
        self._emit("meta.attach.encoder", ok=True)
        return self

    def set_phase(self, phase: str):
        self.phase = str(phase)
        if self.ultra and hasattr(self.ultra, "set_phase"):
            try:
                self.ultra.set_phase(self.phase)
            except Exception:
                pass
        self._emit("meta.phase", phase=self.phase)

    # -------------------------------------------
    # Telemetry core
    # -------------------------------------------
    def _log_telemetry(self, rec: dict):
        self._tel_emit_ctr += 1
        if (self._tel_emit_ctr % self._tel_downsample_n) != 0:
            return
        if len(self.telemetry) >= self._telemetry_cap:
            self.telemetry = self.telemetry[-(self._telemetry_cap // 2):]
        self.telemetry.append(rec)
        self._emit("meta.telemetry", **rec)

    def _append_event_node(self, ev_min: Dict[str, Any]) -> int:
        node_id = len(self._ev_nodes)
        ev = dict(ev_min)
        ev["entropy"] = self._archetype_entropy[-1][1] if self._archetype_entropy else None
        ev["conf_thresh"] = self.confident_thresh
        self._ev_nodes.append(ev)
        if self._ev_last_id is not None:
            self._ev_edges.append((self._ev_last_id, node_id, f"{ev.get('module','')}.{ev.get('event','')}"))
        self._ev_last_id = node_id
        return node_id

    # -------------------------------------------
    # Pulse bus (Kairos × Ultra × Holo × Sandbox × KB)
    # -------------------------------------------
    def _pulse(self, kind: str, **payload):
        now = time.time()
        self._pulse_ct += 1
        payload = dict(payload or {})
        payload.setdefault("phase", self.phase)
        payload.setdefault("ts", now)
        self._emit(f"pulse.{kind}", **payload)

        if self.kairos is not None:
            try:
                self.kairos.step(time_step=int(self._pulse_ct))
                if self.TELEMETRY_FULL and hasattr(self.kairos, "get_state"):
                    self._emit("kairos.phase", **self.kairos.get_state())
            except Exception as e:
                # Fail-fast per strict policy (clarify where/when)
                raise RuntimeError(f"[MetaLayer._pulse] Kairos step failed for kind='{kind}' phase='{self.phase}': {e}")

        if self.ultra and hasattr(self.ultra, "observe"):
            try:
                self.ultra.observe(kind, **{k: payload[k] for k in payload if k not in ("grid","inp","out")})
            except Exception:
                pass

        # Holo echo marker already happens via _emit
        self._last_pulse_ts = now

    # -------------------------------------------
    # Similarity feedback (bidirectional)
    # -------------------------------------------
    def update_similarity_feedback(self, sim_score: float, dH: float = 0.0, epi: float = 0.0, binder: float = 0.0):
        rec = {"sim_score": float(sim_score), "dH": float(dH), "epi": float(epi), "binder": float(binder)}
        self._log_telemetry({"event": "hybrid.rescore", **rec})
        self._pulse("similarity_feedback", **rec)

    # -------------------------------------------
    # Rule lifecycle hooks (preserved)
    # -------------------------------------------
    def on_rule_learned(self, rule_desc: dict):
        self._log_telemetry({"event": "rule.learn", **(rule_desc or {})})
        self._pulse("rule_learned", **(rule_desc or {}))
        if self.rulebase and hasattr(self.rulebase, "register_rule"):
            try:
                self.rulebase.register_rule(rule_desc)
            except Exception:
                pass

    def on_rule_applied(self, rule_desc: dict, success: bool, sim_score: float = 0.0):
        rec = {"event": "rule.apply", "success": bool(success), "sim_score": float(sim_score), **(rule_desc or {})}
        self._log_telemetry(rec)
        self._pulse("rule_applied", **rec)
        if self.encoder and hasattr(self.encoder, "record_feedback") and rule_desc:
            try:
                lab = str(rule_desc.get("label","generic"))
                self.encoder.record_feedback(lab, memory_layer="rules", success=bool(success))
            except Exception:
                pass

    # -------------------------------------------
    # Curiosity feedback relay (preserved)
    # -------------------------------------------
    def on_curiosity_reward(self, value: float, depth: int = 0, context: Optional[dict] = None):
        ctx = dict(context or {})
        ctx.update({"value": float(value), "depth": int(depth)})
        self._log_telemetry({"event": "curiosity.reward", **ctx})
        self._pulse("curiosity_reward", **ctx)

    # -------------------------------------------
    # Sandbox outcomes (preserved)
    # -------------------------------------------
    def on_sandbox_outcome(self, success: bool, task_id: Optional[str] = None, meta: Optional[dict] = None):
        rec = {"success": bool(success), "task_id": task_id or "na"}
        rec.update(meta or {})
        self._log_telemetry({"event": "sandbox.outcome", **rec})
        self._pulse("sandbox_outcome", **rec)

    # -------------------------------------------
    # Holo bridge (entropy/compression → kairos) (preserved)
    # -------------------------------------------
    def holo_feedback(self, rin: float = None, rout: float = None, ratio: float = None):
        if rin is not None or rout is not None or ratio is not None:
            payload = {}
            if rin  is not None:  payload["rin"]  = float(rin)
            if rout is not None:  payload["rout"] = float(rout)
            if ratio is not None: payload["ratio"] = float(ratio)
            self._pulse("holo_feedback", **payload)

    # -------------------------------------------
    # Task lifecycle (preserved)
    # -------------------------------------------
    def task_begin(self, task_id: str, n_train: int, n_test: int):
        self._log_telemetry({"event": "task.begin", "task_id": task_id, "train": int(n_train), "test": int(n_test)})
        self.set_phase("task")
        self._pulse("task_begin", task_id=task_id, train=int(n_train), test=int(n_test))

    def task_end(self, task_id: str, acc: float):
        self._log_telemetry({"event": "task.end", "task_id": task_id, "acc": float(acc)})
        self._pulse("task_end", task_id=task_id, acc=float(acc))

    # -------------------------------------------
    # Provenance & firewall
    # -------------------------------------------
    def _provenance_bonus(self, source: str) -> float:
        try:
            return float(self._prov_bonus.get(str(source) or "none", 0.0))
        except Exception:
            return 0.0

    def _entropy_firewall(self, comps: Dict[str, Any]) -> bool:
        try:
            dH      = float(comps.get("entropy_delta", 0.0))
            epi_gap = float(comps.get("epi_gap", 0.0))
            bnd     = float(comps.get("binder_delta", 0.0))
            stable  = bool(comps.get("stable", True))
            # use SSOT dials
            return (abs(dH) > self._fw_dh) or (abs(bnd) > self._fw_bind) or (epi_gap > self._fw_epi) or (not stable)
        except Exception:
            return False

    # -------------------------------------------
    # Prediction hooks (preserved + hardened)
    # -------------------------------------------
    def predict_shape(self, inp: np.ndarray, topk: int = 1) -> List[Tuple[Tuple[int,int], float]]:
        import ast  # import hygiene for literal_eval
        candidates: Dict[str, float] = defaultdict(float)
        shape_key = str(tuple(getattr(inp, "shape", ())))

        # Holo votes
        if self.holo is not None and hasattr(self.holo, "get"):
            try:
                hits = self.holo.get(inp, topk=max(3, topk))
                for (pred, meta, _dist) in hits:
                    if pred is None or not hasattr(pred, "shape"):
                        continue
                    s = str(tuple(pred.shape))
                    w = float(meta.get("confidence", 0.5))
                    candidates[s] += w
            except Exception:
                pass

        # Local stats
        stats = self._shape_stats.get(shape_key)
        if stats and stats["counts"]:
            total = sum(stats["counts"].values())
            for s, c in stats["counts"].items():
                ok = stats["success"].get(s, 0)
                tot = c
                acc = (ok / tot) if tot > 0 else 0.0
                candidates[s] += (c / max(1, total)) * (0.6 + 0.4 * acc)

        if not candidates:
            return []
        items = list(candidates.items())
        z = sum(v for _, v in items)
        scored: List[Tuple[Tuple[int,int], float]] = []
        for s, v in items:
            try:
                shp = ast.literal_eval(s)
                scored.append((tuple(shp), v / max(1e-9, z)))
            except Exception:
                continue
        scored.sort(key=lambda t: t[1], reverse=True)
        return scored[:max(1, topk)]

    def predict_success(self, inp: np.ndarray, rule: Optional['Rule'] = None) -> float:
        base = float(self.ma_success)
        rk = getattr(rule, "kind", None) if rule else None
        rk_acc = None
        if rk is not None:
            ok, fail = 0, 0
            for ev in reversed(self.telemetry[-1000:]):
                if ev.get("event") == "observe" and ev.get("rule_kind") == rk:
                    if ev.get("success"): ok += 1
                    else: fail += 1
            tot = ok + fail
            rk_acc = (ok / tot) if tot > 3 else None

        shape_key = str(tuple(getattr(inp, "shape", ())))
        sh_acc = None
        stats = self._shape_stats.get(shape_key)
        if stats and stats["counts"]:
            ok_s = sum(stats["success"].values())
            tot_s = sum(stats["counts"].values())
            sh_acc = (ok_s / tot_s) if tot_s > 0 else None

        probs = [p for p in (base, rk_acc, sh_acc) if p is not None]
        if not probs:
            return max(0.05, min(0.95, base))
        w = [0.5, 0.3, 0.2][:len(probs)]
        w = np.array(w); w /= w.sum()
        return float(np.dot(w, np.array(probs)))

    # -------------------------------------------
    # Holo recall → evaluate → add-on-success (new explicit API; repaired)
    # -------------------------------------------
    def holo_recall_then_evaluate(
        self,
        inp: np.ndarray,
        evaluator,                  # callable(inp: np.ndarray, pred: np.ndarray) -> Tuple[float, Dict[str, Any]]
        *,
        topk: int = 3,
        admit_thresh: Optional[float] = None,
        origin: str = "meta",
        subject: Optional[str] = None,
        task_id: Optional[str] = None,
        train_index: Optional[int] = None,
    ) -> Optional[Tuple[np.ndarray, Dict[str, Any], float]]:        
        if self.holo is None:
            return None

        hits = None
        try:
            hits = self.holo.get(inp, topk=max(1, int(topk)))
        except Exception:
            hits = None
        if not hits:
            return None

        best_tuple = None  # (pred, meta, score_eff, score_raw, comps)
        thr = float(self.confident_thresh if admit_thresh is None else admit_thresh)
        eps = 1e-3

        for pred, meta, _dist in hits:
            try:
                score_raw, comps = evaluator(inp, pred)
                score_raw = float(score_raw)
            except Exception:
                continue

            # Firewall + provenance blending (guard and clamp)
            firewalled = bool(self._entropy_firewall(comps or {}))
            source_rk = self._canon_rule_kind((meta or {}).get("rule_kind", "inference") if isinstance(meta, dict) else "inference")
            prov_bonus = max(0.0, self._provenance_bonus(source_rk))
            prov_bonus = min(0.15, prov_bonus)  # clamp provenance contribution
            score_eff = max(0.0, min(1.0, score_raw + prov_bonus))

            # If firewall trips, cap to tentative_thresh - ε
            if firewalled:
                score_eff = min(score_eff, max(0.0, self.tentative_thresh - eps))

            # Telemetry (downsampled) + Ultra mirroring via _emit
            tel = {
                "module": "meta",
                "event": "holo_recall_eval",
                "score_raw": float(score_raw),
                "score_eff": float(score_eff),
                "firewalled": bool(firewalled),
                "subject": subject or (task_id or "generic"),
                "task_id": task_id,
                "train_index": train_index,
                "rule_kind": source_rk,
            }
            self._log_telemetry(tel)
            self._emit("holo_recall_eval", **tel)

            # Track best by effective score
            if (best_tuple is None) or (score_eff > best_tuple[2]):
                best_tuple = (pred, meta or {}, float(score_eff), float(score_raw), comps or {})

        if best_tuple is None:
            return None

        pred, meta, score_eff, score_raw, comps = best_tuple

        # Accept-check: only return on threshold pass
        if score_eff >= thr:
            try:
                self.holo.add(inp, pred, {
                    "subject": subject or (task_id or "generic"),
                    "depth": 1,
                    "rule_kind": "recall_commit",
                    "confidence": float(score_eff),
                    "task_id": task_id,
                    "train_index": train_index,
                    "origin": origin,
                    # Confidence Field Decomposition for auditability
                    "score_raw": float(score_raw),
                    "prov_bonus": float(min(0.15, max(0.0, self._provenance_bonus((meta or {}).get('rule_kind','inference'))))),
                    "firewall_cap": float(max(0.0, self.tentative_thresh - 1e-3) if self._entropy_firewall(comps or {}) else 1.0),
                })
            except Exception:
                pass
            return (pred, meta, float(score_eff))
        return None

    # -------------------------------------------
    # Observe (recall → evaluate → learn)
    # -------------------------------------------
    def _holo_learn_if_ok(self, inp: np.ndarray, out: np.ndarray, score_eff: float, source: str, task_id: Optional[str], train_index: Optional[int]):
        if self.holo is None:
            return
        rk = self._canon_rule_kind(source or "inference")
        if score_eff >= self.confident_thresh:
            try:
                self.holo.add(inp, out, {
                    "subject": task_id or "generic",
                    "depth": 1,
                    "rule_kind": rk,
                    "confidence": float(score_eff),
                    "task_id": task_id,
                    "train_index": train_index,
                    "origin": "meta",
                })
                self._emit("meta.holo_learn", ok=True, score_eff=float(score_eff))
            except Exception:
                pass
        elif score_eff >= self.tentative_thresh:
            # Tentative shadow write gated by firewall elsewhere (score_eff already includes firewall capping)
            try:
                self.holo.add(inp, out, {
                    "subject": task_id or "generic",
                    "depth": 1,
                    "rule_kind": rk,
                    "confidence": float(score_eff),
                    "task_id": task_id,
                    "train_index": train_index,
                    "origin": "meta_shadow",
                })
                self._emit("meta.holo_shadow", ok=True, score_eff=float(score_eff))
            except Exception:
                pass

    def observe(self, comps: Dict[str, float], success: bool,
                inp: np.ndarray = None, out: np.ndarray = None, rule: Any = None,
                ops: List[Tuple[str, Dict[str, Any]]] = None,
                task_id: str = None, train_index: int = None):
        sc = float(comps.get("score", 0.0))
        x  = 1.0 if success else 0.0
        inp_shape = tuple(getattr(inp, "shape", ()))
        out_shape = tuple(getattr(out, "shape", ()))

        # Similarity learning
        if hasattr(self.sim, "nudge_after_outcome"):
            self.sim.nudge_after_outcome(comps, success)  # type: ignore
        self.ma_success = (1 - self.beta) * self.ma_success + self.beta * x

        # Mode adapt
        if len(self.history) >= 20:
            recent = [1.0 if h.get("success", False) else 0.0 for h in list(self.history)[-20:]]
            rate = float(np.mean(recent)) if recent else 0.0
            new_mode = "ADAPT" if rate < 0.25 else "NORMAL"
            if new_mode != self.mode:
                self._log_telemetry({"module":"meta","event":"mode_switch","from":self.mode,"to":new_mode})
            self.mode = new_mode
        self._mode_log.append((time.time(), self.mode))

        # Record
        self._record_counter += 1
        self._observe_counter += 1
        source = getattr(rule, "kind", None) if rule else (comps.get("source") or "inference")
        source = self._canon_rule_kind(source)
        record = {
            "record_idx": self._record_counter,
            "task_id": task_id,
            "train_index": train_index,
            "success": bool(success),
            "score": sc,
            "rule": source,
            "ops": ops,
            "timestamp": time.time(),
            "inp_shape": inp_shape,
            "out_shape": out_shape,
            "event": "observe",
            "rule_kind": source,
        }
        self.history.append(record)
        self._log_telemetry(record)

        # Firewall + provenance blend
        bonus = self._provenance_bonus(source)
        bonus = min(0.15, max(0.0, bonus))  # provenance guard
        sc_eff = float(max(0.0, min(1.0, sc + bonus)))
        firewalled = self._entropy_firewall(comps)
        if firewalled:
            sc_eff = min(sc_eff, max(0.0, self.tentative_thresh - 1e-3))
        self._log_telemetry({
            "module":"meta","event":"prov_blend","source":source,
            "bonus":float(bonus),"score_raw":float(sc),"score_eff":float(sc_eff),
            "firewalled":bool(firewalled)
        })

        # KB memory of xforms
        if inp is not None and (ops or (rule and getattr(rule, "kind", None) == "xform")):
            chain = ops if ops is not None else (rule.payload.get("ops", []) if rule else [])
            try:
                if success and sc_eff >= self.confident_thresh:
                    self.kb.remember_xform(inp, out, chain, confidence=1.0, meta={"prov_source": source, "score_raw": sc, "score_eff": sc_eff})  # type: ignore
                elif sc_eff >= self.tentative_thresh:
                    self.kb.remember_xform(inp, out, chain, confidence=float(sc_eff), meta={"prov_source": source, "score_raw": sc, "score_eff": sc_eff})  # type: ignore
                if hasattr(self.kb, "promote_tentative"):
                    self.kb.promote_tentative()  # type: ignore
            except Exception:
                pass

        # Local stats
        if out_shape:
            s = self._shape_stats[str(inp_shape)]
            s["counts"][str(out_shape)] += 1
            if success: s["success"][str(out_shape)] += 1
            else:       s["fail"][str(out_shape)] += 1
            s["last_ts"] = time.time()

        # Symbolic ML feedback
        if self.ml is not None and hasattr(self.ml, "record_feedback"):
            try:
                self.ml.record_feedback(label=source or "inference", memory_layer="meta", success=success, weight=1.0, meta={"task_id": task_id, "score": sc})
            except Exception:
                pass

        # Curiosity orchestrator
        try:
            self.handle_curiosity(inp, out, success=success, task_id=task_id)
        except Exception:
            pass

        # Holo learn-on-success (with tentative shadow option)
        try:
            if (inp is not None) and (out is not None):
                self._holo_learn_if_ok(inp, out, sc_eff, source or "inference", task_id, train_index)
        except Exception:
            raise

        # Adapt thresholds/curiosity periodically
        if self._observe_counter % 50 == 0:
            self._adaptive_thresholds()
            self._adaptive_curiosity()

        # Entropy spike monitor
        if len(self._archetype_entropy) >= 2:
            try:
                last_H = self._archetype_entropy[-1][1]
                prev_H = self._archetype_entropy[-2][1]
                if abs(last_H - prev_H) > 0.5:
                    self.mode = "ADAPT"
                    self._log_telemetry({
                        "module":"meta","event":"entropy_spike",
                        "prev": prev_H,"current": last_H
                    })
            except Exception:
                pass

        # Digest telemetry → KB occasionally
        if self._observe_counter % 200 == 0:
            self._digest_telemetry_to_kb()

        # Pulse
        self._pulse("observe", task_id=task_id, success=bool(success), score=float(sc), eff=float(sc_eff))

    # -------------------------------------------
    # Curiosity orchestration (preserved)
    # -------------------------------------------
    def should_explore(self) -> bool:
        return getattr(self, "curiosity_budget", 0) > 0

    def spend_curiosity(self, amt: int = 1):
        self.curiosity_budget = max(0, getattr(self, "curiosity_budget", 0) - int(amt))
        setattr(self, "_curiosity_spent", getattr(self, "_curiosity_spent", 0) + int(amt))
        self._log_telemetry({"module":"meta","event":"curiosity_spend","amt": int(amt), "remaining": self.curiosity_budget})
        self._kb_push("curiosity_spend", amt=int(amt), remaining=self.curiosity_budget, t=time.time())

    def handle_curiosity(self, inp, out, success: bool, task_id=None):
        if not hasattr(self, "curiosity") or self.curiosity is None:
            return
        if not success:
            if self.curiosity.bump_fail_streak():  # type: ignore
                found = self.curiosity.explore(inp, out, budget=3, task_id=task_id)  # type: ignore
                if found > 0:
                    self._log_telemetry({
                        "module":"meta","event":"curiosity_triggered",
                        "task_id":task_id,"fail_streak":self.curiosity.fail_streak,  # type: ignore
                        "new_rules":found
                    })
                self.curiosity.reset_fail_streak()  # type: ignore
        else:
            self.curiosity.reset_fail_streak()  # type: ignore

    # -------------------------------------------
    # Experience learning + sandbox rescue (preserved)
    # -------------------------------------------
    def learn_from_experience(self, inp, out, rule, success, comps,
                              ops=None, task_id=None, train_index=None):
        self.observe(comps, success, inp, out, rule, ops, task_id, train_index)

        if (not success) and self.should_explore() and (self.sandbox is not None):
            alt = None
            try:
                alt = self.sandbox.discover_chain(inp, out)  # type: ignore
            except Exception:
                alt = None

            if alt:
                try:
                    commit_xform(  # type: ignore
                        inp, out, alt, solver=globals().get("solver", None),
                        meta_extra={
                            "discovery":"sandbox",
                            "task_id":task_id,
                            "train_index":train_index,
                            "phase":os.environ.get("KAGGLE_PHASE","train")
                        },
                        conf=0.70, source="sandbox"
                    )
                except Exception:
                    try:
                        self.kb.remember_xform(inp, out, alt, confidence=0.7)  # type: ignore
                    except Exception:
                        pass

                try:
                    if hasattr(self, "ml") and self.ml is not None:
                        self.ml.ingest_sandbox_outcome(
                            kind="discover_chain", success=True,
                            inp=inp, out=out, chain=alt, score=1.0,
                            task_id=task_id, train_index=train_index
                        )
                except Exception:
                    pass
            else:
                try:
                    ts = getattr(out, "shape", None)
                    if ts and self.sandbox is not None:
                        ref = np.zeros(ts, dtype=int)
                        rescued = self.sandbox.rescue(inp, ts, self.sim, ref)  # type: ignore
                        if self.holo is not None:
                            self.holo.add(inp, rescued, {
                                "subject": task_id or "generic",
                                "depth": 1, "rule_kind": "rescue_shape",
                                "confidence": 0.5
                            })
                        self._log_telemetry({
                            "module":"sandbox","event":"rescue_shape","ok":True,
                            "target_shape":str(tuple(ts))
                        })
                except Exception:
                    pass
            self.spend_curiosity()

    # -------------------------------------------
    # KB / export helpers (preserved)
    # -------------------------------------------
    def _kb_push(self, topic: str, **payload):
        try:
            if hasattr(self.kb, "push_meta_stats") and callable(self.kb.push_meta_stats):
                self.kb.push_meta_stats(topic, dict(payload))
            elif hasattr(self.kb, "append_log") and callable(self.kb.append_log):
                self.kb.append_log(f"meta.{topic}", dict(payload))
        except Exception:
            pass

    def meta_summary(self) -> Dict[str, Any]:
        out = {
            "phase": self.phase,
            "pulses": int(self._pulse_ct),
            "telemetry_queue": int(len(self.telemetry)),
            "has": {
                "kb": bool(self.kb is not None),
                "rulebase": bool(self.rulebase is not None),
                "rulegen": bool(self.rulegen is not None),
                "sandbox": bool(self.sandbox is not None),
                "holo": bool(self.holo is not None),
                "sim": bool(self.sim is not None),
                "encoder": bool(self.encoder is not None),
                "ultra": bool(self.ultra is not None),
                "kairos": bool(self.kairos is not None),
                "blender": bool(self.blender is not None),
            },
        }
        self._emit("meta.summary", **out)
        if self.holo and hasattr(self.holo, "_telemetry"):
            try:
                self.holo._telemetry("meta.summary", **out)
            except Exception:
                pass
        return out

    def export_meta_csv(self, path: str = os.path.join("exports", "meta", "meta_summary.csv")):
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            s = self.meta_summary()
            with open(path, "w", newline="", encoding="utf-8") as f:
                w = csv.writer(f); w.writerow(["key", "value"])
                w.writerow(["phase", s.get("phase","")])
                w.writerow(["pulses", s.get("pulses",0)])
                w.writerow(["telemetry_queue", s.get("telemetry_queue",0)])
                for k, v in (s.get("has") or {}).items():
                    w.writerow([f"has.{k}", int(bool(v))])
            self._emit("meta.export_csv", path=path)
        except Exception as e:
            self._emit("meta.export_csv_error", error=str(e))

    # -------------------------------------------
    # Adaptive dials (preserved)
    # -------------------------------------------
    def _adaptive_thresholds(self):
        try:
            last = [ev for ev in self.telemetry if ev.get("event")=="observe"][-200:]
            if not last: return
            succ = [1.0 if ev.get("success") else 0.0 for ev in last]
            avg  = float(sum(succ)/len(succ))
            tgt_c = min(self._thresh_ceiling, max(self._thresh_floor_c, 0.75 + 0.15*(avg-0.5)))
            tgt_t = min(self._thresh_ceiling, max(self._thresh_floor_t, 0.40 + 0.20*(avg-0.5)))
            self.confident_thresh = 0.9*self.confident_thresh + 0.1*tgt_c
            self.tentative_thresh = 0.9*self.tentative_thresh + 0.1*tgt_t
            self._log_telemetry({
                "module":"meta","event":"thresh_update",
                "confident":self.confident_thresh,"tentative":self.tentative_thresh
            })
            self._kb_push("thresholds", confident=self.confident_thresh, tentative=self.tentative_thresh, avg=avg, t=time.time())
        except Exception:
            pass

    def _adaptive_curiosity(self):
        try:
            last = [ev for ev in self.telemetry if ev.get("event")=="observe"][-100:]
            if not last: return
            succ = [1 if ev.get("success") else 0 for ev in last]
            rate = float(sum(succ)/len(succ))
            if not hasattr(self, "curiosity_budget"):
                self.curiosity_budget = 0
            if not hasattr(self, "_curiosity_max"):
                self._curiosity_max = 64
            if not hasattr(self, "_curiosity_step_up"):
                self._curiosity_step_up = 3
            if not hasattr(self, "_curiosity_step_dn"):
                self._curiosity_step_dn = 1
            if rate < 0.30:
                self.curiosity_budget = min(self._curiosity_max, self.curiosity_budget + self._curiosity_step_up)
            elif rate > 0.55:
                self.curiosity_budget = max(0, self.curiosity_budget - self._curiosity_step_dn)
            self._log_telemetry({
                "module":"meta","event":"curiosity_budget",
                "rate":rate,"budget":self.curiosity_budget
            })
            self._kb_push("curiosity_budget", rate=rate, budget=self.curiosity_budget, t=time.time())
        except Exception:
            pass

    # -------------------------------------------
    # Telemetry replay (preserved)
    # -------------------------------------------
    def replay_telemetry(self):
        count = 0
        for ev in self.telemetry:
            if ev.get("event") == "observe":
                comps = {"score": ev.get("score", 0.0)}
                if hasattr(self.sim, "nudge_after_outcome"):
                    self.sim.nudge_after_outcome(comps, bool(ev.get("success")))
                count += 1
        try:
            for i, node in enumerate(self._ev_nodes[:-2]):
                if node.get("module") == "sandbox" and node.get("event") in ("discover_chain","rescue_shape"):
                    for j in range(i+1, min(i+6, len(self._ev_nodes))):
                        nxt = self._ev_nodes[j]
                        if nxt.get("module") == "meta" and nxt.get("event")=="observe" and nxt.get("success"):
                            try:
                                if hasattr(self, "symbolic"):
                                    self.symbolic.record_feedback(  # type: ignore
                                        label="sandbox_assist",
                                        memory_layer="meta",
                                        success=True,
                                        weight=0.5,
                                        meta={"from_event": node.get("event")}
                                    )
                            except Exception:
                                pass
                            self._log_telemetry({
                                "module":"meta","event":"self_reflection",
                                "narrative":f"Sandbox assist '{node.get('event')}' led to success"
                            })
                            break
        except Exception:
            pass

        try:
            if 'logger' in globals():
                logger.info(f"🔁 Telemetry replay reinforced {count} observations.")
        except Exception:
            pass
        self._kb_push("replay", reinforced=count, t=time.time())

    # -------------------------------------------
    # Telemetry → KB digestion (motifs)
    # -------------------------------------------
    def _digest_telemetry_to_kb(self):
        try:
            window = list(self.history)[-200:]
            chain_stats = defaultdict(lambda: {"cnt":0, "ok":0, "score_sum":0.0, "samples": []})
            for r in window:
                ops = r.get("ops")
                if not ops:
                    continue
                try:
                    canon = tuple((op[0], tuple(sorted(op[1].items()))) for op in ops if isinstance(op, (list, tuple)) and len(op)==2 and isinstance(op[1], dict))
                except Exception:
                    continue
                key = hashlib.md5(repr(canon).encode("utf-8")).hexdigest()
                cs = chain_stats[key]
                cs["cnt"] += 1
                cs["ok"]  += 1 if r.get("success") else 0
                cs["score_sum"] += float(r.get("score", 0.0))
                if len(cs["samples"]) < 5:
                    cs["samples"].append(r)
            for key, st in chain_stats.items():
                if st["cnt"] < 3:
                    continue
                acc = st["ok"] / float(st["cnt"])
                avg_s = st["score_sum"] / float(st["cnt"])
                if acc >= 0.6 or avg_s >= self.confident_thresh:
                    r0 = st["samples"][0]
                    inp_shape = r0.get("inp_shape", ())
                    out_shape = r0.get("out_shape", ())
                    try:
                        self.kb.promote_motif_by_ops(  # type: ignore
                            motif_key=key,
                            ops=r0.get("ops"),
                            stats={"count": st["cnt"], "acc": acc, "avg_score": avg_s, "inp_shape": inp_shape, "out_shape": out_shape}
                        )
                        self._log_telemetry({"module":"meta","event":"kb_motif_promotion","motif": key, "acc": acc, "avg_score": avg_s, "count": st["cnt"]})
                    except Exception:
                        self._log_telemetry({"module":"meta","event":"kb_motif_detected","motif": key, "acc": acc, "avg_score": avg_s, "count": st["cnt"]})
            self._kb_push("motifs", n=len(chain_stats), t=time.time())
        except Exception as e:
            self._emit("meta.warn", where="_digest_telemetry_to_kb", error=str(e))

    # -------------------------------------------
    # Maintenance tick (batch)
    # -------------------------------------------
    def maintenance_tick(self):
        self._pulse("maintenance_tick", dt=time.time()-self._last_pulse_ts)
        if self.curiosity and hasattr(self.curiosity, "maintenance"):
            try: self.curiosity.maintenance()  # type: ignore
            except Exception: pass
        if self.encoder and hasattr(self.encoder, "prune"):
            try: self.encoder.prune()  # type: ignore
            except Exception: pass
        if self.sandbox and hasattr(self.sandbox, "decay"):
            try: self.sandbox.decay()  # type: ignore
            except Exception: pass

    # -------------------------------------------
    # Reporting + Visualization (preserved)
    # -------------------------------------------
  
    def _in_dir(self, prefix_or_path: str, ext: Optional[str] = None) -> str:
        base = prefix_or_path
        export_dir = os.path.join("exports", "meta")
        if not any(sep in base for sep in (os.sep, "/", "\\")):
            base = os.path.join(export_dir, base)
        if ext and not os.path.splitext(base)[1]:
            base = f"{base}.{ext.lstrip('.')}"
        self._ensure_dir(os.path.dirname(base))
        return base

    def export_archetype_survival(self, prefix="archetype_survival"):
        try:
            csv_path = self._in_dir(prefix, "csv")
            png_path = self._in_dir(prefix, "png")
            with open(csv_path, "w", newline="") as f:
                w = csv.writer(f); w.writerow(["time","count"])
                for t, c in self._archetype_counts:
                    w.writerow([t, c])
            import matplotlib.pyplot as plt
            if self._archetype_counts:
                ts, cs = zip(*self._archetype_counts)
                plt.figure(figsize=(7,4)); plt.plot(ts, cs, linewidth=2)
                plt.title("Archetype Survival Over Time"); plt.xlabel("time"); plt.ylabel("count")
                plt.tight_layout(); plt.savefig(png_path); plt.close()
            self._log_telemetry({"module":"meta","event":"export_archetype_survival","prefix":prefix})
        except Exception:
            pass

    def export_label_correlation(self, prefix="label_correlation"):
        try:
            labels = list(self._symbolic_timeline.keys())
            if not labels:
                return
            series = {lab: [y for (_, y) in self._symbolic_timeline[lab]][-self._rolling_window:]
                      for lab in labels}
            L = len(labels)
            mat = np.zeros((L, L), dtype=float)
            for i in range(L):
                for j in range(L):
                    s1, s2 = series[labels[i]], series[labels[j]]
                    m = min(len(s1), len(s2))
                    if m >= 6:
                        v1, v2 = np.array(s1[-m:], float), np.array(s2[-m:], float)
                        c = float(np.corrcoef(v1, v2)[0,1]) if np.std(v1)>1e-9 and np.std(v2)>1e-9 else 0.0
                    else:
                        c = 0.0
                    mat[i, j] = c
            csv_path = self._in_dir(prefix, "csv")
            with open(csv_path, "w", newline="") as f:
                w = csv.writer(f); w.writerow([""]+labels)
                for i, lab in enumerate(labels):
                    w.writerow([lab]+[float(x) for x in mat[i]])
            import matplotlib.pyplot as plt
            png_path = self._in_dir(prefix, "png")
            plt.figure(figsize=(10,8))
            plt.imshow(mat, vmin=-1, vmax=1)
            plt.colorbar(label="corr")
            plt.xticks(range(L), labels, rotation=90)
            plt.yticks(range(L), labels)
            plt.title("Label Correlation Heatmap")
            plt.tight_layout(); plt.savefig(png_path); plt.close()
            self._log_telemetry({"module":"meta","event":"export_label_correlation","prefix":prefix})
        except Exception:
            pass

    def export_meta_timeline(self, prefix="meta_timeline"):
        try:
            csv_path = self._in_dir(prefix, "csv")
            with open(csv_path, "w", newline="") as f:
                w = csv.writer(f)
                w.writerow(["idx","time","task_id","train_index","score","success","rule","inp_shape","out_shape"])
                for r in self.history:
                    w.writerow([r.get("record_idx"), r.get("timestamp"), r.get("task_id"),
                                r.get("train_index"), r.get("score"), int(r.get("success",0)),
                                r.get("rule"), str(r.get("inp_shape")), str(r.get("out_shape"))])
            import matplotlib.pyplot as plt
            ts = [r["timestamp"] for r in self.history]
            sc = [r["score"] for r in self.history]
            ok = [1 if r["success"] else 0 for r in self.history]
            if ts:
                png_scores = self._in_dir(f"{prefix}_scores", "png")
                plt.figure(figsize=(8,4))
                plt.plot(ts, sc, alpha=0.85)
                plt.title("Score Timeline"); plt.xlabel("time"); plt.ylabel("score")
                plt.tight_layout(); plt.savefig(png_scores); plt.close()

                buf, roll = [], []
                for y in ok:
                    buf.append(y); buf = buf[-self._rolling_window:]
                    roll.append(float(sum(buf)/len(buf)))
                png_success = self._in_dir(f"{prefix}_success", "png")
                plt.figure(figsize=(8,4))
                plt.plot(ts, roll, linewidth=2)
                plt.title("Rolling Success"); plt.xlabel("time"); plt.ylabel("rate"); plt.ylim(-0.05,1.05)
                plt.tight_layout(); plt.savefig(png_success); plt.close()
            self._log_telemetry({"module":"meta","event":"export_meta_timeline","prefix":prefix})
        except Exception:
            pass

    def export_mode_timeline(self, path="meta_modes.png"):
        try:
            if not self._mode_log:
                return
            import matplotlib.pyplot as plt
            out_path = self._in_dir(path)
            mt, modes = zip(*self._mode_log)
            plt.figure(figsize=(7,2))
            plt.plot(mt, [1 if m=="ADAPT" else 0 for m in modes], drawstyle="steps-post")
            plt.title("Mode timeline (1=ADAPT, 0=NORMAL)")
            plt.tight_layout(); plt.savefig(out_path); plt.close()
            self._log_telemetry({"module":"meta","event":"export_mode_timeline","path":out_path})
        except Exception:
            pass

    def export_success_hist(self, path="meta_success_hist.png"):
        try:
            ok = [1 if r.get("success") else 0 for r in self.history]
            if not ok:
                return
            import matplotlib.pyplot as plt
            out_path = self._in_dir(path)
            plt.figure(figsize=(6,4))
            plt.hist(ok, bins=[-0.5,0.5,1.5])
            plt.title("Success/Failure histogram"); plt.xlabel("0/1"); plt.ylabel("count")
            plt.tight_layout(); plt.savefig(out_path); plt.close()
            self._log_telemetry({"module":"meta","event":"export_success_hist","path":out_path})
        except Exception:
            pass

    def export_dashboard(self, prefix="meta_dashboard"):
        try:
            import matplotlib.pyplot as plt
            from matplotlib.backends.backend_pdf import PdfPages
            pdf_path = self._in_dir(prefix, "pdf")
            with PdfPages(pdf_path) as pdf:
                if self._archetype_counts:
                    ts, counts = zip(*self._archetype_counts)
                    plt.figure(); plt.plot(ts, counts); plt.title("Archetype survival"); pdf.savefig(); plt.close()
                if self._archetype_entropy:
                    ts, Hs = zip(*self._archetype_entropy)
                    plt.figure(); plt.plot(ts, Hs); plt.title("Archetype diversity (entropy)"); pdf.savefig(); plt.close()
                if self._mode_log:
                    mt, modes = zip(*self._mode_log)
                    plt.figure(); plt.plot(mt, [1 if m=="ADAPT" else 0 for m in modes]); plt.title("Mode timeline"); pdf.savefig(); plt.close()
                ok = [1 if r.get("success") else 0 for r in self.history]
                if ok:
                    plt.figure(); plt.hist(ok, bins=2); plt.title("Success/fail histogram"); pdf.savefig(); plt.close()
        except Exception:
            pass

    # -------------------------------------------
    # Telemetry exports (preserved)
    # -------------------------------------------
    def export_telemetry_jsonl(self, path="meta_telemetry.jsonl"):
        try:
            out_path = self._in_dir(path)
            with open(out_path, "w", encoding="utf-8") as f:
                for ev in self.telemetry:
                    f.write(json.dumps(ev, default=str) + "\n")
            self._log_telemetry({"module":"meta","event":"export_telemetry_jsonl","path":out_path,"n":len(self.telemetry)})
        except Exception:
            pass

    def export_telemetry_summary(self, prefix="meta_telemetry"):
        try:
            by_kind = defaultdict(lambda: {"ok":0, "fail":0})
            for ev in self.telemetry:
                if ev.get("event")=="observe":
                    rk = ev.get("rule_kind","")
                    if ev.get("success"): by_kind[rk]["ok"] += 1
                    else:                  by_kind[rk]["fail"] += 1
            csv_path = self._in_dir(f"{prefix}_summary", "csv")
            with open(csv_path, "w", newline="") as f:
                w = csv.writer(f); w.writerow(["rule_kind","ok","fail","acc"])
                for k,v in by_kind.items():
                    tot = v["ok"]+v["fail"]; acc = (v["ok"]/tot) if tot>0 else 0.0
                    w.writerow([k,v["ok"],v["fail"],acc])
            try:
                import matplotlib.pyplot as plt
                labs = list(by_kind.keys())
                accs = [(v["ok"]/max(1,(v["ok"]+v["fail"]))) for v in by_kind.values()]
                png_path = self._in_dir(f"{prefix}_acc", "png")
                plt.figure(figsize=(7,4))
                plt.bar(labs, accs); plt.title("Telemetry accuracy per rule_kind"); plt.ylim(0,1)
                plt.tight_layout(); plt.savefig(png_path); plt.close()
            except Exception:
                pass
            self._log_telemetry({"module":"meta","event":"export_telemetry_summary","prefix":prefix})
        except Exception:
            pass

    # -------------------------------------------
    # Event graph export (preserved)
    # -------------------------------------------
    def export_event_graph(self, prefix="meta_event_graph"):
        try:
            json_path = self._in_dir(prefix, "json")
            csv_path  = self._in_dir(f"{prefix}_edges", "csv")
            graph = {
                "nodes": self._ev_nodes,
                "edges": [{"src": u, "dst": v, "label": lbl} for (u,v,lbl) in self._ev_edges]
            }
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(graph, f, indent=2)
            with open(csv_path, "w", newline="", encoding="utf-8") as f:
                w = csv.writer(f); w.writerow(["src","dst","label"])
                for (u,v,lbl) in self._ev_edges:
                    w.writerow([u,v,lbl])
            self._log_telemetry({"module":"meta","event":"export_event_graph","prefix":prefix,
                                 "nodes":len(self._ev_nodes),"edges":len(self._ev_edges)})
        except Exception:
            pass

    # -------------------------------------------
    # Shape stats exports (preserved)
    # -------------------------------------------
    def export_shape_stats(self, prefix="meta_shapes"):
        try:
            csv_path = self._in_dir(prefix, "csv")
            js_path  = self._in_dir(f"{prefix}_summary", "json")
            rows = []
            summary = {}
            for inp_s, rec in self._shape_stats.items():
                total = sum(rec["counts"].values())
                if total == 0: continue
                summary[inp_s] = {"total": int(total), "outcomes": {}}
                for out_s, c in rec["counts"].items():
                    ok = rec["success"].get(out_s, 0)
                    fl = rec["fail"].get(out_s, 0)
                    acc = (ok / (ok+fl)) if (ok+fl)>0 else 0.0
                    rows.append([inp_s, out_s, c, ok, fl, round(acc, 6)])
                    summary[inp_s]["outcomes"][out_s] = {"count": int(c), "ok": int(ok), "fail": int(fl), "acc": float(acc)}
            with open(csv_path, "w", newline="", encoding="utf-8") as f:
                w = csv.writer(f); w.writerow(["inp_shape","out_shape","count","ok","fail","acc"])
                for r in rows: w.writerow(r)
            with open(js_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2)
            try:
                import matplotlib.pyplot as plt
                shapes = list(summary.keys())
                accs = []
                for s in shapes:
                    outs = summary[s]["outcomes"]
                    if outs:
                        top = max(outs.items(), key=lambda kv: kv[1]["count"])
                        accs.append(top[1]["acc"])
                    else:
                        accs.append(0.0)
                png_path = self._in_dir(f"{prefix}_acc", "png")
                plt.figure(figsize=(max(6,len(shapes)*0.4), 3))
                plt.bar(range(len(shapes)), accs)
                plt.xticks(range(len(shapes)), shapes, rotation=90)
                plt.ylim(0,1); plt.title("Top-Outcome Accuracy per Input Shape")
                plt.tight_layout(); plt.savefig(png_path); plt.close()
            except Exception:
                pass
            self._log_telemetry({"module":"meta","event":"export_shape_stats","prefix":prefix})
        except Exception:
            pass

    # -------------------------------------------
    # Dual evaluation ingestion (preserved)
    # -------------------------------------------
    def ingest_dual_eval(self, strict_solver_acc: float, partial_solver_acc: float,
                         strict_sandbox_acc: float, partial_sandbox_acc: float,
                         total_cases: int):
        if not hasattr(self, "ml") or self.ml is None:
            return
        try:
            self.ml.ingest_dual_outcome(
                system="solver",
                strict_acc=float(strict_solver_acc),
                partial_acc=float(partial_solver_acc),
                total=int(total_cases)
            )
            self.ml.ingest_dual_outcome(
                system="sandbox",
                strict_acc=float(strict_sandbox_acc),
                partial_acc=float(partial_sandbox_acc),
                total=int(total_cases)
            )
        except Exception:
            pass

    # -------------------------------------------
    # Shutdown (flush + checkpoint) (preserved)
    # -------------------------------------------
    def shutdown(self):
        try:
            self.export_meta_csv()
            self.export_shape_stats()
            self.export_event_graph()
            # dashboard/JSONL optional (callers can add)
        except Exception:
            pass
        try:
            if self.ml is not None:
                self.ml.save_state(); self.ml.stop()
        except Exception:
            pass



# ===========================================
# Sandbox Explorer  (Full Integrated Version + RHCM H2H + Intelligent Search)
# ===========================================
# ---------- safe defaults / knobs (legacy names preserved) ----------
SANDBOX_MAX_DEPTH            = globals().get("SANDBOX_MAX_DEPTH", 3)
SANDBOX_BEAM_WIDTH           = globals().get("SANDBOX_BEAM_WIDTH", 8)
SANDBOX_STOCH_EPS            = globals().get("SANDBOX_STOCH_EPS", 0.08)
SANDBOX_COMPLEXITY_PEN       = globals().get("SANDBOX_COMPLEXITY_PEN", 0.03)
SANDBOX_DIFF_VISUALS         = globals().get("SANDBOX_DIFF_VISUALS", True)
SANDBOX_DIFF_DIR             = globals().get("SANDBOX_DIFF_DIR", "sandbox_diffs")
SANDBOX_LOG_JSONL            = globals().get("SANDBOX_LOG_JSONL", "sandbox_events.jsonl")
SANDBOX_VISUAL_MAX_PER_TASK  = globals().get("SANDBOX_VISUAL_MAX_PER_TASK", 2)
SANDBOX_EXPORT_DIR           = globals().get("SANDBOX_EXPORT_DIR", os.path.join("exports", "sandbox"))
SANDBOX_ROLLUP_CSV           = globals().get("SANDBOX_ROLLUP_CSV", os.path.join(SANDBOX_EXPORT_DIR, "task_rollup.csv"))

# optional JSONL housekeeping (bytes); 0 disables
_SANDBOX_JSONL_ROTATE_BYTES  = globals().get("_SANDBOX_JSONL_ROTATE_BYTES", 2_000_000)

# ---------- local lightweight priors (operator credit) ----------
_OP_PRIORS: Dict[Tuple[str, Tuple[int,int], Tuple[int,...]], float] = defaultdict(float)

def _op_prior_key(op_name: str, grid: np.ndarray) -> Tuple[str, Tuple[int,int], Tuple[int,...]]:
    try:
        pal = tuple(sorted(np.unique(grid).tolist())[:6])
        return (op_name, tuple(grid.shape), pal)
    except Exception:
        return (op_name, (0,0), ())

def _get_op_prior(op_name: str, grid: np.ndarray) -> float:
    return float(_OP_PRIORS.get(_op_prior_key(op_name, grid), 0.0))

def _update_op_prior(op_name: str, grid: np.ndarray, reward: float):
    try:
        k = _op_prior_key(op_name, grid)
        _OP_PRIORS[k] = float(max(-1.0, min(1.0, _OP_PRIORS.get(k, 0.0) + 0.5 * reward)))
    except Exception:
        pass

# ---------- simple canonicalization for cycles/sym redundancies ----------
def _canonicalize_ops(chain: List[Tuple[str, Dict[str, Any]]]) -> List[Tuple[str, Dict[str, Any]]]:
    rot_sum = 0
    out: List[Tuple[str, Dict[str, Any]]] = []
    for name, kw in chain:
        if name == "rot":
            rot_sum = (rot_sum + int(kw.get("k", 1))) % 4
            continue
        else:
            if rot_sum % 4 != 0:
                out.append(("rot", {"k": rot_sum}))
                rot_sum = 0
            if out and name in ("flip_lr", "flip_ud") and out[-1][0] == name:
                out.pop()
                continue
            out.append((name, kw))
    if rot_sum % 4 != 0:
        out.append(("rot", {"k": rot_sum}))
    return out


# ==========================================================
# Sandbox Explorer (rewritten, hardened, RHCM-aware)
# ==========================================================
# Metrics/physics/geometry shims
_epi = globals().get("_epi", _safe0)
_shape_similarity = globals().get("_shape_similarity", _safe0)
_physics_plausibility = globals().get("_physics_plausibility", _safe0)
_inv_composite = globals().get("_inv_composite", _safe0)
_op_priority = globals().get("_op_priority", lambda name: 0.5)
_get_op_prior = globals().get("_get_op_prior", lambda name, arr=None: 0.0)
_entropy = globals().get("_entropy", _safe0)
_binder_like = globals().get("_binder_like", _safe0)
_entropy_persistence_index = globals().get("_entropy_persistence_index", _safe0)

# Array helpers
_ensure_int_ndarray = globals().get("_ensure_int_ndarray", lambda a: np.asarray(a, dtype=int))
_sanitize_grid = globals().get("_sanitize_grid", lambda a: np.asarray(a, dtype=int))
pad_to_same_shape = globals().get("pad_to_same_shape", lambda a, b, pad_val=0: (a, b))
_mass_pad = globals().get("_mass_pad", lambda a, shape, pad: np.full(shape, pad, int))
_entropy_crop = globals().get("_entropy_crop", lambda a, shape: a[:shape[0], :shape[1]])
_canonicalize_ops = globals().get("_canonicalize_ops", lambda ops: ops)

# Logging / telemetry shims
meta_log = globals().get("meta_log", lambda *a, **k: None)

# KEEL wrappers (shims)
keel_compress_grid = globals().get("keel_compress_grid", lambda grid_u8, q_ll=3.0, deblock=True: (bytes([0]), {}))
keel_decompress_grid = globals().get("keel_decompress_grid", lambda blob: np.zeros((1,1), dtype=np.uint8))
keel_metrics = globals().get("keel_metrics", lambda a, b: {"psnr": 0.0, "ssim_proxy": 0.0})

# Kairos shim
KairosPulseManager = globals().get("KairosPulseManager", None)

# Rulebase / KB / record shims
Rule = globals().get("Rule", lambda *a, **k: ("rule", a, k))
RuleRecord = globals().get("RuleRecord", lambda *a, **k: ("record", a, k))
commit_xform = globals().get("commit_xform", lambda *a, **k: None)

# Candidate generator shim
generate_candidates = globals().get("generate_candidates", lambda g: [])

# -----------------------------------------------------------
# Cheating guard for target-shape usage (unchanged policy)
# -----------------------------------------------------------
def _allow_target_shape_use() -> bool:
    phase = os.environ.get("KAGGLE_PHASE", "train").strip().lower()
    # disallow during mock test / submission
    return phase not in {"mock", "submission"}

# ==========================================================
#  Sandbox Explorer
# ==========================================================
class SandboxExplorer:
    # -------- helper: scoped paths per run/lane --------
    def _paths(self, lane: str) -> Dict[str, str]:
        base = os.path.join("runs", self.run_id, "sandbox", str(lane))
        try:
            os.makedirs(base, exist_ok=True)
            os.makedirs(os.path.join(base, "diffs"), exist_ok=True)
            os.makedirs(os.path.join(base, "exports"), exist_ok=True)
        except Exception:
            pass
        return {
            "base": base,
            "log_jsonl": os.path.join(base, "sandbox_events.jsonl"),
            "rollup_csv": os.path.join(base, "exports", "task_rollup.csv"),
            "discover_csv": os.path.join(base, "exports", "_discover_events.csv"),
            "near_miss_jsonl": os.path.join(base, "exports", "near_misses.jsonl"),
            "diff_dir": os.path.join(base, "diffs"),
        }

    def __init__(self,
                 kb: Optional["SymbolicKB"] = None,
                 max_depth: int = SANDBOX_MAX_DEPTH,
                 trans_shifts: Optional[List[int]] = None,
                 meta: Optional["MetaLayer"] = None,
                 holo: Optional["HoloMemory"] = None,
                 ultra: Optional["SymbolicUltraAgent"] = None,
                 rulebase: Optional["GlobalRulebase"] = None,
                 curiosity: Optional["CuriosityEngine"] = None,
                 sim: Optional["HybridSimilarity"] = None,
                 run_id: Optional[str] = None,
                 **kwargs):
        self.kb = kb
        self.meta = meta
        self.holo = holo
        self.ultra = ultra
        self.rulebase = rulebase
        self.curiosity = curiosity
        self.sim = sim
        self.run_id = str(run_id) if run_id is not None else "na"
        self.lane = os.environ.get("SANDBOX_LANE", "CLASSIC")
        self._pathmap = self._paths(self.lane)  # initial (may change per H2H lane)

        # Two-way wiring with Meta
        try:
            if self.meta is not None:
                if hasattr(self.meta, "attach_sandbox"): self.meta.attach_sandbox(self)
                if self.kb is not None and hasattr(self.meta, "attach_kb"): self.meta.attach_kb(self.kb)
                if self.holo is not None and hasattr(self.meta, "attach_holo"): self.meta.attach_holo(self.holo)
                if self.ultra is not None and hasattr(self.meta, "attach_ultra"): self.meta.attach_ultra(self.ultra)
        except Exception:
            pass

        self.max_depth = int(max_depth)
        self.trans_shifts = trans_shifts if trans_shifts is not None else [-2, -1, 0, 1, 2]

        self.learned_chains: List[List[Tuple[str, Dict[str, Any]]]] = []
        self._rescue_cache: Dict[Tuple[Any, ...], List[Tuple[str, Dict[str, Any]]]] = {}
        self._event_id = 0

        self.weights: Dict[str, float] = {"sim": 0.55, "phys": 0.20, "inv": 0.20, "prio": 0.15}
        self.weight_path = os.path.join("deployment", "sandbox_weights.json")

        # Kairos × KEEL rolling diagnostics
        self.kairos = None
        try:
            if KairosPulseManager is not None:
                self.kairos = KairosPulseManager(n=30, pulse_amplitude=0.3)  # type: ignore
        except Exception:
            self.kairos = None
        self.kairos_flux_history = deque(maxlen=256)
        self.keel_ratio_history  = deque(maxlen=256)
        self.last_keel_snap_ts   = time.time()
        self.last_kairos_ts      = time.time()

        # KEEL ratio cache
        self._keel_cache: Dict[Tuple[int, Tuple[int, int]], float] = {}
        self._depth_flux: float = 0.0

        # budget accounting
        self._last_budget_alloc = 0
        self._last_budget_used = 0

        self._emit("sandbox.init", run_id=self.run_id, max_depth=self.max_depth, shifts=self.trans_shifts, weights=self.weights)
        self._bootstrap_weights()

        self._visuals_used: Dict[str, int] = {}
        self._rollup_rows: List[Dict[str, Any]] = []
        try:
            os.makedirs(self._pathmap["diff_dir"], exist_ok=True)
            os.makedirs(os.path.dirname(self._pathmap["rollup_csv"]), exist_ok=True)
        except Exception:
            pass

        self._emit("sandbox.ready", max_depth=self.max_depth, shifts=self.trans_shifts, weights=self.weights)

    # -----------------------------
    # Telemetry (atomic, robust)
    # -----------------------------
    def _emit(self, event: str, **payload):
        # enrich payload with last kairos/keel
        try:
            if "kairos_flux" not in payload and self.kairos_flux_history:
                payload["kairos_flux"] = float(self.kairos_flux_history[-1])
        except Exception:
            pass
        try:
            if "keel_ratio" not in payload and self.keel_ratio_history:
                payload["keel_ratio"] = float(self.keel_ratio_history[-1])
        except Exception:
            pass

        # bus
        try:
            meta_log(event, **payload)
        except Exception:
            pass

        # KB narration (optional)
        try:
            if self.kb is not None and hasattr(self.kb, "narrations"):
                self.kb.narrations.append(f"[Sandbox] {event}: {payload}")
        except Exception:
            pass

        # rotation + append (scoped path)
        path = self._pathmap.get("log_jsonl", SANDBOX_LOG_JSONL)
        try:
            d = os.path.dirname(path)
            if d: os.makedirs(d, exist_ok=True)
        except Exception:
            pass
        try:
            if _SANDBOX_JSONL_ROTATE_BYTES and os.path.isfile(path):
                sz = os.path.getsize(path)
                if sz >= int(_SANDBOX_JSONL_ROTATE_BYTES):
                    import gzip, shutil
                    rot_path = path + ".1.gz"
                    with open(path, "rb") as fin, gzip.open(rot_path, "wb") as fout:
                        shutil.copyfileobj(fin, fout)
                    tmp = path + ".tmp"
                    with open(tmp, "w", encoding="utf-8") as ftmp:
                        ftmp.write("")
                    os.replace(tmp, path)
                    try: meta_log("sandbox.jsonl_rotated", path=rot_path, size=sz)
                    except Exception: pass
        except Exception:
            pass
        try:
            with open(path, "a", encoding="utf-8") as f:
                f.write(json.dumps({"ts": time.time(), "event": event, **payload}) + "\n")
        except Exception:
            pass

        # Ultra echo
        try:
            if self.ultra is not None and hasattr(self.ultra, "observe"):
                self.ultra.observe("sandbox", **{"event": event, **payload})
        except Exception:
            pass

        # meta hook
        try:
            if self.meta is not None and hasattr(self.meta, "record_sandbox_event"):
                self.meta.record_sandbox_event({"event": event, **payload})
        except Exception:
            pass

    # -----------------------------
    # Weights bootstrap/save
    # -----------------------------
    def _bootstrap_weights(self):
        try:
            if os.path.isfile(self.weight_path):
                with open(self.weight_path, "r") as f:
                    w = json.load(f)
                if isinstance(w, dict):
                    for k in list(self.weights.keys()):
                        if k in w:
                            self.weights[k] = float(w[k])
            s = sum(self.weights.values())
            if s > 0:
                for k in self.weights:
                    self.weights[k] /= s
            self._emit("sandbox.weights_bootstrap", weights=self.weights.copy())
        except Exception as e:
            self._emit("sandbox.weights_bootstrap_failed", error=str(e))
        try:
            if self.kairos is not None:
                self.kairos.step(time_step=len(self.learned_chains))
                flux = abs(getattr(self.kairos, "last_entropy_flux", 0.0))
                self.kairos_flux_history.append(float(flux))
                self._emit("sandbox.kairos_bootstrap", flux=float(flux))
        except Exception:
            pass

    def _save_weights(self):
        try:
            os.makedirs(os.path.dirname(self.weight_path), exist_ok=True)
            with open(self.weight_path, "w") as f:
                json.dump(self.weights, f, indent=2)
            self._emit("sandbox.weights_saved", path=self.weight_path)
        except Exception as e:
            self._emit("sandbox.weights_save_failed", error=str(e))
        try:
            mean_flux = float(np.mean(self.kairos_flux_history)) if self.kairos_flux_history else 0.0
            mean_keel = float(np.mean(self.keel_ratio_history)) if self.keel_ratio_history else 1.0
            self._emit("sandbox.system_drift", flux=mean_flux, keel=mean_keel)
            if self.meta is not None and hasattr(self.meta, "record_compression_drift"):
                self.meta.record_compression_drift(mean_keel)
        except Exception:
            pass

    def save_weights(self):
        return self._save_weights()

    def tune_weights(self, feedback: Dict[str, float]):
        for k, delta in (feedback or {}).items():
            if k in self.weights:
                self.weights[k] = max(0.0, min(1.0, self.weights[k] + float(delta)))
        s = sum(self.weights.values())
        if s > 0:
            for k in self.weights:
                self.weights[k] /= s
        self._save_weights()

    # -----------------------------
    # KEEL ratio cache helper
    # -----------------------------
    def _keel_ratio_cached(self, arr: np.ndarray) -> float:
        try:
            key = (id(arr), tuple(arr.shape))
            hit = self._keel_cache.get(key)
            if hit is not None:
                return float(hit)
            u8 = np.clip(_ensure_int_ndarray(arr), 0, 255).astype(np.uint8)
            b, _ = keel_compress_grid(u8, q_ll=3.0, deblock=True)
            r = float(u8.size) / max(1, len(b))
            self._keel_cache[key] = r
            if len(self._keel_cache) > 2048:
                self._keel_cache.pop(next(iter(self._keel_cache)))
            return float(r)
        except Exception:
            return 1.0

    # -----------------------------
    # Budget / Ω profiles (helpers)
    # -----------------------------
    def _keel_avg(self, k: int = 12) -> float:
        try:
            if self.keel_ratio_history:
                arr = list(self.keel_ratio_history)[-int(max(1, k)):]
                return float(sum(arr) / max(1, len(arr)))
        except Exception:
            pass
        return 1.0

    def _omega_state(self) -> str:
        try:
            f = float(abs(getattr(self, "_depth_flux", 0.0)))
            if f < 1.0:   return "Ω0"
            if f < 3.0:   return "Ω1"
            if f < 6.0:   return "Ω2"
            if f < 10.0:  return "Ω3"
            return "Ω4"
        except Exception:
            return "Ω2"

    def _budget_params(self, beam_width: int, md: int) -> Dict[str, Any]:
        # Compression scaled multiplier m ∈ [0.5, 1.0]
        try:
            keel = max(0.0, self._keel_avg())
            m = float(1.0 - np.tanh(max(0.0, keel - 1.0)))
            m = float(max(0.5, min(1.0, m)))
        except Exception:
            m = 1.0

        bw_eff = int(max(1, round(beam_width * m)))
        md_eff = int(max(1, round(md * (0.75 + 0.25 * m))))

        # Ω string (coarse) at init; detailed Ω is recomputed per-depth
        omega = self._omega_state()
        # conservative defaults; detailed dials are handled via _omega_dials(...)
        explore_frac = 0.25
        if omega in ("Ω0", "Ω1"):
            explore_frac = 0.15
        elif omega == "Ω3":
            explore_frac = 0.40
        elif omega == "Ω4":
            explore_frac = 0.50

        explore_cap = int(max(1, int(np.ceil(bw_eff * explore_frac))))
        return {
            "m": float(m),
            "beam_width_eff": int(bw_eff),
            "md_eff": int(md_eff),
            "explore_cap": int(explore_cap),
            "omega": omega,
        }

    def _discover_csv_path(self) -> str:
        try:
            rid = getattr(self, "run_id", "na")
            lane = getattr(self, "lane", os.environ.get("SANDBOX_LANE", "CLASSIC"))
            root = os.path.join("runs", str(rid), "sandbox", str(lane))
            os.makedirs(root, exist_ok=True)
            return os.path.join(root, "_discover_events.csv")
        except Exception:
            return "_discover_events.csv"

    def _append_discover_row(self, row: Dict[str, Any]) -> None:
        try:
            path = self._discover_csv_path()
            exists = os.path.isfile(path) and (os.path.getsize(path) > 0)
            tmp = path + ".tmp"
            with open(tmp, "a", newline="", encoding="utf-8") as f:
                w = csv.writer(f)
                if not exists:
                    w.writerow([
                        "ts","run_id","lane","task_id",
                        "beam_width","beam_width_eff","md","md_eff",
                        "omega","m","budget_allocated","budget_used",
                        "score","sim_final","inv_final","phys_final",
                        "gold_shape_used","keel_ratio","kairos_flux","chain_len"
                    ])
                w.writerow([
                    int(time.time()), getattr(self, "run_id", "na"), getattr(self, "lane", os.environ.get("SANDBOX_LANE","CLASSIC")),
                    row.get("task_id","na"),
                    row.get("beam_width",0), row.get("beam_width_eff",0), row.get("md",0), row.get("md_eff",0),
                    row.get("omega","Ω2"), f"{row.get('m',1.0):.6f}",
                    row.get("budget_allocated",0), row.get("budget_used",0),
                    f"{row.get('score',0.0):.6f}", f"{row.get('sim_final',0.0):.6f}", f"{row.get('inv_final',0.0):.6f}", f"{row.get('phys_final',0.0):.6f}",
                    int(bool(row.get("gold_shape_used",False))), f"{row.get('keel_ratio',1.0):.6f}", f"{row.get('kairos_flux',0.0):.6f}",
                    int(row.get("chain_len",0)),
                ])
            os.replace(tmp, path)
        except Exception:
            pass

    # -----------------------------
    # Ω mapping (Kairos flux → search dials)
    # -----------------------------
    def _omega_bucket(self, flux: float) -> int:
        # 0..4 bands (lightweight heuristic if no symbolic state available)
        try:
            state = getattr(self.kairos, "symbolic_state", None)
            if state is not None and isinstance(state, dict):
                return int(state.get("omega", 2))
        except Exception:
            pass
        f = abs(float(flux))
        if   f < 1.0:  return 0
        elif f < 3.0:  return 1
        elif f < 6.0:  return 2
        elif f < 10.0: return 3
        else:          return 4

    def _omega_dials(self, omega: int, beam_eff: int) -> Dict[str, Any]:
        # returns explore slots and filter tolerances
        if omega <= 1:
            return {"explore": max(1, beam_eff // 6),
                    "epi_gain_min": 0.01 if omega == 1 else 0.00,
                    "mass_tol": 0.08, "centroid_tol": 0.30}
        if omega == 2:
            return {"explore": max(1, beam_eff // 4),
                    "epi_gain_min": 0.005,
                    "mass_tol": 0.10, "centroid_tol": 0.35}
        if omega == 3:
            return {"explore": max(1, beam_eff // 3),
                    "epi_gain_min": -0.005,
                    "mass_tol": 0.12, "centroid_tol": 0.40}
        # omega 4
        return {"explore": max(1, (beam_eff + 1) // 2),
                "epi_gain_min": -0.02,
                "mass_tol": 0.14, "centroid_tol": 0.45}

    # -----------------------------
    # Neighbor ops (size/mass aware)
    # -----------------------------
    def _neighbor_ops(self, inp: np.ndarray, cur: np.ndarray, out: np.ndarray, allow_target: bool) -> List[Tuple[str, Dict[str, Any]]]:
        ops: List[Tuple[str, Dict[str, Any]]] = []
        for k in (1, 2, 3): ops.append(("rot", {"k": k}))
        ops += [("flip_lr", {}), ("flip_ud", {}), ("flip_diag", {}), ("flip_diag", {"anti": True})]
        for dr in self.trans_shifts:
            for dc in self.trans_shifts:
                if dr == 0 and dc == 0: continue
                ops.append(("trans", {"dr": dr, "dc": dc, "pad_val": 0}))
        for axis in ("v", "h", "vh"): ops.append(("symmetry", {"axis": axis}))
        try:
            pal = np.unique(cur)
            if len(pal) > 2:
                ops.append(("hist_recolor", {}))
                try:
                    vals, cnts = np.unique(cur, return_counts=True)
                    order = np.argsort(-cnts)[:8]
                    mapping = {int(vals[i]): int(j + 1) for j, i in enumerate(order)}
                    ops.append(("recolor", {"map": mapping}))
                except Exception: pass
        except Exception: pass
        shapes = set([cur.shape])
        if allow_target: shapes.add(out.shape)
        R, C = cur.shape
        for dR in (-2, -1, 1, 2):
            for dC in (-2, -1, 1, 2):
                r = max(1, R + dR); c = max(1, C + dC); shapes.add((r, c))
        try:
            mass_cur = float(np.sum(cur != -1)); mass_out = float(np.sum(out != -1))
            mass_delta = abs(mass_cur - mass_out) / max(1.0, mass_out) if mass_out > 0 else 0.0
        except Exception:
            mass_delta = 0.0
        try:
            recent_keel = float(np.mean(list(self.keel_ratio_history)[-8:])) if self.keel_ratio_history else 1.0
        except Exception:
            recent_keel = 1.0
        shape_list = list(shapes); random.shuffle(shape_list)
        shape_list = shape_list[:(6 if recent_keel > 1.15 else 10)]
        if mass_delta < 0.05:
            shape_list = shape_list[:max(3, len(shape_list)//2)]
        for sh in shape_list:
            ops.append(("pad_to", {"shape": sh, "pad_val": 0}))
            ops.append(("crop_to", {"shape": sh}))
            ops.append(("mass_pad", {"shape": sh, "pad_val": 0}))
            ops.append(("entropy_crop", {"shape": sh}))
        return ops

    # -----------------------------
    # Step scorer
    # -----------------------------
    def _score_step(self, prev: np.ndarray, cur: np.ndarray, out: np.ndarray, op_name: str) -> Tuple[float, Dict[str, float]]:
        color_sim = float(_epi(cur, out))
        shape_sim = float(_shape_similarity(cur, out))
        sim  = 0.6 * color_sim + 0.4 * shape_sim
        phys = float(_physics_plausibility(prev, cur))
        inv  = float(_inv_composite(prev, cur))
        base_prio = float(_op_priority(op_name))
        ctx_boost = float(_get_op_prior(op_name, cur))
        prio = 0.85 * base_prio + 0.15 * (0.5 * (ctx_boost + 1.0))
        kairos_flux = float(abs(self._depth_flux))
        comp_pen = float(SANDBOX_COMPLEXITY_PEN * (1.0 + 0.5 * np.tanh(kairos_flux / 10.0)))
        kairos_mod = 1.0 + 0.01 * np.tanh(kairos_flux / 10.0)
        try:
            r1 = self._keel_ratio_cached(prev); r2 = self._keel_ratio_cached(cur)
            keel_ratio = (r1 + r2) / 2.0
            self.keel_ratio_history.append(float(keel_ratio))
        except Exception:
            keel_ratio = 1.0
        score = self.weights["sim"] * sim + self.weights["phys"] * phys + self.weights["inv"] * inv + self.weights["prio"] * prio
        score -= comp_pen; score *= float(kairos_mod)
        det = {"sim": float(sim), "phys": float(phys), "inv": float(inv), "prio": float(prio),
               "comp_pen": float(comp_pen), "score": float(score),
               "kairos_flux": float(kairos_flux), "kairos_mod": float(kairos_mod), "keel_ratio": float(keel_ratio)}
        return score, det

    # -------------------------------------------------
    # discover_chain (intelligent hooks + budgets + Ω-aware search)
    # -------------------------------------------------
    def discover_chain(self, inp: np.ndarray, out: np.ndarray,
                       beam_width: int = SANDBOX_BEAM_WIDTH, max_depth: Optional[int] = None,
                       task_id: Optional[str] = None, allow_target_shape: Optional[bool] = None
                       ) -> Optional[List[Tuple[str, Dict[str, Any]]]]:
        # ----- scoped lane + paths -----
        md_base = int(max_depth or self.max_depth)
        inp = _sanitize_grid(inp); out = _sanitize_grid(out)
        allow_target = (self._allow_target_shape_use() if (allow_target_shape is None) else bool(allow_target_shape))
        self._event_id += 1
        self.lane = os.environ.get("SANDBOX_LANE", self.lane or "CLASSIC")
        self._pathmap = self._paths(self.lane)

        # ----- deterministic RNG (SHA1) -----
        from hashlib import sha1
        seed_hex = sha1(f"{self.run_id}|{self.lane}|{self._event_id}".encode("utf-8")).hexdigest()[:8]
        rng_seed = int(seed_hex, 16)
        rng = random.Random(rng_seed)
        self._emit("sandbox.lane", lane=self.lane, event_id=self._event_id, rng_seed=rng_seed)

        # ----- rescue cache -----
        try:
            pal_inp = np.unique(inp).astype(int)
            pal_sig = tuple(sorted(pal_inp.tolist())[:6] + [int(len(np.unique(out)))])
        except Exception:
            pal_sig = ()
        start_key = (tuple(inp.shape), tuple(out.shape), pal_sig)
        if start_key in self._rescue_cache:
            return self._rescue_cache[start_key]

        # ----- compression-scaled exploration budget (via helper) -----
        _bp = self._budget_params(beam_width=int(beam_width), md=int(md_base))
        beam_eff = int(_bp["beam_width_eff"])
        md_eff   = int(_bp["md_eff"])
        explore_cap = int(_bp["explore_cap"])
        omega_init_str = str(_bp["omega"])
        budget_alloc = int(beam_eff * md_eff)
        budget_used = 0
        self._last_budget_alloc = budget_alloc
        self._last_budget_used = 0

        # budget telemetry (init)
        try:
            self._emit("sandbox.budget_init",
                       beam_width=int(beam_width), md=int(md_base),
                       beam_width_eff=int(beam_eff), md_eff=int(md_eff),
                       omega=omega_init_str, m=float(_bp["m"]),
                       budget_allocated=int(budget_alloc))
        except Exception:
            pass

        # search state
        beam = [([], inp, 0.0, [])]
        best_chain, best_pred, best_score, best_details = None, None, -1e9, []
        prev_best_score = -1e9

        for depth in range(1, md_eff + 1):
            # single Kairos step
            try:
                if self.kairos is not None:
                    self.kairos.step(time_step=(self._event_id * 1000 + depth))
                    self._depth_flux = float(abs(getattr(self.kairos, "last_entropy_flux", 0.0)))
                    self.kairos_flux_history.append(float(self._depth_flux))
            except Exception:
                self._depth_flux = 0.0

            omega = self._omega_bucket(self._depth_flux)
            dials = self._omega_dials(omega, beam_eff)

            candidates = []
            for chain, grid, chain_score, deltas in beam:
                neigh = self._neighbor_ops(inp, grid, out, allow_target)
                try:
                    if self.curiosity is not None and hasattr(self.curiosity, "suggest_ops"):
                        extra = self.curiosity.suggest_ops(inp=inp, cur=grid, out=out, depth=depth, beam=len(beam))
                        if isinstance(extra, list): neigh.extend(extra)
                except Exception: pass
                if SANDBOX_STOCH_EPS > 0: rng.shuffle(neigh)

                for op in neigh:
                    name = op[0]
                    nxt = _apply_single(grid, op)
                    # Ω-aware fast filter
                    try:
                        if not _geom_phys_filter(grid, nxt, out,
                                                 epi_gain_min=float(dials["epi_gain_min"]),
                                                 mass_tol=float(dials["mass_tol"]),
                                                 centroid_tol=float(dials["centroid_tol"])):
                            continue
                    except Exception:
                        pass
                    if np.array_equal(nxt, grid):  # skip no-op
                        continue
                    cand_chain = _canonicalize_ops(chain + [op])
                    step, det = self._score_step(grid, nxt, out, name)
                    total = chain_score + step
                    cand_deltas = deltas + [{"dH": abs(_entropy(nxt) - _entropy(grid)), "epi": _epi(grid, nxt),
                                              "binder": _binder_like(nxt),
                                              "kairos_flux": det.get("kairos_flux", 0.0),
                                              "keel_ratio": det.get("keel_ratio", 1.0)}]
                    candidates.append((cand_chain, nxt, total, cand_deltas))
                    budget_used += 1  # count scored candidate as budget usage

            self._last_budget_used = budget_used
            # budget pulse
            try:
                keel_avg = float(np.mean(self.keel_ratio_history)) if self.keel_ratio_history else 1.0
            except Exception:
                keel_avg = 1.0
            try:
                self._emit("sandbox.budget", depth=int(depth), beam_eff=int(beam_eff), md_eff=int(md_eff),
                           keel_avg=float(keel_avg), m=float(_bp["m"]),
                           budget_alloc=int(budget_alloc), budget_used=int(budget_used), omega=int(omega))
            except Exception:
                pass

            # Ultra-guided diversification
            try:
                if self.ultra and hasattr(self.ultra, "suggest_ops"):
                    candidates = self.ultra.suggest_ops(candidates=candidates, k=max(beam_eff, 1))
            except Exception:
                pass

            # Hybrid re-ranking
            try:
                if self.sim is not None and hasattr(self.sim, "rerank"):
                    candidates = self.sim.rerank(candidates, k=max(beam_eff, 1))
            except Exception:
                pass

            # Sort + Ω-aware explore slots
            candidates.sort(key=lambda t: -t[2])
            explore = int(max(1, min(explore_cap, dials.get("explore", max(1, beam_eff // 4)))))
            main = candidates[:max(0, beam_eff - explore)]
            tails = candidates[max(0, beam_eff - explore):]
            rng.shuffle(tails)
            beam = (main + tails[:min(explore, max(0, len(tails)))])[:beam_eff]

            for c, g, s, d in beam:
                if s > best_score:
                    best_chain, best_pred, best_score, best_details = c, g, s, d

            try:
                self._emit("sandbox.depth_stats",
                           depth=int(depth),
                           cand=int(len(candidates)),
                           kept=int(len(beam)),
                           best_score=float(best_score),
                           omega=int(omega),
                           explore=int(explore),
                           budget_used=int(budget_used), budget_allocated=int(budget_alloc))
            except Exception:
                pass

            # stagnation → curiosity burst
            if depth > 1 and best_score < (prev_best_score + 1e-6):
                try:
                    if self.curiosity and hasattr(self.curiosity, "burst"):
                        burst = self.curiosity.burst(inp, out, beam)
                        if isinstance(burst, list):
                            beam.extend(burst[:max(1, beam_eff // 3)])
                except Exception:
                    pass
            prev_best_score = best_score

        if best_chain is None:
            self._emit("sandbox.no_chain", task_id=task_id)
            return None

        self.learned_chains.append(best_chain)
        self._rescue_cache[start_key] = best_chain

        sim_final  = _epi(best_pred, out)
        inv_final  = _inv_composite(inp, best_pred)
        phys_final = _physics_plausibility(inp, best_pred)
        narr       = _narrate_chain(best_chain, best_details)
        gold_shape_used = any(
            (op[0] in ("pad_to", "crop_to", "mass_pad", "entropy_crop") and tuple(op[1].get("shape", ())) == tuple(out.shape))
            for op in best_chain if isinstance(op, tuple) and len(op) > 1 and isinstance(op[1], dict))
        gold_shape_used = bool(gold_shape_used) and allow_target

        kairos_flux_final, keel_ratio_final = float(self._depth_flux), 1.0
        try:
            r1 = self._keel_ratio_cached(best_pred); r2 = self._keel_ratio_cached(out)
            keel_ratio_final = (r1 + r2) / 2.0
            self.keel_ratio_history.append(float(keel_ratio_final))
        except Exception:
            pass

        # Visual diff
        try:
            if SANDBOX_DIFF_VISUALS:
                tid = str(task_id) if task_id is not None else "na"
                used = self._visuals_used.get(tid, 0)
                if used < int(SANDBOX_VISUAL_MAX_PER_TASK):
                    ts = int(time.time())
                    path = os.path.join(self._pathmap["diff_dir"], f"diff_{tid}_{ts}.png")
                    _visual_diff_heatmap(best_pred, out, path)
                    self._visuals_used[tid] = used + 1
        except Exception:
            pass

        meta = {"discovery": "sandbox", "score": float(best_score), "sim_final": float(sim_final),
                "inv_final": float(inv_final), "phys_final": float(phys_final),
                "depth": int(len(best_chain)), "task_id": task_id, "ts": float(time.time()),
                "gold_shape_used": bool(gold_shape_used),
                "phase": os.environ.get("KAGGLE_PHASE", "train"),
                "binder_like": float(_binder_like(best_pred)),
                "epi_persistence": float(_entropy_persistence_index(inp, out)),
                "kairos_flux": float(kairos_flux_final), "keel_ratio": float(keel_ratio_final),
                "beam_width_eff": int(beam_eff), "md_eff": int(md_eff),
                "budget_used": int(budget_used), "budget_allocated": int(budget_alloc),
                "lane": self.lane}

        # ---- commit path remains single source of truth ----
        # KB writes are additive/diagnostic; commit_xform stays the atomic persistent write.
        try:
            if self.kb is not None:
                self.kb.remember_xform(inp, out, best_chain, confidence=max(0.55, min(0.98, sim_final)))
                if hasattr(self.kb, "narrations"):
                    self.kb.narrations.append(f"[Sandbox] Discovered chain (score={best_score:.3f} / EPI={sim_final:.3f} / INV={inv_final:.3f} / PHYS={phys_final:.3f}) :: {narr}")
                if hasattr(self.kb, "add_fact"):
                    self.kb.add_fact("sandbox.discovery", json.dumps({"task_id": task_id, **meta}))
        except Exception as e:
            self._emit("sandbox.kb_write_failed", error=str(e))
        try:
            rule = Rule("xform", {"ops": best_chain})
            rec = RuleRecord(input_grid=inp, output_grid=out, rule=rule, meta=meta)
            if (self.kb is not None and hasattr(self.kb, "rulebase") and hasattr(self.kb.rulebase, "add")):
                self.kb.rulebase.add(rec)
            if self.rulebase is not None and hasattr(self.rulebase, "_pulse"):
                self.rulebase._pulse("sandbox", meta)
        except Exception as e:
            self._emit("sandbox.rulebase_add_failed", error=str(e))

        # operator credit
        try:
            prev = inp
            for op in best_chain:
                cur = _apply_single(prev, op)
                gain = max(0.0, _epi(cur, out) - _epi(prev, out))
                if "_update_op_rank" in globals():
                    try: globals()["_update_op_rank"](op[0], reward=gain)
                    except Exception: pass
                if "_update_op_prior" in globals():
                    try: globals()["_update_op_prior"](op[0], cur, reward=gain)
                    except Exception: pass
                prev = cur
        except Exception:
            pass

        # ---- discovery CSV (scoped, append + rotate) ----
        try:
            p = self._pathmap["discover_csv"]
            os.makedirs(os.path.dirname(p), exist_ok=True)
            rotate = (os.path.getsize(p) if os.path.isfile(p) else 0) >= 2_000_000
            if rotate:
                import gzip, shutil
                with open(p, "rb") as fin, gzip.open(p + ".1.gz", "wb") as fout:
                    shutil.copyfileobj(fin, fout)
                tmp = p + ".tmp"
                with open(tmp, "w", encoding="utf-8") as f: f.write("")
                os.replace(tmp, p)
            exists = os.path.isfile(p) and os.path.getsize(p) > 0
            tmp = p + ".tmp"
            with open(tmp, "a", newline="", encoding="utf-8") as f:
                w = csv.writer(f)
                if not exists:
                    w.writerow(["run_id","task_id","lane","depth","score","sim_final","inv_final","phys_final",
                                "gold_shape_used","keel_ratio","kairos_flux","beam_width_eff","md_eff","budget_used","budget_allocated"])
                w.writerow([self.run_id, task_id, self.lane, int(len(best_chain)), float(best_score),
                            float(sim_final), float(inv_final), float(phys_final),
                            int(bool(gold_shape_used)), float(keel_ratio_final), float(kairos_flux_final),
                            int(beam_eff), int(md_eff), int(budget_used), int(budget_alloc)])
            os.replace(tmp, p)
        except Exception:
            pass

        # ---- near-miss dump (top-K rejected) ----
        try:
            K = 50  # reserved for future heavier capture
            near_path = self._pathmap["near_miss_jsonl"]
            os.makedirs(os.path.dirname(near_path), exist_ok=True)
            with open(near_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "t": time.time(), "run_id": self.run_id, "task_id": task_id, "lane": self.lane,
                    "accepted_chain": [(op[0], op[1]) for op in best_chain],
                    "score": float(best_score), "depth": int(len(best_chain)),
                    "sim_final": float(sim_final),
                    "inv_final": float(inv_final),
                    "phys_final": float(phys_final),
                    "beam_eff": int(beam_eff), "md_eff": int(md_eff),
                    "budget_used": int(budget_used), "budget_alloc": int(budget_alloc)
                }) + "\n")
        except Exception:
            pass

        # discovery emit
        self._emit("sandbox.discovery",
                   task_id=task_id, score=float(best_score), chain_len=int(len(best_chain)),
                   gold_shape_used=bool(gold_shape_used), sim_final=float(sim_final),
                   inv_final=float(inv_final), phys_final=float(phys_final),
                   kairos_flux=float(kairos_flux_final), keel_ratio=float(keel_ratio_final),
                   beam_eff=int(beam_eff), md_eff=int(md_eff),
                   budget_used=int(budget_used), budget_alloc=int(budget_alloc),
                   chain=[(op[0], op[1]) for op in best_chain])

        # append discover row (lane-scoped CSV)
        try:
            self._append_discover_row({
                "task_id": task_id,
                "beam_width": int(beam_width), "beam_width_eff": int(beam_eff),
                "md": int(md_base), "md_eff": int(md_eff),
                "omega": self._omega_state(), "m": float(_bp["m"]),
                "budget_allocated": int(budget_alloc), "budget_used": int(budget_used),
                "score": float(best_score), "sim_final": float(sim_final),
                "inv_final": float(inv_final), "phys_final": float(phys_final),
                "gold_shape_used": bool(gold_shape_used),
                "keel_ratio": float(keel_ratio_final), "kairos_flux": float(kairos_flux_final),
                "chain_len": int(len(best_chain)),
            })
            self._emit("sandbox.budget_finalize",
                       task_id=task_id, budget_used=int(budget_used), budget_allocated=int(budget_alloc),
                       omega=self._omega_state(), m=float(_bp["m"]))
        except Exception:
            pass

        if self.ultra is not None:
            try: self.ultra.observe("sandbox_discovery", **meta)
            except Exception: pass
        if self.holo is not None:
            try: self.holo.add(inp, out, meta)  # strict arrays only
            except Exception: pass
        if self.curiosity is not None and hasattr(self.curiosity, "ingest_sandbox_result"):
            try: self.curiosity.ingest_sandbox_result(inp, out, meta)
            except Exception: pass

        # rollup row with budget stats
        self._rollup_rows.append({
            "task_id": task_id, "depth": int(len(best_chain)), "score": float(best_score),
            "sim_final": float(sim_final), "inv_final": float(inv_final), "phys_final": float(phys_final),
            "gold_shape_used": int(bool(gold_shape_used)), "ts": float(time.time()),
            "budget_used": int(budget_used), "budget_alloc": int(budget_alloc)
        })

        # atomic commit is here (single source of truth)
        if "solver" in globals() and (globals().get("solver") is not None):
            try:
                commit_xform(inp, out, best_chain, globals()["solver"],
                             meta_extra={"discovery": "sandbox", "task_id": task_id, "score": float(best_score),
                                         "sim_final": float(sim_final), "inv_final": float(inv_final),
                                         "phys_final": float(phys_final),
                                         "entropy_slope": float(best_details[-1].get("dH", 0.0) if best_details else 0.0),
                                         "gold_shape_used": bool(gold_shape_used),
                                         "phase": os.environ.get("KAGGLE_PHASE", "train"),
                                         "kairos_flux": float(kairos_flux_final), "keel_ratio": float(keel_ratio_final)})
            except Exception as e:
                try: meta_log("sandbox.commit_fail", task_id=task_id, error=str(e))
                except Exception: pass

        try:
            if hasattr(self, "ml"):
                self.ml.ingest_sandbox_outcome(kind="discover_chain", success=True,
                                               inp=inp, out=out, chain=best_chain,
                                               score=float(best_score), task_id=task_id)
        except Exception:
            pass
        return best_chain

    # -----------------------------
    # Helper: allow target shape?
    # -----------------------------
    def _allow_target_shape_use(self) -> bool:
        try:
            v = os.environ.get("SANDBOX_ALLOW_TARGET_SHAPE", "1").strip().lower()
            return v not in ("0", "false", "no")
        except Exception:
            return True

    # -----------------------------
    # Rollup export (now with budget stats)
    # -----------------------------
    def finalize_exports(self):
        try:
            p = self._pathmap["rollup_csv"]
            os.makedirs(os.path.dirname(p), exist_ok=True)
            if not self._rollup_rows:
                self._emit("sandbox.rollup.empty")
                return
            by_task = {}
            for r in self._rollup_rows:
                tid = r.get("task_id")
                cur = by_task.get(tid, {"task_id": tid, "discoveries": 0, "best_score": -1e9,
                                        "mean_score_sum": 0.0, "sim_sum": 0.0,
                                        "inv_sum": 0.0, "phys_sum": 0.0, "gold_uses": 0,
                                        "budget_used_sum": 0, "budget_alloc_sum": 0})
                cur["discoveries"] += 1
                cur["best_score"]   = max(cur["best_score"], float(r.get("score", -1e9)))
                cur["mean_score_sum"] += float(r.get("score", 0.0))
                cur["sim_sum"]   += float(r.get("sim_final", 0.0))
                cur["inv_sum"]   += float(r.get("inv_final", 0.0))
                cur["phys_sum"]  += float(r.get("phys_final", 0.0))
                cur["gold_uses"] += int(r.get("gold_shape_used", 0))
                cur["budget_used_sum"]  += int(r.get("budget_used", 0))
                cur["budget_alloc_sum"] += int(r.get("budget_alloc", 0))
                by_task[tid] = cur
            agg_rows = []
            for tid, cur in by_task.items():
                n = max(1, cur["discoveries"])
                mean_budget_used = cur["budget_used_sum"] / n
                mean_score = cur["mean_score_sum"] / n
                efficiency = float(mean_score / max(1.0, mean_budget_used))
                agg_rows.append({"task_id": tid, "discoveries": cur["discoveries"],
                                 "best_score": round(cur["best_score"], 6),
                                 "mean_score": round(mean_score, 6),
                                 "mean_sim":   round(cur["sim_sum"] / n, 6),
                                 "mean_inv":   round(cur["inv_sum"] / n, 6),
                                 "mean_phys":  round(cur["phys_sum"] / n, 6),
                                 "gold_shape_used_count": cur["gold_uses"],
                                 "mean_budget_used": round(mean_budget_used, 3),
                                 "efficiency_score_per_budget": round(efficiency, 6)})
            tmp_path = p + ".tmp"
            with open(tmp_path, "w", newline="", encoding="utf-8") as f:
                f.write("# schema:sandbox_rollup.v2 (with budget)\n")
                writer = csv.DictWriter(f, fieldnames=list(agg_rows[0].keys()))
                writer.writeheader()
                writer.writerows(agg_rows)
            os.replace(tmp_path, p)
            self._emit("sandbox.rollup.written", path=p, n=len(agg_rows))
            if self.meta is not None and hasattr(self.meta, "record_rollup_entry"):
                for row in agg_rows:
                    self.meta.record_rollup_entry(row.get("task_id"), row)
        except Exception as e:
            self._emit("sandbox.rollup.failed", error=str(e))

    # -----------------------------
    # Evaluator socket: replay near misses / accepted context
    # -----------------------------
    def replay_near_misses(self, task_id: Optional[str] = None, k: int = 50) -> List[Dict[str, Any]]:
        """
        Lightweight reader for the lane-scoped near_misses.jsonl.
        Returns up to k latest entries (optionally filtered by task_id) with accepted context.
        NOTE: For full near-miss candidate chains, extend capture during the search loop.
        """
        out: List[Dict[str, Any]] = []
        p = self._pathmap.get("near_miss_jsonl")
        try:
            if not p or not os.path.isfile(p):
                return out
            # Read tail-efficiently
            with open(p, "r", encoding="utf-8") as f:
                lines = f.readlines()[-max(1, k * 4):]  # over-read a bit, then filter
            for line in reversed(lines):
                try:
                    obj = json.loads(line.strip())
                    if (task_id is None) or (obj.get("task_id") == task_id):
                        out.append(obj)
                        if len(out) >= k:
                            break
                except Exception:
                    continue
            return list(reversed(out))
        except Exception:
            return out

    def __del__(self):
        try:
            self.finalize_exports()
        except Exception:
            pass


# -----------------------------------------------------------
# Visuals (helper)
# -----------------------------------------------------------
def _visual_diff_heatmap(a: np.ndarray, b: np.ndarray, path: str):
    if not SANDBOX_DIFF_VISUALS:
        return
    try:
        try:
            import matplotlib
            try: matplotlib.use("Agg")
            except Exception: pass
        except Exception:
            matplotlib = None
        _plt = None
        if matplotlib is not None:
            try:
                import matplotlib.pyplot as plt
                _plt = plt
            except Exception:
                _plt = None
        if _plt is None:
            return
        A, B = pad_to_same_shape(a, b, pad_val=-1)
        diff = (A != B).astype(int)
        _plt.figure(figsize=(3.2, 3.2))
        _plt.imshow(diff, interpolation="nearest")
        _plt.axis("off")
        _plt.tight_layout(pad=0.1)
        os.makedirs(os.path.dirname(path), exist_ok=True)
        _plt.savefig(path, dpi=140)
        _plt.close()
        try: meta_log("visual.diff", path=path)
        except Exception: pass
    except Exception:
        pass

# -----------------------------------------------------------
# Narration helpers
# -----------------------------------------------------------
def _op_to_english(op: Tuple[str, Dict[str, Any]]) -> str:
    name, kw = op[0], (op[1] if len(op) > 1 else {})
    if name == "rot":          return f"rotate {int(kw.get('k',1))*90}°"
    if name == "flip_lr":      return "mirror horizontally"
    if name == "flip_ud":      return "mirror vertically"
    if name == "flip_diag":    return "diagonal flip" + (" (anti)" if kw.get("anti") else "")
    if name == "trans":        return f"translate (dr={kw.get('dr',0)}, dc={kw.get('dc',0)})"
    if name == "pad_to":       return f"pad to {tuple(kw.get('shape',('?','?')))}"
    if name == "crop_to":      return f"crop to {tuple(kw.get('shape',('?','?')))}"
    if name == "mass_pad":     return f"mass-aware pad to {tuple(kw.get('shape',('?','?')))}"
    if name == "entropy_crop": return f"entropy-balanced crop to {tuple(kw.get('shape',('?','?')))}"
    if name == "recolor":      return "recolor by learned mapping"
    if name == "hist_recolor": return "histogram recolor"
    if name == "symmetry":
        ax = kw.get("axis","v")
        if ax == "vh": return "enforce vertical+horizontal symmetry"
        return f"enforce {ax}-axis symmetry"
    return name

def _narrate_chain(chain: List[Tuple[str, Dict[str, Any]]], deltas: List[Dict[str, float]]) -> str:
    parts = []
    for i, op in enumerate(chain):
        frag = _op_to_english(op)
        if i < len(deltas):
            d = deltas[i]
            frag += f" (ΔH={d.get('dH',0):+.3f}, EPI={d.get('epi',0):.3f}, Binder={d.get('binder',0):+.3f})"
        parts.append(frag)
    return " → ".join(parts)

# -----------------------------------------------------------
# Op executor shim (helper)
# -----------------------------------------------------------
def _apply_single(arr: np.ndarray, op: Tuple[str, Dict[str, Any]]) -> np.ndarray:
    name, params = op[0], (op[1] if len(op) > 1 else {})
    a = _ensure_int_ndarray(arr)
    try:
        if name == "rot":
            return np.rot90(a, k=int(params.get("k", 1)) % 4)
        if name == "flip_lr":
            return np.fliplr(a)
        if name == "flip_ud":
            return np.flipud(a)
        if name == "flip_diag":
            return (np.fliplr(a).T) if params.get("anti", False) else a.T
        if name == "trans":
            dr, dc = int(params.get("dr",0)), int(params.get("dc",0))
            pad = int(params.get("pad_val",0))
            R, C = a.shape
            out = np.full((R, C), pad, dtype=int)
            r0 = max(0, dr); c0 = max(0, dc)
            r1 = min(R, R+dr); c1 = min(C, C+dc)
            sr0 = max(0, -dr); sc0 = max(0, -dc)
            sr1 = sr0 + (r1 - r0); sc1 = sc0 + (c1 - c0)
            if r1>r0 and c1>c0: out[r0:r1, c0:c1] = a[sr0:sr1, sc0:sc1]
            return out
        if name == "recolor":
            mp = dict(params.get("map", {}))
            if all(mp.get(int(v), int(v)) == int(v) for v in np.unique(a) if int(v) != -1):
                return a
            vec = np.vectorize(lambda v: int(v) if int(v) == -1 else mp.get(int(v), int(v)))
            return vec(a).astype(int)
        if name == "pad_to":
            shape = tuple(params.get("shape", a.shape))
            pad = int(params.get("pad_val", 0))
            R, C = shape
            out = np.full((R, C), pad, dtype=int)
            rs = min(R, a.shape[0]); cs = min(C, a.shape[1])
            out[:rs, :cs] = a[:rs, :cs]
            return out
        if name == "crop_to":
            shape = tuple(params.get("shape", a.shape))
            return a[:shape[0], :shape[1]]
        if name == "mass_pad":
            return _mass_pad(a, tuple(params.get("shape", a.shape)), int(params.get("pad_val",0)))
        if name == "entropy_crop":
            return _entropy_crop(a, tuple(params.get("shape", a.shape)))
        if name == "hist_recolor":
            flat = a.flatten()
            vals, cnts = np.unique(flat, return_counts=True)
            order = np.argsort(-cnts)
            mapping = {int(vals[i]): int(idx+1) for idx, i in enumerate(order)}
            if all(mapping.get(int(v), int(v)) == int(v) for v in np.unique(a) if int(v) != -1):
                return a
            vec = np.vectorize(lambda v: int(v) if int(v) == -1 else mapping.get(int(v), int(v)))
            return vec(a).astype(int)
        if name == "symmetry":
            axis = params.get("axis","v")
            arr = a.copy()
            R, C = arr.shape
            if "v" in axis:
                for j in range(C//2):
                    arr[:, C-1-j] = arr[:, j]
            if "h" in axis:
                for i in range(R//2):
                    arr[R-1-i, :] = arr[i, :]
            return arr
    except Exception:
        return a
    return a

def sandbox_apply_ops(arr: np.ndarray, ops: List[Tuple[str, Dict[str, Any]]]) -> np.ndarray:
    x = _ensure_int_ndarray(arr)
    for op in (ops or []):
        x = _apply_single(x, op)
    return _sanitize_grid(x)

# ==========================================================
# (3) Sandbox/solver loop integration (recall-first, add-on-success)
# ==========================================================
def solve_or_search(current_grid: np.ndarray, evaluate_fn, accept_fn, subject: str = "solver"):
    try:
        h = globals().get("holo", None)
        get_fn = getattr(h, "get", None); add_fn = getattr(h, "add", None)
        if callable(get_fn):
            hits = get_fn(current_grid, topk=1)
            if hits:
                pred, meta_hit, dist = hits[0]
                try: meta_log("holo.recall_hit", dist=dist, conf=float(meta_hit.get("confidence", 0.0)),
                              subject=subject, shape=tuple(current_grid.shape))
                except Exception: pass
                ok, score, gold = evaluate_fn(pred)
                if ok:
                    accept_fn(pred)
                    try:
                        if callable(add_fn):
                            add_fn(current_grid, pred if gold is None else gold,
                                   {"subject": subject, "confidence": max(0.85, float(meta_hit.get("confidence", 0.6))),
                                    "rule_kind": "recall_success"})
                    except Exception as e:
                        try: meta_log("holo.add_fail", site="solver.recall_success", error=str(e))
                        except Exception: pass
                    try:
                        u = globals().get("ultra", None)
                        if u and hasattr(u, "observe"):
                            u.observe("solver_recall_success", subject=subject, score=float(score))
                    except Exception: pass
                    return pred, score
    except Exception as e:
        try: meta_log("holo.recall_fail", error=str(e), subject=subject)
        except Exception: pass
        raise

    best = None; best_score = -1e9
    try:
        cand = generate_candidates(current_grid)
    except Exception as e:
        try: meta_log("solver.generate_candidates_error", error=str(e))
        except Exception: pass
        cand = []

    for pred in cand:
        ok, score, gold = evaluate_fn(pred)
        if score > best_score:
            best, best_score = pred, score
        if ok:
            accept_fn(pred)
            try:
                h = globals().get("holo", None)
                add_fn = getattr(h, "add", None)
                if callable(add_fn):
                    add_fn(current_grid, pred if gold is None else gold,
                           {"subject": subject, "confidence": 0.9, "rule_kind": "search_success"})
            except Exception as e:
                try: meta_log("holo.add_fail", site="solver.search_success", error=str(e))
                except Exception: pass
                raise
            try:
                u = globals().get("ultra", None)
                if u and hasattr(u, "observe"):
                    u.observe("solver_search_success", subject=subject, score=float(score))
            except Exception: pass
            return pred, score

    try:
        u = globals().get("ultra", None)
        if u and hasattr(u, "observe"):
            u.observe("solver_search_miss", subject=subject, score=float(best_score))
    except Exception: pass
    return best, best_score

# ==========================================================
# RHCM vs Classic: Parallel Head-to-Head Harness
# ==========================================================
def _apply_rhcm_bias(explorer: "SandboxExplorer"):
    try:
        w = dict(explorer.weights)
        w.update({k: _RHCM_BIAS[k] for k in _RHCM_BIAS})
        s = sum(w.values())
        if s > 0:
            for k in w:
                w[k] = w[k] / s
        explorer.weights = w
        if hasattr(explorer, "_emit"):
            explorer._emit("sandbox.rhcm_bias_applied", weights=w, lane=getattr(explorer, "lane", None))
    except Exception:
        pass

def _geom_phys_filter(prev: np.ndarray, nxt: np.ndarray, out: np.ndarray,
                      epi_gain_min: float = 0.0,
                      mass_tol: float = 0.10,
                      centroid_tol: float = 0.35) -> bool:
    try:
        gain = float(_epi(nxt, out) - _epi(prev, out))
        if gain < float(epi_gain_min): return False
    except Exception: pass
    try:
        m_prev = float(np.sum(prev != -1))
        m_nxt  = float(np.sum(nxt  != -1))
        m_out  = float(np.sum(out  != -1))
        if m_out > 0:
            if abs(m_nxt - m_out) / max(1.0, m_out) > float(mass_tol):
                return False
    except Exception: pass
    try:
        def _centroid(a: np.ndarray):
            ys, xs = np.where(a != -1)
            if len(ys) == 0: return (0.0, 0.0)
            return (float(np.mean(ys)), float(np.mean(xs)))
        cy, cx = _centroid(nxt); oy, ox = _centroid(out)
        R, C = out.shape
        dist = ((cy - oy)**2 + (cx - ox)**2)**0.5 / max(1.0, (R**2 + C**2)**0.5)
        if dist > float(centroid_tol):
            return False
    except Exception: pass
    return True

def _eval_chain(explorer_ctor_args: dict, use_rhcm: bool, inp: np.ndarray, out: np.ndarray,
                beam_width: int, max_depth: int, task_id: Optional[str]) -> dict:
    try:
        ex = SandboxExplorer(**explorer_ctor_args)
        ex.lane = "RHCM" if use_rhcm else "CLASSIC"
        ex._pathmap = ex._paths(ex.lane)
        if use_rhcm: _apply_rhcm_bias(ex)
        if hasattr(ex, "_emit"): ex._emit("sandbox.lane", lane=ex.lane)
        chain = ex.discover_chain(inp=inp, out=out, beam_width=beam_width, max_depth=max_depth, task_id=task_id)
        if chain is None:
            return {"ok": False, "lane": ex.lane, "score": -1e9, "sim": 0.0, "chain": None}
        pred = sandbox_apply_ops(inp, chain)
        sim  = float(_epi(pred, out))
        return {"ok": True, "lane": ex.lane, "score": float(sim), "sim": float(sim), "chain": chain, "pred": pred}
    except Exception as e:
        try: meta_log("sandbox.parallel_eval_error", lane=("RHCM" if use_rhcm else "CLASSIC"), error=str(e))
        except Exception: pass
        return {"ok": False, "lane": ("RHCM" if use_rhcm else "CLASSIC"), "score": -1e9, "sim": 0.0, "chain": None}

def sandbox_head_to_head(inp: np.ndarray, out: np.ndarray,
                         explorer_ctor_args: Optional[dict] = None,
                         beam_width: int = 8,
                         max_depth: Optional[int] = None,
                         task_id: Optional[str] = None,
                         parallel: str = "thread",
                         timeout_sec: float = 45.0) -> dict:
    explorer_ctor_args = dict(explorer_ctor_args or {})
    md = int(max_depth or 3)

    # process mode foot-gun guard
    if parallel == "process":
        non_picklables = any(explorer_ctor_args.get(k) is not None for k in ("kb", "meta", "holo", "ultra", "rulebase", "curiosity", "sim"))
        if non_picklables:
            try: meta_log("sandbox.h2h_downgrade", reason="non_picklables", mode_from="process", mode_to="thread")
            except Exception: pass
            parallel = "thread"

    Executor = ThreadPoolExecutor if parallel == "thread" else ProcessPoolExecutor
    out_bundle = {"winner": None, "results": {}}

    with Executor(max_workers=2) as pool:
        fut_rhcm = pool.submit(_eval_chain, explorer_ctor_args, True,  inp, out, beam_width, md, task_id)
        fut_base = pool.submit(_eval_chain, explorer_ctor_args, False, inp, out, beam_width, md, task_id)
        try:
            for fut in as_completed([fut_rhcm, fut_base], timeout=timeout_sec):
                res = fut.result()
                out_bundle["results"][res["lane"]] = res
        except TimeoutError:
            for f in (fut_rhcm, fut_base):
                try: f.cancel()
                except Exception: pass

    r = out_bundle["results"].get("RHCM")
    c = out_bundle["results"].get("CLASSIC")
    if r is None and c is None:
        return {"winner": None, "results": out_bundle.get("results", {})}

    def _score_key(x):
        if not x or not x.get("ok"):
            return (-1e9, +999, -1e9)
        pred = x.get("pred", None)
        phys = float(_physics_plausibility(inp, pred)) if pred is not None else 0.0
        return (float(x.get("score", -1e9)), -len(x.get("chain") or []), float(phys))

    wr, wc = _score_key(r), _score_key(c)
    winner = "RHCM" if wr > wc else "CLASSIC"
    out_bundle["winner"] = winner

    try:
        meta_log("sandbox.head_to_head_complete",
                 winner=winner,
                 r_score=float(wr[0]), r_len=int(len((r or {}).get("chain") or [])),
                 c_score=float(wc[0]), c_len=int(len((c or {}).get("chain") or [])),
                 task_id=task_id)
    except Exception: pass
    try:
        u = globals().get("ultra", None)
        if u and hasattr(u, "observe"):
            u.observe("head_to_head", winner=winner,
                      r_ok=bool((r or {}).get("ok", False)), c_ok=bool((c or {}).get("ok", False)))
    except Exception: pass
    return out_bundle

def solve_with_head_to_head(current_grid: np.ndarray, gold: Optional[np.ndarray],
                            explorer_ctor_args: dict,
                            evaluate_fn, accept_fn,
                            beam_width: int = 8,
                            max_depth: Optional[int] = None,
                            subject: str = "solver", parallel: str = "thread"):
    out = gold if gold is not None else current_grid
    pack = sandbox_head_to_head(current_grid, out,
                                explorer_ctor_args=explorer_ctor_args,
                                beam_width=beam_width, max_depth=max_depth,
                                task_id=getattr(globals().get("solver", None), "current_task_id", None),
                                parallel=parallel)
    win = pack.get("results", {}).get(pack.get("winner"))
    if win and win.get("ok") and win.get("pred") is not None:
        ok, score, _ = evaluate_fn(win["pred"])
        if ok:
            accept_fn(win["pred"])
            try: meta_log("solver.head_to_head_accept", lane=pack["winner"], score=float(score))
            except Exception: pass
            return win["pred"], score
    return None, -1e9

# ============================================================
# Symbolic ML Controller Helpers (hardened)
# ============================================================

def _require(name: str):
    if name not in globals():
        raise RuntimeError(f"[monolith] required global '{name}' not found")
    return globals()[name]

def LOG():
    return _require("logger")

def META():
    return _require("meta_log")

def HAS_PHYS() -> bool:
    # "compute_invariants" is your physics/invariants scorer in this codebase
    return "compute_invariants" in globals() or "InvariantScorer" in globals()

def PHYS():
    # Prefer InvariantScorer class if available, else function compute_invariants
    if "InvariantScorer" in globals():
        try:
            return InvariantScorer(logger=globals().get("EXPLAIN", None))  # type: ignore
        except Exception:
            pass
    return _require("compute_invariants")


def HAS_KAIROS() -> bool:
    return "kairos" in globals()

def KAIROS():
    return _require("kairos")

def HAS_KEEL() -> bool:
    return ("keel" in globals()) or ("kairos" in globals()) or ("CompressionKEEL" in globals())

def _keel_record(rin: float = None, rout: float = None, note: str = ""):
    try:
        keel = globals().get("keel", None)
        if keel and hasattr(keel, "record"):
            keel.record(rin=rin, rout=rout, note=note)
            return
        # Fallback: kairos may expose keel-like updates
        if HAS_KAIROS():
            k = KAIROS()
            if hasattr(k, "update_keel_ratio"):
                k.update_keel_ratio(rin=rin, rout=rout)
            elif hasattr(k, "step"):
                k.step(int(getattr(k, "phase_time", 0)) + 1)
        META()("keel.record", rin=rin, rout=rout, note=note)
    except Exception:
        pass


# ============================================================
# SymbolicMLController (rewritten & tightened)
# ============================================================
class SymbolicMLController:  
   
    def __init__(self,
                 meta: Optional[Any] = None,
                 holo: Optional[Any] = None,
                 sandbox: Optional[Any] = None,
                 kb: Optional[Any] = None,
                 rulebase: Optional[Any] = None,
                 curiosity: Optional[Any] = None,
                 kairos_obj: Optional[Any] = None,
                 keel: Optional[Any] = None,
                 enable_threads: bool = True,
                 persistent_state_path: str = "deployment/symbolic_ml_state.json",
                 training_max_queue: int = 4096,
                 feedback_max_queue: int = 8192,
                 health_period_sec: float = 5.0,
                 dry_run: bool = False,
                 succ_strip_every_sec: float = 60.0):

        # External graph refs (may be extended/overridden via attach())
        self.meta = meta
        self.holo = holo
        self.sandbox = sandbox
        self.kb = kb
        self.rulebase = rulebase
        self.curiosity = curiosity
        self.kairos = kairos_obj or (KAIROS() if HAS_KAIROS() else None)
        self.keel = keel if keel is not None else globals().get("keel", None)

        # Encoder
        try:
            self.encoder = AdaptiveSymbolicEncoder(meta=self.meta, on_classify=None, on_evolve=None, on_prune=None)  # type: ignore
        except Exception:
            # minimal stub to avoid hard failure if encoder missing
            class _EncStub:
                def __init__(self): self.archetypes, self.stats = {}, {}
                def record_feedback(self, **_): pass
                def summarize(self): return {"n_archetypes": len(self.archetypes)}
            self.encoder = _EncStub()

        # Queues / state
        self.training_queue: Deque[Dict[str, Any]] = deque(maxlen=int(training_max_queue))
        self.feedback_queue: Deque[Dict[str, Any]] = deque(maxlen=int(feedback_max_queue))
        self._replay_deferred: Deque[Dict[str, Any]] = deque(maxlen=2048)

        self._rolling_success: Deque[float] = deque(maxlen=1024)
        self._loop_durations: Deque[float] = deque(maxlen=128)
        self._training_batches_seen: int = 0
        self._label_stats: Dict[str, Dict[str, float]] = {}
        self._failure_labels: Deque[str] = deque(maxlen=128)
        self._replay_spacing_map: Dict[str, int] = {}
        self._priority_scale: Dict[str, float] = {}
        self._comp_ratio_ema: Optional[float] = None

        # Backoff + cadence
        self._feedback_error_streak: int = 0
        self._feedback_backoff_cycles: int = 0
        self._succ_strip_last_emit: float = 0.0
        self._succ_strip_period: float = float(max(5.0, succ_strip_every_sec))
        self._dry_run: bool = bool(dry_run)
        self._health_period = float(health_period_sec)
              
        # Pacing telemetry cadence (rate-limit noisy events)
        self._last_pacing_emit: float = 0.0
        self._pacing_min_interval: float = 0.75  # seconds between pacing snapshots


        # Threads
        self._stop_flag = False
        self._th_training = None
        self._th_feedback = None
        self._th_health = None

        # Analytics mirrors (rolling)
        self._hybrid_roll: Deque[float] = deque(maxlen=256)
        self._kairos_flux_roll: Deque[float] = deque(maxlen=256)

        # Persistence path
        self._state_path = persistent_state_path
        os.makedirs(os.path.dirname(self._state_path) or ".", exist_ok=True)

        # Load previous state
        self.load_state(self._state_path)

        # One-shot diagnostics of bus graph
        try:
            self._emit("symbolic.bus_lock",
                       metas=bool(self.meta), kairos=bool(self.kairos), holo=bool(self.holo),
                       kb=bool(self.kb), rulebase=bool(self.rulebase))
        except Exception:
            pass

        # Deterministic dry-run harness
        if self._dry_run:
            try:
                import numpy as _np, random as _r
                _r.seed(0); _np.random.seed(0)  # type: ignore
                META()("ml.dry_run_enabled")
            except Exception:
                pass

        # Threads
        if enable_threads and not self._dry_run:
            self.start()

        # Rhythm + keel taps
        try:
            if self.kairos and hasattr(self.kairos, "step"):
                self.kairos.step(0)
                META()("kairos.attach.ml", controller="SymbolicML")
        except Exception:
            pass
        try:
            if self.holo and hasattr(self.holo, "compression_ratio"):
                _keel_record(rin=self.holo.compression_ratio, rout=self.holo.compression_ratio, note="ml.init")
        except Exception:
            pass

        # Live heartbeat (pulsed)
        try:
            if "start_live_meta_heartbeat" in globals():
                start_live_meta_heartbeat(self,
                    meta_path="explanations.jsonl",
                    state_path=self._state_path,
                    window=200,
                    daemon=True
                )
        except Exception:
            try: META()("ml.heartbeat_init_failed")
            except Exception: pass

        # Explanation/physics hooks
        try:
            if "install_explanation_hooks" in globals():
                install_explanation_hooks(
                    encoder=self.encoder,
                    meta=self.meta,
                    solver=globals().get("solver", None),
                    sandbox=self.sandbox
                )
        except Exception:
            pass

        self._emit("symbolic.init",
                   state_path=self._state_path,
                   queues={"training": len(self.training_queue), "feedback": len(self.feedback_queue)})
            
        def _emit_pacing(self, site: str, extra: Optional[Dict[str, Any]] = None):
           
            now = time.time()
            if (now - self._last_pacing_emit) < self._pacing_min_interval:
                return
            self._last_pacing_emit = now

            try:
                # Core queues / cadence
                q_train = len(self.training_queue)
                q_feed  = len(self.feedback_queue)
                batches = int(self._training_batches_seen)

                # Rolling performance + dynamics
                win = list(self._rolling_success)
                rolling_success = float(np.mean(win)) if win else 0.0
                flux = self._kairos_flux()
                flux_mean = (float(np.mean(self._kairos_flux_roll)) if self._kairos_flux_roll else None)
                hybrid_mean = (float(np.mean(self._hybrid_roll)) if self._hybrid_roll else None)

                # Compression / perceptual taps
                comp = float(getattr(self.holo, "compression_ratio", 1.0)) if self.holo is not None else None
                psnr = float(getattr(self.holo, "psnr", getattr(self.keel, "psnr", 0.0)) or 0.0) if self.holo or self.keel else None
                ssim = float(getattr(self.holo, "ssim", getattr(self.keel, "ssim", 0.0)) or 0.0) if self.holo or self.keel else None

                # Priority map summary
                pmap = dict(self._priority_scale)
                top_pri = sorted(pmap.items(), key=lambda kv: -kv[1])[:3]
                # Entropy of the priority map (already computed elsewhere but keep local)
                pri_vals = list(pmap.values())
                if pri_vals:
                    p = np.array(pri_vals, dtype=float); p /= float(p.sum() or 1.0)
                    pri_entropy = float(-np.sum(p * np.log(p + 1e-12)))
                else:
                    pri_entropy = 0.0

                payload = {
                    "site": site,
                    "q_train": q_train,
                    "q_feedback": q_feed,
                    "batches_seen": batches,
                    "rolling_success": round(rolling_success, 6),
                    "kairos_flux": flux,
                    "kairos_flux_mean": flux_mean,
                    "hybrid_mean": hybrid_mean,
                    "compression_ratio": comp,
                    "psnr": psnr,
                    "ssim": ssim,
                    "priority_top": top_pri,
                    "priority_entropy": pri_entropy,
                }
                if extra:
                    payload.update(extra)

                self._emit("ml.training.pacing", **payload)
            except Exception:
                # Never block loops on telemetry
                pass


    # ---------- attach & capabilities ----------
    def attach(self,
               meta: Optional[Any] = None,
               holo: Optional[Any] = None,
               sandbox: Optional[Any] = None,
               kb: Optional[Any] = None,
               rulebase: Optional[Any] = None,
               curiosity: Optional[Any] = None,
               kairos: Optional[Any] = None,
               keel: Optional[Any] = None) -> None:
        """Bi-directional linking; safe to call multiple times."""
        if meta is not None:      self.meta = meta
        if holo is not None:      self.holo = holo
        if sandbox is not None:   self.sandbox = sandbox
        if kb is not None:        self.kb = kb
        if rulebase is not None:  self.rulebase = rulebase
        if curiosity is not None: self.curiosity = curiosity
        if kairos is not None:    self.kairos = kairos
        if keel is not None:      self.keel = keel
        # back-links (guarded)
        try:
            if self.meta is not None: self.meta.ml = self
        except Exception: pass
        try:
            if self.sandbox is not None: self.sandbox.ml = self
        except Exception: pass
        try:
            if self.kb is not None: self.kb.ml = self
        except Exception: pass
        # capability echo
        ok = {
            "meta": bool(self.meta), "holo": bool(self.holo), "sandbox": bool(self.sandbox),
            "kb": bool(self.kb), "rulebase": bool(self.rulebase),
            "curiosity": bool(self.curiosity), "kairos": bool(self.kairos), "keel": bool(self.keel)
        }
        self._emit("ml.attach_links", **ok)

    def capabilities(self) -> Dict[str, Any]:
        return {
            "feedback": True,
            "training": True,
            "drift_watch": True,
            "hybrid_analytics": True,
            "holo": bool(self.holo),
            "sandbox": bool(self.sandbox),
            "kb": bool(self.kb),
            "kairos": bool(self.kairos),
        }

    # ---------- lifecycle ----------
    def start(self, enable_threads: bool = True):
        if not enable_threads or self._dry_run:
            return
        if not self._th_training or not self._th_training.is_alive():
            self._th_training = threading.Thread(target=self._loop_training, name="ML-Training", daemon=True)
            self._th_training.start()
        if not self._th_feedback or not self._th_feedback.is_alive():
            self._th_feedback = threading.Thread(target=self._loop_feedback, name="ML-Feedback", daemon=True)
            self._th_feedback.start()
        if not self._th_health or not self._th_health.is_alive():
            self._th_health   = threading.Thread(target=self._loop_health,   name="ML-Health",   daemon=True)
            self._th_health.start()
        self._emit("symbolic.threads_started")

    def _join_threads(self, timeout: float = 2.0):
        for th in (self._th_training, self._th_feedback, self._th_health):
            try:
                if th and th.is_alive():
                    th.join(timeout=timeout)
            except Exception:
                pass

    def stop(self):
        self._stop_flag = True
        self._emit("symbolic.stop", status="shutdown")
        self._join_threads(timeout=2.0)

    # ---------- telemetry bridge ----------
    def _emit(self, event: str, **payload):
        rec = {"module": "SymbolicML", "event": event, "time": time.time(), **payload}
        try: META()(event, **rec)
        except Exception: pass
        try:
            if self.meta is not None and hasattr(self.meta, "_log_telemetry"):
                self.meta._log_telemetry(dict(rec))
        except Exception: pass
        try:
            if self.kb is not None and hasattr(self.kb, "push_meta_stats"):
                self.kb.push_meta_stats(dict(rec))
            elif self.kb is not None and hasattr(self.kb, "narrations"):
                self.kb.narrations.append(f"[ML] {event}: {payload}")
        except Exception: pass
        try:
            if self.holo is not None and hasattr(self.holo, "add"):
                self.holo.add("ml_event", {"event": event}, {"t": rec["time"]})
        except Exception: pass
        try:
            LOG().info(f"[ML] {event} :: {{k:v for k,v in payload.items() if k!='samples'}}")
        except Exception: pass
        try:
            if self.kairos and hasattr(self.kairos, "step"):
                self.kairos.step(int(getattr(self.kairos, "phase_time", 0)) + 1)
        except Exception: pass
        try:
            ultra = globals().get("ultra", None)
            if ultra is not None and hasattr(ultra, "observe"):
                ultra.observe(f"ml.{event}", **payload)
        except Exception:
            pass

    # ---------- public API ----------
    def update(self, label: str, success: bool, weight: float = 1.0, meta: Optional[Dict[str, Any]] = None):
        # Kept for compatibility (simple archetype counter path)
        self.training_queue.append({"label": label, "success": success, "weight": float(weight), "meta": dict(meta or {})})

    def status(self) -> Dict[str, Any]:
        return self.controller_status()

    # ---------- Kairos / compression helpers ----------
    def _kairos_flux(self) -> float:
        try:
            if self.kairos is not None:
                if hasattr(self.kairos, "last_entropy_flux"):
                    return float(getattr(self.kairos, "last_entropy_flux", 0.0))
                if hasattr(self.kairos, "get_state"):
                    st = self.kairos.get_state()
                    return float(st.get("entropy_flux", 0.0)) if isinstance(st, dict) else 0.0
        except Exception:
            pass
        return 0.0

    def _kairos_state_code(self) -> int:
        fx = self._kairos_flux()
        if fx >= 60: return 4
        if fx >= 30: return 3
        if fx >= 10: return 2
        if fx >= 3:  return 1
        return 0

    def _update_priority_from_compression(self):
        try:
            if self.holo is None:
                return
            cr = float(getattr(self.holo, "compression_ratio", 1.0))
            psnr = float(getattr(self.holo, "psnr", getattr(self.keel, "psnr", 0.0)) or 0.0)
            ssim = float(getattr(self.holo, "ssim", getattr(self.keel, "ssim", 0.0)) or 0.0)
            alpha = 0.05
            self._comp_ratio_ema = cr if self._comp_ratio_ema is None else (1 - alpha) * self._comp_ratio_ema + alpha * cr

            up = (cr > (self._comp_ratio_ema * 1.02))
            down = (cr < (self._comp_ratio_ema * 0.98))
            damp = max(0.85, min(1.0, 1.0 - 0.003 * max(0.0, psnr - 30.0) - 0.10 * max(0.0, ssim - 0.8)))

            if up:
                self._priority_scale["sandbox_assist"] = min(2.0, self._priority_scale.get("sandbox_assist", 1.0) * (1.02 * damp))
                self._priority_scale["curiosity_assist"] = min(2.0, self._priority_scale.get("curiosity_assist", 1.0) * (1.02 * damp))
            elif down:
                for k in ("sandbox_assist", "curiosity_assist"):
                    self._priority_scale[k] = 1.0 + (self._priority_scale.get(k, 1.0) - 1.0) * 0.99
        except Exception:
            pass

    def _priority_entropy_regularizer(self):
        try:
            import numpy as _np  # type: ignore
            vals = _np.array([max(1e-6, float(v)) for v in self._priority_scale.values()], dtype=float)  # type: ignore
            if vals.size == 0:
                return
            p = vals / float(_np.sum(vals))
            ent = float(-_np.sum(p * _np.log(p + 1e-12)))
            if ent < max(0.5, _np.log(len(vals)) * 0.35):
                for k in list(self._priority_scale.keys()):
                    self._priority_scale[k] = 1.0 + (self._priority_scale.get(k, 1.0) - 1.0) * 0.98
        except Exception:
            pass

    # ---------- adaptive helpers ----------
    def _update_label_stats(self, lbl: str, success: bool):
        st = self._label_stats.get(lbl) or {"win": 0.0, "total": 0.0}
        st["win"] += 1.0 if success else 0.0
        st["total"] += 1.0
        self._label_stats[lbl] = st

    def _bucket_learn_scale(self, lbl: str) -> float:
        st = self._label_stats.get(lbl) or {"win": 0.0, "total": 0.0}
        if st["total"] < 10.0:
            return 1.2
        wr = float(st["win"] / max(1.0, st["total"]))
        if wr < 0.35: return 1.5
        if wr < 0.60: return 1.2
        if wr > 0.85: return 0.8
        return 1.0

    def _emit_success_strip(self):
        """Export a tiny 1xN visualization of recent success (PNG if PIL present; NPY otherwise)."""
        try:
            vis_on = bool(getattr(globals().get("SOLVER_TOGGLES", object()), "VIS_EXPORTS", True))
            if not vis_on:
                return
            now = time.time()
            if (now - self._succ_strip_last_emit) < self._succ_strip_period:
                return
            self._succ_strip_last_emit = now

            win = list(self._rolling_success)[-64:]
            if not win:
                return

            import numpy as _np  # type: ignore
            arr = _np.clip(_np.array(win, dtype=float), 0.0, 1.0).reshape(1, -1) * 255.0  # type: ignore
            img = arr.astype("uint8")

            out_dir = os.path.join("exports", "ml")
            os.makedirs(out_dir, exist_ok=True)

            try:
                from PIL import Image  # type: ignore
                path = os.path.join(out_dir, f"succ_strip_{int(now)}.png")
                Image.fromarray(img).resize((img.shape[1] * 4, 16)).save(path)
                META()("ml.succ_strip", path=path, n=len(win))
            except Exception:
                # safer fallback: npy
                path = os.path.join(out_dir, f"succ_strip_{int(now)}.npy")
                _np.save(path, img)  # type: ignore
                META()("ml.succ_strip_npy", path=path, n=len(win))
        except Exception:
            pass

    # ---------- Feedback API ----------
    def record_feedback(self,
                        label: str,
                        memory_layer: str,
                        success: bool,
                        weight: float = 1.0,
                        meta: Optional[Dict[str, Any]] = None):
        item = {
            "label": str(label),
            "layer": str(memory_layer),
            "success": bool(success),
            "weight": float(max(0.0, weight)),
            "meta": dict(meta or {}),
            "ts": time.time()
        }
        self.feedback_queue.append(item)
        self._emit("symbolic.feedback_enqueued", label=label, layer=memory_layer,
                   success=bool(success), weight=float(weight))

    def ingest_sandbox_outcome(self, kind: str, success: bool,
                               inp: Optional["np.ndarray"] = None,
                               out: Optional["np.ndarray"] = None,
                               chain: Optional[List[Any]] = None,
                               score: float = 0.0,
                               task_id: Optional[str] = None,
                               blended_score: Optional[float] = None,
                               **kw):
        w = 1.0 + 0.5 * max(0.0, float(score))
        meta = dict(kw or {})
        meta.update({"source": "sandbox", "kind": kind, "task_id": task_id, "chain_len": len(chain or [])})
        if blended_score is not None:
            meta["blended_score"] = float(blended_score)
            try:
                self._hybrid_roll.append(float(blended_score))
            except Exception:
                pass

        self.record_feedback(label=kind, memory_layer="sandbox", success=success, weight=w, meta=meta)

        # physics/invariant-informed bonus (if both grids present)
        try:
            if HAS_PHYS() and inp is not None and out is not None:
                invs = PHYS()
                inv_bonus = 0.0
                try:
                    if hasattr(invs, "score_pair"):
                        m = invs.score_pair(inp, out) or {}
                        inv_delta = float(m.get("score", 0.0))
                        inv_bonus = max(0.0, min(0.10, 0.10 * inv_delta))
                    else:
                        gi = invs(inp); go = invs(out)  # function path
                        # heuristic: if outputs closer to inputs in invariant space → tiny bonus
                        inv_bonus = max(0.0, min(0.08, 0.5 * float(getattr(go, "epi", 0.0))))
                except Exception:
                    inv_bonus = 0.0
                self.record_feedback(label=f"{kind}.inv", memory_layer="sandbox", success=success,
                                     weight=w * (1.0 + inv_bonus),
                                     meta={"inv_bonus": inv_bonus, "task_id": task_id, "blended_score": blended_score})
                self._emit("symbolic.feedback.invariant_bonus", kind=kind, bonus=inv_bonus)
        except Exception:
            pass

        # pass signal back to sandbox (if it listens)
        try:
            if self.sandbox is not None and hasattr(self.sandbox, "ingest_feedback"):
                self.sandbox.ingest_feedback(kind=kind, success=success, score=score, task_id=task_id)
        except Exception:
            pass

        # optional visual card
        vis_on = bool(getattr(globals().get("SOLVER_TOGGLES", object()), "VIS_EXPORTS", True))
        if vis_on and "save_card_triptych" in globals() and inp is not None and out is not None:
            try:
                out_dir = os.path.join("exports", "ml")
                os.makedirs(out_dir, exist_ok=True)
                pred = inp if chain is None else (globals().get("apply_ops") or globals().get("sandbox_apply_ops"))(inp, chain)  # type: ignore
                title = f"{kind} | {'OK' if success else 'FAIL'} | score={score:.3f}"
                path = os.path.join(out_dir, f"{(task_id or 'na').replace(':','_')}_{int(time.time())}.png")
                save_card_triptych(inp, pred, out, path, title=title)  # type: ignore
                META()("ml.visual_saved", path=path, kind=kind, ok=bool(success))
            except Exception:
                pass

    def ingest_dual_outcome(self, system: str, strict_acc: float, partial_acc: float, total: int):
        self._emit("symbolic.dual_outcome",
                   system=str(system), strict_acc=float(strict_acc), partial_acc=float(partial_acc), total=int(total))
        success = (strict_acc >= 0.80) or (partial_acc >= 0.90)
        weight = float(max(0.1, (strict_acc + partial_acc) / 2.0))
        self.record_feedback(label=f"dual_{system}", memory_layer="evaluation", success=success, weight=weight,
                             meta={"system": system, "strict_acc": strict_acc, "partial_acc": partial_acc, "total": total})
        # optional rulebase nudge
        try:
            if self.rulebase and hasattr(self.rulebase, "nudge_thresholds"):
                self.rulebase.nudge_thresholds(strict_acc=strict_acc, partial_acc=partial_acc)
                self._emit("ml.rulebase.nudge", system=system)
        except Exception:
            pass
        try:
            if self.rulebase and hasattr(self.rulebase, "_emit"):
                self.rulebase._emit("ml.dual_outcome", system=system, strict=strict_acc, partial=partial_acc, total=total)
        except Exception:
            pass

    # ---------- loops ----------
    def _loop_training(self):
        import numpy as _np  # type: ignore
        while not self._stop_flag:
            t0 = time.time()
            try:
                # drain due deferred replays
                try:
                    while self._replay_deferred and self._replay_deferred[0].get("due_batch", 0) <= self._training_batches_seen:
                        self.training_queue.append(self._replay_deferred.popleft()["item"])
                except Exception:
                    pass

                if not self.training_queue:
                    if self._should_self_play():
                        self._idle_self_play()
                    time.sleep(0.01)
                    continue

                # Kairos-paced batch sizing
                k_code = self._kairos_state_code()
                var = float(_np.std(list(self._rolling_success)[-64:])) if len(self._rolling_success) >= 8 else 0.0
                # Higher flux → smaller batch; high variance → smaller batch
                base = 32
                if k_code >= 4: base = 12
                elif k_code == 3: base = 16
                elif k_code == 2: base = 24
                if var > 0.35: base = max(8, int(base * 0.75))
                batch_n = max(8, min(64, base))
                self._emit_pacing("training", {"batch": batch_n, "flux_code": k_code, "var": float(var)})


                batch = self._drain(self.training_queue, n=batch_n)
                if not batch:
                    time.sleep(0.01)
                    continue

                ok_cnt = 0
                # train
                for b in batch:
                    try:
                        lbl, suc = b["label"], bool(b["success"])
                        self._update_label_stats(lbl, suc)
                        scale = self._bucket_learn_scale(lbl)

                        enc = self.encoder
                        weighted = getattr(enc, "record_feedback_weighted", None)
                        if callable(weighted):
                            weighted(label=lbl,
                                     memory_layer=b.get("meta", {}).get("layer", "general"),
                                     success=suc,
                                     weight=float(scale * b.get("weight", 1.0)))
                        else:
                            reps = int(max(1, round(scale)))
                            reps = min(3, reps)
                            for _ in range(reps):
                                enc.record_feedback(
                                    label=lbl,
                                    memory_layer=b.get("meta", {}).get("layer", "general"),
                                    success=suc
                                )
                    except Exception:
                        pass
                    self._rolling_success.append(1.0 if b["success"] else 0.0)
                    ok_cnt += 1 if b["success"] else 0

                self._training_batches_seen += 1

                # encoder temporal decay nudge (if present)
                try:
                    getattr(self.encoder, "_apply_temporal_decay", lambda: None)()
                except Exception:
                    pass

                # holo memory note
                try:
                    if self.holo is not None and hasattr(self.holo, "add"):
                        self.holo.add("ml_training_batch",
                                      {"ok": ok_cnt, "n": len(batch)},
                                      {"succ_win": float(_np.mean(list(self._rolling_success)[-64:])) if len(self._rolling_success) >= 4 else 0.0})
                except Exception:
                    pass

                # kairos tick + record flux sample
                try:
                    fx = self._kairos_flux()
                    self._kairos_flux_roll.append(float(fx))
                    if self.kairos and hasattr(self.kairos, "step"):
                        self.kairos.step(int(getattr(self.kairos, "phase_time", 0)) + 1)
                except Exception:
                    pass

                # keel tap
                try:
                    if self.holo is not None and hasattr(self.holo, "compression_ratio"):
                        _keel_record(rin=self.holo.compression_ratio, rout=self.holo.compression_ratio, note="ml.training")
                except Exception:
                    pass

                # persistence cadence (faster at high flux)
                persist_every = 64 if k_code >= 3 else 128
                if self._training_batches_seen % persist_every == 0:
                    self.save_state(self._state_path)
                    self._emit("symbolic.training_persist", batches=self._training_batches_seen)

            except Exception as e:
                self._emit("symbolic.training_error", error=str(e))
            finally:
                self._loop_durations.append(time.time() - t0)

    def _loop_feedback(self):
        phi = (1.0 + 5 ** 0.5) / 2.0  # golden ratio scaling heuristic
        import numpy as _np  # type: ignore
        while not self._stop_flag:
            try:
                if self._feedback_backoff_cycles > 0:
                    time.sleep(0.05)
                    self._feedback_backoff_cycles -= 1

                if not self.feedback_queue:
                    time.sleep(0.01)
                    continue

                item = self.feedback_queue.popleft()
                lbl = item["label"]
                suc = bool(item["success"])
                base_w = float(item["weight"])
                layer = item.get("layer", "")
                meta = item.get("meta", {}) or {}

                # priority scaling per source
                scale = self._priority_scale.get(lbl, 1.0)
                if layer == "sandbox":
                    scale *= 1.15
                elif layer == "evaluation":
                    scale *= 1.10

                # hybrid-aware boost (small multiplicative on success)
                blend = float(meta.get("blended_score", 0.0))
                success_boost = (1.0 + min(0.10, 0.02 * max(0.0, blend))) if suc else 1.0
                # optional invariant bonus (if meta has inp/out)
                inv_bonus = 0.0
                try:
                    if HAS_PHYS() and "inp" in meta and "out" in meta:
                        invs = PHYS()
                        if hasattr(invs, "score_pair"):
                            m = invs.score_pair(meta["inp"], meta["out"]) or {}
                            inv_bonus = max(0.0, min(0.10, 0.10 * float(m.get("score", 0.0))))
                except Exception:
                    inv_bonus = 0.0
                if inv_bonus > 0:
                    self._emit("symbolic.feedback.invariant_bonus", label=lbl, bonus=inv_bonus)

                w = base_w * scale * (1.0 + inv_bonus)
                w = float(max(0.05, min(5.0, w * phi * success_boost)))

                if not suc:
                    self._failure_labels.append(lbl)
                    self._mutate_failure({"label": lbl, "weight": w, "meta": meta})

                    # curiosity ping
                    try:
                        if self.curiosity is not None and hasattr(self.curiosity, "explore"):
                            gi, go = _np.zeros((1, 1), int), _np.ones((1, 1), int)  # type: ignore
                            self.curiosity.explore(gi, go, budget=1, task_id=meta.get("task_id"))
                    except Exception:
                        pass

                self.training_queue.append({
                    "label": lbl,
                    "success": suc,
                    "weight": w,
                    "meta": meta
                })
                
                self._emit_pacing("feedback")


                # Holo fold (non-sandbox lightweight commits)
                try:
                    if self.holo is not None and meta.get("source") not in ("sandbox", "sim"):
                        self.holo.add("ml_feedback", {"label": lbl, "success": suc}, {"confidence": min(0.99, w / 10.0)})
                except Exception:
                    pass

                # Sandbox can listen to feedback too
                try:
                    if self.sandbox is not None and hasattr(self.sandbox, "ingest_feedback"):
                        self.sandbox.ingest_feedback(kind="ml_feedback", success=suc, score=w)
                except Exception:
                    pass

                # reset backoff on success
                self._feedback_error_streak = 0

                # hybrid stats periodic mirror
                if len(self._hybrid_roll) and (len(self._hybrid_roll) % 16 == 0):
                    try:
                        vals = list(self._hybrid_roll)
                        self._emit("ml.hybrid.stats",
                                   mean=float(_np.mean(vals)),  # type: ignore
                                   mx=float(max(vals)), mn=float(min(vals)), n=len(vals))
                    except Exception:
                        pass

            except Exception as e:
                self._feedback_error_streak += 1
                self._feedback_backoff_cycles = min(50, self._feedback_backoff_cycles + 2)
                self._emit("symbolic.feedback_error", error=str(e), streak=int(self._feedback_error_streak))

    def _loop_health(self):
        import numpy as _np  # type: ignore
        while not self._stop_flag:
            try:
                # Kairos-driven jitter
                flux = self._kairos_flux()
                jitter = 1.0 + 0.06 * float(_np.tanh(flux / 25.0))  # type: ignore
                time.sleep(self._health_period * jitter)

                status = self.controller_status()
                self._emit("symbolic.health", **status)
                # Ultra mirror
                try:
                    ultra = globals().get("ultra", None)
                    if ultra is not None and hasattr(ultra, "observe"):
                        ultra.observe("ml.health", **status)
                except Exception:
                    pass

                # aggregate feedback for meta dashboard
                try:
                    if self.meta is not None and hasattr(self.meta, "record_training_summary"):
                        self.meta.record_training_summary({
                            "ml_rolling_success": status.get("rolling_success"),
                            "ml_batches_seen": status.get("batches_seen"),
                            "priority_entropy": status.get("priority_entropy"),
                            "hybrid_mean": status.get("hybrid_mean"),
                        })
                except Exception:
                    pass

                # kairos echo
                try:
                    if self.kairos and hasattr(self.kairos, "step"):
                        self.kairos.step(int(getattr(self.kairos, "phase_time", 0)) + 1)
                        if hasattr(self.kairos, "get_state"):
                            META()("kairos.health.tick", **(self.kairos.get_state() or {}))
                except Exception:
                    pass

                # keel tap
                try:
                    if self.holo is not None and hasattr(self.holo, "compression_ratio"):
                        _keel_record(rin=self.holo.compression_ratio, rout=self.holo.compression_ratio, note="ml.health")
                except Exception:
                    pass

                # compression-driven priority adaptation + entropy regularization
                self._update_priority_from_compression()
                self._priority_entropy_regularizer()

                # drift → sandbox boost + warmup
                if len(self._rolling_success) >= 64:
                    window = list(self._rolling_success)[-64:]
                    std = float(_np.std(window))  # type: ignore
                    if std > 0.35:
                        self._priority_scale["sandbox_assist"] = min(2.0, self._priority_scale.get("sandbox_assist", 1.0) * 1.05)
                        self._emit("symbolic.drift_boost", label="sandbox_assist", std=std,
                                   new_scale=self._priority_scale["sandbox_assist"])
                        try:
                            if self.sandbox is not None and hasattr(self.sandbox, "warmup"):
                                self.sandbox.warmup(steps=8)
                        except Exception:
                            pass

                # small visual quick-look
                self._emit_success_strip()
                self._emit_pacing("health")


            except Exception as e:
                self._emit("symbolic.health_error", error=str(e))

    # ---------- failure mutation (KB glyph reseeding + Holo assist) ----------
    def _mutate_failure(self, item: Dict[str, Any]):
        try:
            meta = dict(item.get("meta") or {})
            glyph_hint = meta.get("glyph_hint")

            if not glyph_hint and self.kb is not None:
                try:
                    keys = list(getattr(self.kb, "idx_by_glyph_shape", {}).keys())
                    if keys:
                        glyph_hint = random.choice(keys)[0]
                        meta["glyph_hint"] = glyph_hint
                except Exception:
                    pass

            # confidence-aware replay spacing
            lbl = item["label"]
            base_gap = 4
            wr = 0.0
            st = self._label_stats.get(lbl)
            if st and st.get("total", 0.0) > 0.0:
                wr = float(st.get("win", 0.0) / max(1.0, st.get("total", 0.0)))
            gap = max(2, base_gap - int(wr * 2.0))
            self._replay_spacing_map[lbl] = gap

            deferred = {
                "due_batch": int(self._training_batches_seen + gap),
                "item": {
                    "label": f"{lbl}.replay",
                    "success": False,
                    "weight": float(max(0.05, item.get("weight", 0.3) * 0.75)),
                    "meta": meta
                }
            }
            self._replay_deferred.append(deferred)
            self._emit("symbolic.replay_enqueued", label=item["label"], hint=meta.get("glyph_hint"), due=deferred["due_batch"])

            # Holo recall-assisted reseed (if API available)
            try:
                if self.holo and hasattr(self.holo, "get"):
                    # best-effort: ask for nearest memory to shape/context if provided
                    # Note: your Holo.get signature may differ; adjust as needed.
                    cur = meta.get("inp")  # optional snapshot of input grid
                    if cur is not None:
                        hits = self.holo.get(cur, topk=1)
                        if hits:
                            _pred, _hm, dist = hits[0]
                            self._emit("ml.replay.holo_seed", label=lbl, dist=float(dist))
            except Exception:
                pass
        except Exception as e:
            self._emit("symbolic.replay_error", error=str(e))

    # ---------- idle self-play ----------
    def _should_self_play(self) -> bool:
        try:
            return self._kairos_state_code() < 3
        except Exception:
            return True

    def _idle_self_play(self):
        try:
            if not self._should_self_play():
                return
            syllabus = ["synthetic.align", "synthetic.normalize", "synthetic.rotate", "synthetic.flip",
                        "synthetic.recolor", "synthetic.scale", "synthetic.pad"]
            if self._failure_labels:
                last_fail = self._failure_labels[-1]
                if isinstance(last_fail, str):
                    syllabus = [f"{last_fail}.assist"] + syllabus
            lbl = syllabus[self._training_batches_seen % len(syllabus)]
            suc = bool(random.random() < 0.6)
            self.training_queue.append({"label": lbl, "success": suc, "weight": 0.3, "meta": {"source": "sim"}})
        except Exception:
            pass

    # ---------- utilities ----------
    @staticmethod
    def _drain(q: Deque[Dict[str, Any]], n: int) -> List[Dict[str, Any]]:
        out = []
        for _ in range(min(n, len(q))):
            out.append(q.popleft())
        return out

    def controller_status(self) -> Dict[str, Any]:
        import numpy as _np  # type: ignore
        mean_loop = float(_np.mean(self._loop_durations)) if self._loop_durations else 0.0  # type: ignore
        win = list(self._rolling_success)
        succ_rate = float(_np.mean(win)) if win else 0.0  # type: ignore
        pri_vals = list(self._priority_scale.values())
        try:
            if pri_vals:
                p = _np.array(pri_vals, float); p = p / float(p.sum() or 1.0)  # type: ignore
                pent = float(-_np.sum(p * _np.log(p + 1e-12)))  # type: ignore
            else:
                pent = 0.0
        except Exception:
            pent = 0.0

        hybrid_mean = (float(_np.mean(self._hybrid_roll)) if len(self._hybrid_roll) else None)  # type: ignore
        kairos_mean = (float(_np.mean(self._kairos_flux_roll)) if len(self._kairos_flux_roll) else None)  # type: ignore

        enc_summary = getattr(self.encoder, "summarize", lambda: {"n_archetypes": len(getattr(self.encoder, "archetypes", {}))})()

        return {
            "queues": {"training": len(self.training_queue), "feedback": len(self.feedback_queue)},
            "loop_runtime_avg_sec": round(mean_loop, 6),
            "rolling_success": round(succ_rate, 6),
            "encoder": enc_summary,
            "batches_seen": int(self._training_batches_seen),
            "priority_scale": dict(self._priority_scale),
            "priority_entropy": pent,
            "hybrid_mean": hybrid_mean,
            "kairos_flux_mean": kairos_mean,
        }

    # ---------- persistence ----------
    def save_state(self, path: Optional[str] = None):
        try:
            p = path or self._state_path
            state = {
                "priority_scale": dict(self._priority_scale),
                "training_batches_seen": int(self._training_batches_seen),
                "rolling_success": list(self._rolling_success),
                "hybrid_roll": list(self._hybrid_roll),
                "kairos_flux_roll": list(self._kairos_flux_roll),
                "encoder": {
                    "archetypes": dict(getattr(self.encoder, "archetypes", {})),
                    "stats": dict(getattr(self.encoder, "stats", {})) if hasattr(self.encoder, "stats") else {}
                }
            }
            with open(p, "w", encoding="utf-8") as f:
                json.dump(state, f, indent=2)
            self._emit("symbolic.save_state", path=p)
        except Exception as e:
            self._emit("symbolic.save_state_error", error=str(e))

    def load_state(self, path: Optional[str] = None):
        p = path or self._state_path
        if not os.path.isfile(p):
            return
        try:
            with open(p, "r", encoding="utf-8") as f:
                state = json.load(f)
            self._priority_scale = dict(state.get("priority_scale", {}))
            self._training_batches_seen = int(state.get("training_batches_seen", 0))
            self._rolling_success.clear()
            for v in state.get("rolling_success", [])[-1024:]:
                self._rolling_success.append(float(v))
            self._hybrid_roll.clear()
            for v in state.get("hybrid_roll", [])[-256:]:
                try: self._hybrid_roll.append(float(v))
                except Exception: pass
            self._kairos_flux_roll.clear()
            for v in state.get("kairos_flux_roll", [])[-256:]:
                try: self._kairos_flux_roll.append(float(v))
                except Exception: pass
            enc = state.get("encoder", {})
            if hasattr(self.encoder, "archetypes"):
                self.encoder.archetypes = dict(enc.get("archetypes", {}))
            if hasattr(self.encoder, "stats"):
                s = enc.get("stats", {})
                if isinstance(s, dict):
                    self.encoder.stats.update(s)
            self._emit("symbolic.load_state", path=p,
                       n_archetypes=len(getattr(self.encoder, "archetypes", {})))
        except Exception as e:
            self._emit("symbolic.load_state_error", error=str(e))


# ===========================================
# Unified Trainer 
# ===========================================

class TrainerAttachError(RuntimeError):
    pass

class UnifiedTrainer:
    def __init__(self, meta=None, rulebase=None, sandbox=None, holo=None,
                 ultra=None, curiosity=None, sim=None, ml=None, kairos=None,
                 exp_dir: str = "exports/training",
                 enable_visuals: bool = True,
                 enable_csv: bool = True,
                 enable_jsonl: bool = True,
                 parallel_sandbox: bool = False,
                 max_workers: int = 4):
        # Reuse existing graph; do not instantiate subsystems here.
        self.meta, self.rulebase, self.sandbox = meta, rulebase, sandbox
        self.holo, self.ultra, self.curiosity = holo, ultra, curiosity
        self.sim, self.ml, self.kairos = sim, ml, kairos
        self.enable_visuals = bool(enable_visuals)
        self.enable_csv = bool(enable_csv)
        self.enable_jsonl = bool(enable_jsonl)
        self.parallel_sandbox = bool(parallel_sandbox)
        self.max_workers = int(max_workers)
        self.solver = None  # bound via bind(solver)

        self._exp_dir = exp_dir
        os.makedirs(self._exp_dir, exist_ok=True)

        self.kairos_flux_history = deque(maxlen=512)
        self.keel_ratio_history = deque(maxlen=512)
        self._train_rows: List[Dict[str, Any]] = []
        self._fail_streak = 0

        # Repairs / Enhancements state
        self._last_kairos_pair_idx: Optional[int] = None
        self._export_error_streak = 0
        self._rescue_error_streak = 0
        self._fuse_until_ts = 0.0
        self._last_keel_ratio: float = 1.0
        self._task_counts: Dict[Any, int] = {}
        self._shape_hist: Dict[Tuple[int, ...], int] = {}
        self._outstanding_rescues = 0
        self._max_outstanding_rescues = max(1, self.max_workers)

        self._emit("trainer.init", ok=True)

    # ---------- lifecycle ----------
    def bind(self, solver):
        if solver is None:
            raise TrainerAttachError("bind: solver=None (build_runtime must provide a solver instance)")
        self.solver = solver
        # lazily fill missing references from solver if not provided at init
        for name in ("meta","rulebase","sandbox","holo","ultra","curiosity","sim","ml","kairos"):
            if getattr(self, name, None) is None:
                try:
                    setattr(self, name, getattr(solver, name))
                except Exception:
                    pass
        self._emit("trainer.bound", solver=type(solver).__name__)
        return self

    def unbind(self):
        self._emit("trainer.unbound")
        self.solver = None
        return self

    # ---------- tiny utilities ----------
    @staticmethod
    def _as_grid(lst) -> Any:
        if np is None:
            return lst
        return np.array(lst, dtype=int)

    @staticmethod
    def _grids_equal(a: Any, b: Any) -> bool:
        try:
            if np is None:
                return a == b
            return a.shape == b.shape and np.array_equal(a, b)
        except Exception:
            return False

    @staticmethod
    def _entropy_safe(arr: Any) -> float:
        if np is None:
            return 0.0
        try:
            vals, counts = np.unique(arr, return_counts=True)
            p = counts / counts.sum()
            return float(-(p * np.log2(p + 1e-12)).sum())
        except Exception:
            return 0.0

    @staticmethod
    def _compact_payload(payload: Dict[str, Any], max_len: int = 256, max_items: int = 8) -> Dict[str, Any]:
        out: Dict[str, Any] = {}
        for k, v in (payload or {}).items():
            if k == "samples":
                continue
            try:
                if np is not None and isinstance(v, np.ndarray):
                    out[k] = {"type": "ndarray", "shape": list(v.shape), "dtype": str(v.dtype)}
                elif isinstance(v, (int, float, str, bool)) or v is None:
                    if isinstance(v, str) and len(v) > max_len:
                        out[k] = v[:max_len] + "…"
                    else:
                        out[k] = v
                elif isinstance(v, (list, tuple)):
                    out[k] = list(v[:max_items]) + (["…"] if len(v) > max_items else [])
                elif isinstance(v, dict):
                    trimmed = {}
                    for i, (kk, vv) in enumerate(v.items()):
                        if i >= max_items:
                            trimmed["…"] = f"+{len(v)-max_items}"
                            break
                        if isinstance(vv, (int, float, str, bool)) or vv is None:
                            trimmed[kk] = vv if not (isinstance(vv, str) and len(vv) > max_len) else vv[:max_len] + "…"
                        else:
                            trimmed[kk] = str(type(vv).__name__)
                    out[k] = trimmed
                else:
                    out[k] = str(type(v).__name__)
            except Exception:
                out[k] = "<err>"
        return out
        
    def _housekeep(self, epoch_idx: int):
        try:
            periodic_housekeeping_and_exports(epoch_idx=epoch_idx)
            # optional: echo a light heartbeat
            self._emit("trainer.housekeep", epoch_idx=int(epoch_idx))
        except Exception as e:
            # never fail training for housekeeping
            self._emit("trainer.housekeeping_fail", error=str(e), epoch_idx=int(epoch_idx))


    def _emit(self, topic: str, **payload):
        rec = {"module": "Trainer", "event": topic, "time": time.time(), **payload}
        # meta_log (structured)
        try:
            ml = _g("meta_log")
            if callable(ml):
                ml(topic, **rec)
        except Exception:
            pass
        # meta telemetry channel
        try:
            if self.meta and hasattr(self.meta, "_log_telemetry"):
                self.meta._log_telemetry(dict(rec))
        except Exception:
            pass
        # Holo event trace (redacted)
        try:
            if self.holo and hasattr(self.holo, "add"):
                self.holo.add(None, None, {"subject": "trainer", "event": topic,
                                            **{k: v for k, v in payload.items() if k != "samples"}})
        except Exception:
            pass
        # Ultra observe (compact)
        try:
            if self.ultra and hasattr(self.ultra, "observe"):
                self.ultra.observe("trainer", topic=topic, **self._compact_payload(payload))
        except Exception:
            pass
        # Logger (lazy formatting)
        try:
            lg = _g("logger")
            if lg and hasattr(lg, "info"):
                lg.info("[Trainer] %s :: %s", topic, self._compact_payload(payload))
        except Exception:
            pass

    def _kairos_step(self, step_idx: int = 0):
        # Guard: step Kairos once per pair index (idempotent-ish)
        try:
            if self._last_kairos_pair_idx == step_idx:
                self._emit("trainer.kairos.already_stepped", idx=step_idx)
                return
            self._last_kairos_pair_idx = step_idx
            if self.kairos and hasattr(self.kairos, "step"):
                self.kairos.step(time_step=step_idx)
                flux = abs(float(getattr(self.kairos, "last_entropy_flux", 0.0)))
                self.kairos_flux_history.append(flux)
                self._emit("trainer.kairos.step", flux=flux)
        except Exception:
            pass

    def _keel_snapshot_file(self, file_path: str):
        # Optional: snapshot compression ratio if keel functions are available
        try:
            with open(file_path, "rb") as f:
                raw = f.read()
            kc = _g("keel_compress_bytes")
            if callable(kc):
                b1, _ = kc(raw, q_ll=3.0, deblock=True)
                ratio = (len(raw) / max(1, len(b1)))
            else:
                ratio = 1.0
            self.keel_ratio_history.append(float(ratio))
            self._last_keel_ratio = float(ratio)
            self._emit("trainer.keel_snapshot", file=os.path.basename(file_path), keel_ratio=float(ratio))
        except Exception:
            pass

    # ---------- main entry ----------
    def train_on_dataset(self, dataset):
        os.makedirs(self._exp_dir, exist_ok=True)

        # Transient shield: slow if a fuse is active
        if time.time() < self._fuse_until_ts:
            time.sleep(0.05)

        # Respect your CSV loader + human rules if available
        try:
            if self.rulebase and hasattr(self.rulebase, "preload_from_csv"):
                self.rulebase.preload_from_csv()
            if self.rulebase and hasattr(self.rulebase, "preload_human_rules"):
                self.rulebase.preload_human_rules()
        except Exception as e:
            self._emit("trainer.rulebase.preload_failed", error=str(e))

        # Tap existing invariants/physics/creativity if present
        inv_cls = _g("InvariantScorer")
        inv = inv_cls(logger=_g("EXPLAIN")) if callable(inv_cls) else None

        phys_cls = _g("PhysicsHeuristics")
        phys = phys_cls(logger=_g("EXPLAIN")) if callable(phys_cls) else None

        cfeat_fn = _g("creativity_features")

        self._train_rows.clear()
        self._fail_streak = 0

        # Optional parallel sandbox rescues
        pool = None
        futures = []
        if self.parallel_sandbox:
            try:
                from concurrent.futures import ThreadPoolExecutor
                pool = ThreadPoolExecutor(max_workers=self.max_workers)
            except Exception:
                pool = None

        # Per-task pacing counters
        self._task_counts.clear()

        for task in (dataset or []):
            tid = task.get("id")
            self._task_counts[tid] = 0
            pairs = task.get("train", []) or []
            for idx, pair in enumerate(pairs):
                xin = self._as_grid(pair["input"])
                yout = self._as_grid(pair["output"])

                # shape histogram for Holo heartbeat
                try:
                    shape_key = tuple(getattr(xin, "shape", ()))
                    self._shape_hist[shape_key] = self._shape_hist.get(shape_key, 0) + 1
                except Exception:
                    pass

                self._kairos_step(step_idx=idx)
                self._task_counts[tid] += 1

                # Optional: your own “learn_from_pair” may exist elsewhere
                try:
                    if hasattr(self, "learn_from_pair"):
                        self.learn_from_pair(xin, yout)
                except Exception as e:
                    self._emit("trainer.learn_from_pair_failed", task_id=tid, idx=idx, error=str(e))

                # Invariants
                try:
                    inv_metrics = inv.score_pair(xin, yout, yout) if inv else {}
                except Exception:
                    inv_metrics = {}
                inv_metrics = inv_metrics or {"dH": 0.0, "epi": 0.0, "binder": 0.0, "score": -1e9, "fits": False}

                # Physics
                try:
                    phys_metrics = phys.score_pair(xin, yout) if phys else {}
                except Exception:
                    phys_metrics = {}
                phys_metrics = phys_metrics or {"mass_delta": 1.0, "centroid_shift": 999.0, "sym_delta": -1.0, "ok": False}

                # Creativity
                try:
                    feats = cfeat_fn(xin) if callable(cfeat_fn) else {}
                except Exception:
                    feats = {}
                used_fallback = False
                try:
                    if self.meta and hasattr(self.meta, "evaluate_creativity"):
                        signal = self.meta.evaluate_creativity(feats, context="trainer")
                    else:
                        if _g("_entropy"):
                            novelty = float(abs(_g("_entropy")(yout) - _g("_entropy")(xin)) > 0.0)
                        else:
                            novelty = float(abs(self._entropy_safe(yout) - self._entropy_safe(xin)) > 0.0)
                        signal = {"composite": novelty, "novelty": bool(novelty), "tag": "no-meta"}
                        used_fallback = True
                except Exception:
                    signal = {"composite": None, "novelty": False, "tag": "err"}
                    used_fallback = True

                try:
                    if self.sim and hasattr(self.sim, "ingest_creativity_vector"):
                        self.sim.ingest_creativity_vector({"task_id": tid, "pair_idx": idx, **feats})
                except Exception:
                    pass

                # KB fact taps (non-fatal)
                try:
                    if self.rulebase and getattr(self.rulebase, "kb", None):
                        kb = self.rulebase.kb
                        if hasattr(kb, "add_fact"):
                            kb.add_fact("task", {"id": tid})
                            kb.add_fact("shape_in", {"shape": tuple(getattr(xin, "shape", ()))})
                            kb.add_fact("shape_out", {"shape": tuple(getattr(yout, "shape", ()))})
                            for k, v in (feats or {}).items():
                                kb.add_fact(f"creativity_{str(k)}", v)
                except Exception:
                    pass

                # Confidence side-channel (simple reliability)
                try:
                    reliability = float(0.5 * (float(inv_metrics.get("fits", False)) + float(phys_metrics.get("ok", False))))
                except Exception:
                    reliability = 0.0

                ok = 1 if signal.get("novelty") else 0
                self._fail_streak = self._fail_streak + 1 if not ok else 0

                # Auto-augment on fail streak (light deterministic augments)
                if self._fail_streak >= 3:
                    try:
                        aug_ops = _g("augment_ops_light")
                        if callable(aug_ops) and self.sandbox and hasattr(self.sandbox, "discover_chain"):
                            xin_aug = aug_ops(xin, seed=self._fail_streak)
                            _ = self.sandbox.discover_chain(xin_aug, yout, task_id=f"{tid}:{idx}:aug", max_depth=2)
                            self._emit("trainer.augment_probe", task_id=tid, pair_idx=idx)
                    except Exception:
                        pass

                # Auto-rescue via sandbox after a fail streak
                if self._fail_streak >= 5:
                    self._emit("trainer.fail_streak", count=self._fail_streak, task_id=tid, pair_idx=idx)

                    def _rescue():
                        try:
                            if self.sandbox and hasattr(self.sandbox, "discover_chain"):
                                # Flux-adaptive depth
                                if np is not None and self.kairos_flux_history:
                                    mean_flux = float(np.mean(list(self.kairos_flux_history)[-16:]))
                                else:
                                    mean_flux = 0.0
                                depth = 5 if mean_flux < 10 else 3 if mean_flux < 30 else 2
                                chain = self.sandbox.discover_chain(xin, yout, task_id=f"{tid}:{idx}", max_depth=depth)
                                if chain and self.rulebase and getattr(self.rulebase, "kb", None):
                                    self.rulebase.kb.remember_xform(xin, yout, chain, confidence=1.0)
                                    try:
                                        if hasattr(self.rulebase.kb, "apply_xform_to_neighbor"):
                                            self.rulebase.kb.apply_xform_to_neighbor(xin, chain, tag="rehearsal")
                                        self._emit("trainer.rehearsal_ping", task_id=tid, pair_idx=idx)
                                    except Exception:
                                        pass
                                    if self.holo and hasattr(self.holo, "add"):
                                        self.holo.add(xin, yout, {"subject": "trainer.rescue",
                                                                  "task_id": tid, "pair_idx": idx, "ops": chain})
                                    # Hybrid/Blender passthrough score (if present)
                                    blended_score = None
                                    try:
                                        blender = getattr(self.ultra, "blender", None) or getattr(self.meta, "hybrid_blender", None)
                                        if blender and hasattr(blender, "blended_score"):
                                            blended_score = float(blender.blended_score(xin, chain, yout))
                                    except Exception:
                                        blended_score = None
                                    if self.ml and hasattr(self.ml, "ingest_sandbox_outcome"):
                                        self.ml.ingest_sandbox_outcome(kind="trainer_rescue", success=True,
                                                                       inp=xin, out=yout, chain=chain,
                                                                       score=1.0, task_id=f"{tid}:{idx}",
                                                                       blended_score=blended_score)
                        except Exception as e:
                            self._rescue_error_streak += 1
                            self._emit("trainer.sandbox_rescue_failed", error=str(e), streak=int(self._rescue_error_streak))
                        finally:
                            self._outstanding_rescues = max(0, self._outstanding_rescues - 1)

                    if pool:
                        if self._outstanding_rescues >= self._max_outstanding_rescues:
                            self._emit("trainer.rescue_skipped_due_to_backpressure",
                                       outstanding=self._outstanding_rescues, limit=self._max_outstanding_rescues)
                        else:
                            self._outstanding_rescues += 1
                            try:
                                futures.append(pool.submit(_rescue))
                            except Exception as e:
                                self._outstanding_rescues = max(0, self._outstanding_rescues - 1)
                                self._emit("trainer.rescue_submit_failed", error=str(e))
                    else:
                        _rescue()
                    self._fail_streak = 0

                # Row + telemetry
                row = {
                    "schema": "trainer_rows.v1",
                    "task_id": tid, "pair_idx": idx,
                    "in_shape": str(tuple(getattr(xin, "shape", ()))),
                    "out_shape": str(tuple(getattr(yout, "shape", ()))),
                    "creativity_score": signal.get("composite"), "creativity_tag": signal.get("tag"),
                    "creativity_provider": ("fallback" if used_fallback else "meta") if signal.get("tag") != "err" else "error",
                    "novelty": signal.get("novelty"),
                    "dH": inv_metrics.get("dH"), "epi": inv_metrics.get("epi"),
                    "binder": inv_metrics.get("binder"), "inv_score": inv_metrics.get("score"),
                    "inv_fits": inv_metrics.get("fits"),
                    "mass_delta": phys_metrics.get("mass_delta"),
                    "centroid_shift": phys_metrics.get("centroid_shift"),
                    "sym_delta": phys_metrics.get("sym_delta"),
                    "phys_ok": phys_metrics.get("ok"),
                    "reliability": reliability,
                    "timestamp": time.time()
                }
                self._train_rows.append(row)
                self._emit("trainer.train_pair", **{k: v for k, v in row.items() if k not in ("schema",)})                
                if len(self._train_rows) % 50 == 0:                    
                    self._housekeep(epoch_idx=len(self._train_rows) // 50)


        # Drain rescues gracefully
        if pool:
            try:
                # Best-effort wait; no hard timeout to avoid killing threads rudely
                for fut in futures:
                    try:
                        fut.result(timeout=5.0)
                    except Exception:
                        pass
                pool.shutdown(wait=True)
            except Exception:
                pass

        self._export_tabular_and_visuals()
        self._export_summary_and_feedback()        
        self._housekeep(epoch_idx=max(1, len(self._train_rows) // 50) + 1)

        if self.rulebase and hasattr(self.rulebase, "save_to_csv"):
            try:
                self.rulebase.save_to_csv()
            except Exception as e:
                self._emit("trainer.rulebase.save_failed", error=str(e))

        # Return a compact summary for upstream orchestration
        try:
            avg_creativity = float(np.mean([r["creativity_score"] for r in self._train_rows if r.get("creativity_score") is not None])) if (np and self._train_rows) else 0.0
            phys_pass_rate = float(np.mean([1 if r.get("phys_ok") else 0 for r in self._train_rows])) if (np and self._train_rows) else 0.0
        except Exception:
            avg_creativity, phys_pass_rate = 0.0, 0.0
        return {
            "n_pairs": len(self._train_rows),
            "avg_creativity": avg_creativity,
            "phys_pass_rate": phys_pass_rate,
            "exp_dir": self._exp_dir
        }

    # ---------- exports ----------
    def _export_tabular_and_visuals(self):
        # CSV + JSONL (guarded by toggles) with atomic writes
        try:
            rows = self._train_rows
            if rows and self.enable_csv:
                csv_path = os.path.join(self._exp_dir, "train_pairs.csv")
                tmp_csv = csv_path + ".tmp"
                with open(tmp_csv, "w", newline="") as f:
                    writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
                    writer.writeheader(); writer.writerows(rows)
                os.replace(tmp_csv, csv_path)
                self._emit("trainer.export_csv", n=len(rows), csv=csv_path)
                self._keel_snapshot_file(csv_path)
                # optional manifest
                am = _g("artifact_manifest")
                if am and hasattr(am, "add"):
                    try: am.add(csv_path, phase="trainer", pass_ix=None)
                    except Exception: pass

            if rows and self.enable_jsonl:
                jl_path = os.path.join(self._exp_dir, "train_pairs.jsonl")
                tmp_jl = jl_path + ".tmp"
                with open(tmp_jl, "w", encoding="utf-8") as f:
                    for r in rows:
                        f.write(json.dumps(r) + "\n")
                os.replace(tmp_jl, jl_path)
                self._emit("trainer.export_jsonl", n=len(rows), jsonl=jl_path)
                self._keel_snapshot_file(jl_path)
                am = _g("artifact_manifest")
                if am and hasattr(am, "add"):
                    try: am.add(jl_path, phase="trainer", pass_ix=None)
                    except Exception: pass

            # Reset error streak on success
            self._export_error_streak = 0
        except Exception as e:
            self._export_error_streak += 1
            if self._export_error_streak >= 3:
                self._fuse_until_ts = time.time() + 1.0  # slow down for a second
                self._emit("trainer.fuse_trip", fuse_until=self._fuse_until_ts, where="exports")
            self._emit("trainer.export_failed", error=str(e))

        if not self.enable_visuals:
            return

        # Visuals (no hard dependency on matplotlib if unavailable)
        try:
            import matplotlib.pyplot as plt  # local import to avoid global dependency
            rows = self._train_rows
            scores = [r["creativity_score"] for r in rows if r.get("creativity_score") is not None]
            inv_scores = [r["inv_score"] for r in rows if r.get("inv_score") not in (None, -1e9)]
            phys_ok = [1 if r.get("phys_ok") else 0 for r in rows]

            if scores:
                plt.figure(figsize=(8, 4)); plt.plot(scores, alpha=0.7)
                plt.title("Creativity Signal Over Training Pairs"); plt.tight_layout()
                path = os.path.join(self._exp_dir, "train_creativity.png")
                plt.savefig(path); plt.close()
                self._emit("trainer.visual", kind="creativity_series", path=path)

            if scores and inv_scores:
                plt.figure(figsize=(6, 6)); plt.scatter(inv_scores, scores, alpha=0.6)
                plt.title("Creativity vs Invariant Score"); plt.tight_layout()
                path = os.path.join(self._exp_dir, "train_creativity_vs_invariants.png")
                plt.savefig(path); plt.close()
                self._emit("trainer.visual", kind="creativity_vs_invariants", path=path)

            if phys_ok:
                if np is not None:
                    grid = np.array(phys_ok).reshape(1, -1)
                else:
                    grid = [[int(x) for x in phys_ok]]
                plt.figure(figsize=(12, 2)); plt.imshow(grid, cmap="Greens", aspect="auto")
                plt.title("Physics OK across Training Pairs"); plt.tight_layout()
                path = os.path.join(self._exp_dir, "train_physics_ok.png")
                plt.savefig(path); plt.close()
                self._emit("trainer.visual", kind="physics_ok", path=path)

            # Rolling ROC-esque: novelty vs inv_score threshold scatter
            if inv_scores and scores:
                try:
                    nov = [1 if r.get("novelty") else 0 for r in rows]
                    plt.figure(figsize=(6, 4)); plt.scatter(inv_scores, nov, alpha=0.4)
                    plt.title("Novelty vs Invariant Score"); plt.tight_layout()
                    path = os.path.join(self._exp_dir, "train_novelty_vs_invariants.png")
                    plt.savefig(path); plt.close()
                    self._emit("trainer.visual", kind="novelty_vs_invariants", path=path)
                except Exception:
                    pass

            self._emit("trainer.visuals_exported")
        except Exception as e:
            self._emit("trainer.visuals_failed", error=str(e))

    def _export_summary_and_feedback(self):
        try:
            rows = self._train_rows
            if np is not None and rows:
                cr_vals = [r["creativity_score"] for r in rows if r.get("creativity_score") is not None]
                inv_vals = [r["inv_score"] for r in rows if r.get("inv_score") not in (None, -1e9)]
                avg_creativity = float(np.mean(cr_vals)) if cr_vals else 0.0
                avg_inv = float(np.mean(inv_vals)) if inv_vals else 0.0
                phys_pass_rate = float(np.mean([1 if r.get("phys_ok") else 0 for r in rows]))
                mean_flux = float(np.mean(self.kairos_flux_history)) if self.kairos_flux_history else 0.0
                mean_keel = float(np.mean(self.keel_ratio_history)) if self.keel_ratio_history else float(self._last_keel_ratio or 1.0)
            else:
                avg_creativity = avg_inv = phys_pass_rate = mean_flux = 0.0
                mean_keel = float(self._last_keel_ratio or 1.0)

            txt_path = os.path.join(self._exp_dir, "training_summary.txt")
            with open(txt_path, "w") as f:
                f.write("Training Summary\n")
                f.write(f"Pairs trained: {len(rows)}\n")
                f.write(f"Avg creativity: {avg_creativity:.3f}\n")
                f.write(f"Avg invariant score: {avg_inv:.3f}\n")
                f.write(f"Physics pass rate: {phys_pass_rate:.2%}\n")
                f.write(f"Kairos mean flux: {mean_flux:.3f}\n")
                f.write(f"Keel mean ratio: {mean_keel:.3f}\n")

            # Small bar chart if available
            try:
                import matplotlib.pyplot as plt
                fig, ax = plt.subplots(figsize=(6, 4))
                metrics = ["avg_creativity", "avg_inv", "phys_pass_rate"]
                vals = [avg_creativity, avg_inv, phys_pass_rate]
                ax.bar(metrics, vals); ax.set_ylim(0, 1); ax.set_title("Training Summary Metrics")
                plt.tight_layout()
                path = os.path.join(self._exp_dir, "training_summary_metrics.png")
                plt.savefig(path); plt.close()
                self._emit("trainer.visual", kind="summary_metrics", path=path)
            except Exception as e:
                self._emit("trainer.summary_plot_failed", error=str(e))

            # Holo histogram heartbeat
            try:
                if self.holo and hasattr(self.holo, "add"):
                    self.holo.add(None, None, {"subject": "trainer.hist",
                                               "shapes": {str(k): v for k, v in self._shape_hist.items()},
                                               "novelty_rate": phys_pass_rate})
            except Exception:
                pass

            # Meta + ML taps (guarded)
            try:
                if self.meta and hasattr(self.meta, "record_training_summary"):
                    self.meta.record_training_summary({
                        "avg_creativity": avg_creativity,
                        "avg_inv": avg_inv,
                        "phys_pass_rate": phys_pass_rate,
                        "n_pairs": len(rows),
                        "mean_flux": mean_flux,
                        "mean_keel": mean_keel,
                    })
            except Exception:
                pass
            try:
                if self.ml and hasattr(self.ml, "record_feedback"):
                    ok = (avg_creativity >= 0.5) and (phys_pass_rate >= 0.5)
                    w = float(max(0.1, (avg_creativity + phys_pass_rate) / 2.0))
                    self.ml.record_feedback(label="trainer.summary", memory_layer="training",
                                            success=ok, weight=w,
                                            meta={"avg_creativity": avg_creativity, "avg_inv": avg_inv,
                                                  "phys_pass_rate": phys_pass_rate, "n_pairs": len(rows),
                                                  "mean_flux": mean_flux, "mean_keel": mean_keel})
            except Exception:
                pass

            # Drift taps
            try:
                self._emit("trainer.system_drift", flux=mean_flux, keel=mean_keel)
            except Exception:
                pass

            self._emit("trainer.summary_exported",
                       avg_creativity=avg_creativity,
                       avg_inv=avg_inv,
                       phys_pass_rate=phys_pass_rate,
                       mean_flux=mean_flux,
                       mean_keel=mean_keel,
                       n=len(rows))
        except Exception as e:
            self._emit("trainer.summary_failed", error=str(e))


def attach_unified_trainer(solver,
                           use_meta=True, use_rulebase=True, use_sandbox=True,
                           use_holo=True, use_ultra=True, use_curiosity=True,
                           use_sim=True, use_ml=True, use_kairos=True,
                           parallel_sandbox=False, max_workers=4):
    # --- Attach-time verification ---
    if solver is None:
        ml = _g("meta_log"); 
        try: ml and ml("trainer.attach", ok=False, reason="solver_none")
        except Exception: pass
        raise TrainerAttachError("attach_unified_trainer: solver is None (build_runtime must return a solver instance)")

    # Minimal solver capability check
    for req in ("predict",):
        if not hasattr(solver, req):
            ml = _g("meta_log")
            try: ml and ml("trainer.attach", ok=False, reason=f"solver_missing:{req}")
            except Exception: pass
            raise TrainerAttachError(f"attach_unified_trainer: solver missing required method: {req}()")

    meta      = getattr(solver, "meta", None)      if use_meta      else None
    rulebase  = getattr(solver, "rulebase", None)  if use_rulebase  else None
    sandbox   = getattr(solver, "sandbox", None)   if use_sandbox   else None
    holo      = getattr(solver, "holo", None)      if use_holo      else None
    ultra     = getattr(solver, "ultra", None)     if use_ultra     else None
    curiosity = getattr(solver, "curiosity", None) if use_curiosity else None
    sim       = getattr(solver, "sim", None)       if use_sim       else None
    ml_handle = getattr(getattr(solver, "meta", None), "symbolic", None) if use_ml else None
    kairos    = getattr(solver, "kairos", None)    if use_kairos    else None

    # Attach-time verification for ML contract (wrap/disable if incomplete)
    try:
        if ml_handle is not None and not (hasattr(ml_handle, "record_feedback") and hasattr(ml_handle, "ingest_sandbox_outcome")):
            mlog = _g("meta_log")
            try: mlog and mlog("trainer.attach.ml_incomplete", ok=False)
            except Exception: pass
            ml_handle = None
    except Exception:
        ml_handle = None

    trainer = UnifiedTrainer(meta=meta, rulebase=rulebase, sandbox=sandbox,
                             holo=holo, ultra=ultra, curiosity=curiosity,
                             sim=sim, ml=ml_handle, kairos=kairos,
                             parallel_sandbox=parallel_sandbox,
                             max_workers=max_workers).bind(solver)

    # Bind onto solver
    setattr(solver, "trainer", trainer)

    try:
        ml = _g("meta_log")
        ml and ml("trainer.attached", ok=True, solver=type(solver).__name__)
    except Exception:
        pass
    return trainer

# ==========================================================
#  ORCHESTRATION HOOKS (phases + saves + exports)
# ==========================================================
try:
    _ORCHESTRATION_HOOKS_INSTALLED
except NameError:
    _ORCHESTRATION_HOOKS_INSTALLED = True

    def _attach_sandbox_and_ultra(solver, run_id="na", phase="boot"):        
        try:
            kb = getattr(solver, "kb", None)
            meta = getattr(solver, "meta", None)
            holo = getattr(solver, "holo", None)
            sim = getattr(solver, "sim", None)
            blender = getattr(solver, "blender", None)
            kairos_ref = globals().get("kairos", None)

            if not hasattr(solver, "sandbox") or solver.sandbox is None:
                ctor = globals().get("SandboxExplorer", None)
                if callable(ctor):
                    try:
                        solver.sandbox = ctor(kb=kb, meta=meta, holo=holo, sim=sim, blender=blender, run_id=run_id)  # type: ignore
                    except Exception:
                        solver.sandbox = ctor(kb=kb, meta=meta, run_id=run_id)  # type: ignore
            if not hasattr(solver, "ultra") or solver.ultra is None:
                solver.ultra = SymbolicUltraAgent(enable=True, run_id=run_id, phase=phase)
            else:
                solver.ultra.set_phase(phase)

            # Initial observability
            try:
                solver.ultra.observe("orchestrate.attach", run_id=run_id,
                                     has_kairos=bool(kairos_ref is not None),
                                     has_holo=bool(holo is not None),
                                     has_sim=bool(sim is not None),
                                     has_blender=bool(blender is not None))
            except Exception:
                pass
        except Exception:
            pass

    def _phase_begin(solver, phase):
        try:
            solver.ultra.set_phase(phase)
        except Exception:
            pass
        # Kairos phase pulse
        try:
            if "kairos" in globals() and hasattr(kairos, "step"):
                kairos.step(int(time.time() % 1000))
                if "meta_log" in globals():
                    meta_log("kairos.phase_begin", **kairos.get_state())  # type: ignore
        except Exception:
            pass
        # Bus checksum + summary
        try:
            cs = solver.ultra._bus_checksum()
            if "meta_log" in globals():
                meta_log("phase.begin", phase=phase, bus_checksum=cs)  # type: ignore
            solver.ultra.observe("phase.begin", phase=phase, bus_checksum=cs)
            solver.ultra._append_phase_log("phase.begin", phase=phase, bus_checksum=cs)
        except Exception:
            pass

    def _phase_end(solver, phase):
        # finalize sandbox artifacts
        try:
            if hasattr(solver, "sandbox") and solver.sandbox is not None:
                try:
                    solver.sandbox.save_weights()
                except Exception:
                    pass
                try:
                    solver.sandbox.finalize_exports()
                except Exception:
                    pass
        except Exception:
            pass

        # decay KB, summarize, export Ultra
        try:
            if hasattr(solver, "ultra") and solver.ultra is not None:
                # decay weights before snapshot
                try:
                    solver.ultra.kb.decay(lam=0.001)
                except Exception:
                    pass
                drift = solver.ultra.kb.drift_since_phase_begin()
                # phase summary pulse
                summ = solver.ultra.emit_state()
                try:
                    if "meta_log" in globals():
                        meta_log("ultra.phase_summary", **summ)  # type: ignore
                except Exception:
                    pass
                # structured observations
                solver.ultra.observe("phase.drift", drift=int(drift))
                solver.ultra.observe("phase.summary", **{k: v for k, v in summ.items() if k not in ("run_id", "phase")})
                # export summary JSON into run/phase dir
                solver.ultra.export_json()  # path auto-scoped
                # append a one-line phase log
                solver.ultra._append_phase_log("phase.summary", **summ)
        except Exception:
            pass

        # optional compression snapshot coupling
        try:
            cr = None
            if "holo" in globals() and hasattr(holo, "compression_ratio"):
                cr = float(holo.compression_ratio)
            elif hasattr(solver, "holo") and solver.holo is not None and hasattr(solver.holo, "compression_ratio"):
                cr = float(solver.holo.compression_ratio)
            if cr is not None:
                if "meta_log" in globals():
                    meta_log("keel.snapshot", ratio=cr, phase=phase)  # type: ignore
                try:
                    solver.ultra.observe("keel.snapshot", ratio=float(cr), phase=phase)
                except Exception:
                    pass
        except Exception:
            pass

        # Kairos phase-end pulse + bus checksum close
        try:
            if "kairos" in globals() and hasattr(kairos, "step"):
                kairos.step(int(time.time() % 1000))
                if "meta_log" in globals():
                    meta_log("kairos.phase_end", **kairos.get_state())  # type: ignore
        except Exception:
            pass
        try:
            cs = solver.ultra._bus_checksum() if hasattr(solver, "ultra") and solver.ultra is not None else "na"
            if "meta_log" in globals():
                meta_log("phase.end", phase=phase, bus_checksum=cs)  # type: ignore
            if hasattr(solver, "ultra") and solver.ultra is not None:
                solver.ultra.observe("phase.end", phase=phase, bus_checksum=cs)
                solver.ultra._append_phase_log("phase.end", phase=phase, bus_checksum=cs)
        except Exception:
            pass

# ===========================================
# Rule Generator (identity / delta / object_map) — KB + Telemetry + Unified Exports
# ===========================================


class RuleGenerator:
    def __init__(
        self,
        meta=None,
        max_shift: int = 3,
        reward_scale: float = 0.05,
        inv_weight: float = 1.0,
        phys_weight: float = 1.0,
        kb: Optional['SymbolicKB'] = None,
        ultra: Optional[Any] = None,
        kairos: Optional[Any] = None,
        holo: Optional[Any] = None,
        encoder: Optional[Any] = None,
        sandbox: Optional[Any] = None,
        solver: Optional[Any] = None,
        rulebase: Optional['GlobalRulebase'] = None,  # type: ignore[name-defined]
    ):
        self.meta = meta
        self.max_shift = int(max_shift)
        self.reward_scale = float(reward_scale)
        self.inv_weight = float(inv_weight)
        self.phys_weight = float(phys_weight)
        self.confidence_memory: Dict[str, float] = {}
        self.export_dir = os.path.join("exports", "rules")
        os.makedirs(self.export_dir, exist_ok=True)

        # collaborators
        self.kb = kb if kb is not None else (meta.kb if meta is not None and hasattr(meta, "kb") else None)
        self.ultra = ultra
        self.kairos = kairos
        self.holo = holo
        self.encoder = encoder
        self.sandbox = sandbox
        self.solver = solver
        self.rulebase = rulebase if rulebase is not None else (getattr(meta, "rulebase", None) if meta is not None else None)

        # hardening add-ons
        self._dedupe_seen = set()  # kind+params hash to avoid exact duplicates
        self._wal_path = os.path.join(self.export_dir, "rulegen.wal.jsonl")
        self._src_counter = Counter()

        # optional confidence calibration hooks
        self._cal = None
        try:
            SC = globals().get("SigmoidCalibrator")
            self._cal = SC() if SC else None
        except Exception:
            self._cal = None
        self._global_cal = globals().get("HYBRID_GLOBAL_CAL", None)

    # ---------- late wiring ----------
    def set_context(self, *, meta=None, kb=None, ultra=None, kairos=None, holo=None, encoder=None, sandbox=None, solver=None, rulebase=None):
        if meta is not None: self.meta = meta
        if kb is not None: self.kb = kb
        if ultra is not None: self.ultra = ultra
        if kairos is not None: self.kairos = kairos
        if holo is not None: self.holo = holo
        if encoder is not None: self.encoder = encoder
        if sandbox is not None: self.sandbox = sandbox
        if solver is not None: self.solver = solver
        if rulebase is not None: self.rulebase = rulebase
        return self

    # ---------- helpers ----------
    def _safe_meta_log(self, topic: str, **payload):
        try:
            _ml = globals().get("_meta_log")
            if callable(_ml):
                _ml(topic, **payload)
            elif 'meta_log' in globals():
                meta_log(topic, **payload)  # type: ignore[name-defined]
        except Exception:
            pass

    def _dedupe_key(self, kind: str, params: Dict[str, Any]) -> str:
        try:
            s = kind + "|" + json.dumps(params, sort_keys=True, ensure_ascii=False)
            return hashlib.sha1(s.encode("utf-8")).hexdigest()[:16]
        except Exception:
            return f"{kind}|fallback"

    def _mk_rule(self, kind: str, payload: Dict[str, Any]):
        """Compatibility constructor for Rule; returns Rule or None."""
        R = globals().get("Rule")
        if R is None:
            self._safe_meta_log("rule_class_missing", kind=kind)
            return None
        # Prefer canonical from_any(kind, params=...)
        try:
            fa = getattr(R, "from_any", None)
            if callable(fa):
                return fa(kind, params=payload)
        except Exception:
            pass
        # Try various ctor shapes
        for attempt in (
            lambda: R(kind, payload),
            lambda: R(kind=kind, payload=payload),
            lambda: R(kind=kind, params=payload),
        ):
            try:
                return attempt()
            except Exception:
                continue
        self._safe_meta_log("rule_construct_failed", kind=kind)
        return None

    # ---------- translate fallback (only if upstream not present) ----------
    def _translate_fallback(self, a: "np.ndarray", dr: int, dc: int, pad_val: int = -999) -> "np.ndarray":
        R, C = a.shape
        out = np.full((R, C), int(pad_val), dtype=int)
        r0, c0 = max(0, dr), max(0, dc)
        r1, c1 = min(R, R + dr), min(C, C + dc)
        sr0, sc0 = max(0, -dr), max(0, -dc)
        sr1, sc1 = sr0 + (r1 - r0), sc0 + (c1 - c0)
        if r1 > r0 and c1 > c0:
            out[r0:r1, c0:c1] = a[sr0:sr1, sc0:sc1]
        return out

    # ---------- shared pulse ----------
    def _pulse(self, topic: str, **payload):
        self._safe_meta_log(topic, **payload)
        try:
            if self.meta is not None and hasattr(self.meta, "_log_telemetry"):
                self.meta._log_telemetry({"module": "RuleGenerator", "event": topic, **payload})
        except Exception:
            pass
        try:
            if self.ultra is not None and hasattr(self.ultra, "observe"):
                self.ultra.observe("rulegen_" + topic.replace(".", "_"), **payload)
        except Exception:
            pass

    def _tick_kairos(self, hint: float = 0.25):
        try:
            if self.kairos is None:
                return
            t = getattr(self.kairos, "phase_time", 0)
            self.kairos.step(t + 1)
            st = self.kairos.get_state()
            if st:
                self._pulse("kairos.tick", **st, hint=float(hint))
        except Exception:
            pass

    def _holo_echo(self, tag: str, payload: dict):
        try:
            if self.holo is None or not hasattr(self.holo, "add") or np is None:
                return
            _z = np.zeros((1, 1), dtype=int)
            self.holo.add(_z, _z, {"subject": "rulegen", "note": tag, **payload})
        except Exception:
            pass

    def _in_dir(self, name: str, ext: Optional[str] = None) -> str:
        base = os.path.join(self.export_dir, name)
        if ext and not base.endswith(f".{ext}"):
            base = f"{base}.{ext}"
        os.makedirs(os.path.dirname(base) or ".", exist_ok=True)
        return base

    def _kb_push(self, topic: str, **payload):
        try:
            if self.meta and hasattr(self.meta, "kb"):
                kb = self.meta.kb
                if hasattr(kb, "push_meta_stats") and callable(kb.push_meta_stats):
                    kb.push_meta_stats(topic, dict(payload))
                elif hasattr(kb, "append_log") and callable(kb.append_log):
                    kb.append_log(f"rulegen.{topic}", dict(payload))
        except Exception:
            pass

    def _apply_ops(self, arr: "np.ndarray", ops: list) -> "np.ndarray":
        # prefer sandbox if available
        sb = self.sandbox or (getattr(self.meta, "sandbox", None) if self.meta is not None else None)
        try:
            if sb is not None and hasattr(sb, "apply_ops"):
                return sb.apply_ops(arr, ops)
        except Exception:
            pass
        # fallback to geometry compose
        try:
            if 'compose_ops' in globals():
                return compose_ops(arr, [(op, kw) for op, kw in (ops or [])])  # type: ignore[name-defined]
        except Exception:
            pass
        return arr

    @staticmethod
    def _value_hist(x: "np.ndarray") -> dict:
        return dict(Counter(np.asarray(x).ravel().tolist()))

    @staticmethod
    def _is_symmetry(inp: "np.ndarray", out: "np.ndarray"):
        if inp.shape == out.shape and np.array_equal(np.rot90(inp, 1), out):
            return ("rot", {"k": 1})
        if inp.shape == out.shape and np.array_equal(np.rot90(inp, 2), out):
            return ("rot", {"k": 2})
        if inp.shape == out.shape and np.array_equal(np.rot90(inp, 3), out):
            return ("rot", {"k": 3})
        if inp.shape == out.shape and np.array_equal(np.fliplr(inp), out):
            return ("flip_lr", {})
        if inp.shape == out.shape and np.array_equal(np.flipud(inp), out):
            return ("flip_ud", {})
        return None

    def _best_translation(self, inp: "np.ndarray", out: "np.ndarray"):
        if inp.shape != out.shape:
            return None
        for dr in range(-self.max_shift, self.max_shift + 1):
            for dc in range(-self.max_shift, self.max_shift + 1):
                if dr == 0 and dc == 0:
                    continue
                try:
                    if 'translate' in globals():
                        shifted = translate(inp, dr=dr, dc=dc, pad_val=-999)  # type: ignore[name-defined]
                    else:
                        raise RuntimeError
                except Exception:
                    shifted = self._translate_fallback(inp, dr=dr, dc=dc, pad_val=-999)
                if np.array_equal(shifted, out):
                    return (dr, dc)
        return None

    def _nlg(self, rule_kind: str, base_conf: float, conf: float, inv: dict, phys: dict, extras: dict) -> str:
        parts = [f"Proposed {rule_kind.replace('_',' ')} (base {base_conf:.2f} → {conf:.2f})."]
        if inv:
            dH = inv.get("dH"); epi = inv.get("epi"); bc = inv.get("binder")
            if dH is not None: parts.append(f"ΔH={dH:+.3f}")
            if epi is not None: parts.append(f"EPI={epi:.3f}")
            if bc is not None: parts.append(f"BinderU={bc:.3f}")
        if phys:
            md = phys.get("mass_delta"); cs = phys.get("centroid_shift"); sd = phys.get("sym_delta")
            if md is not None: parts.append(f"massΔ={md:.3f}")
            if cs is not None: parts.append(f"centroidΔ={cs:.2f}")
            if sd is not None: parts.append(f"symΔ={sd:+.3f}")
        for k, v in (extras or {}).items():
            parts.append(f"{k}={v}")
        return " | ".join(parts)

    def _kairos_conf_mod(self, conf: float) -> float:
        try:
            if self.kairos is None: return conf
            st = self.kairos.get_state()
            flux = float(st.get("entropy_flux", 0.0))
            mod = 1.0 + 0.01 * np.tanh(abs(flux) / 10.0)
            return float(min(1.0, max(0.0, conf * mod)))
        except Exception:
            return conf

    def _keel_conf_mod(self, conf: float) -> float:
        try:
            ratio = None
            if self.holo is not None and hasattr(self.holo, "compression_ratio"):
                ratio = float(self.holo.compression_ratio)
            elif self.meta is not None and hasattr(self.meta, "holo") and hasattr(self.meta.holo, "compression_ratio"):
                ratio = float(self.meta.holo.compression_ratio)
            if ratio is None:
                return conf
            gain = 1.0 + 0.005 * np.tanh(ratio - 1.0)
            return float(min(1.0, max(0.0, conf * gain)))
        except Exception:
            return conf

    def _dynamic_confidence(self, rule_kind: str, base_conf: float, invariants: dict, physics: dict) -> float:
        conf = float(base_conf)
        try:
            dH = abs(float(invariants.get("dH", 0.0)))
            epi = float(invariants.get("epi", 0.0))
            bu = float(invariants.get("binder", 0.0))
            conf += self.inv_weight * (max(0.0, 0.10 - dH) + 0.05 * max(0.0, epi) + 0.05 * max(0.0, bu))
        except Exception:
            pass
        try:
            md = abs(float(physics.get("mass_delta", 0.0)))
            cs = abs(float(physics.get("centroid_shift", 0.0)))
            conf += self.phys_weight * (max(0.0, 0.10 - md) + max(0.0, 0.10 - min(cs / 10.0, 1.0)))
        except Exception:
            pass
        past = float(self.confidence_memory.get(rule_kind, base_conf))
        conf = (conf + past) / 2.0

        # Optional global calibration
        try:
            if callable(self._global_cal):
                conf = float(self._global_cal(conf))
            elif self._cal is not None and hasattr(self._cal, "predict"):
                conf = float(self._cal.predict(conf))
        except Exception:
            pass

        conf = self._kairos_conf_mod(conf)
        conf = self._keel_conf_mod(conf)
        conf = float(min(1.0, max(0.0, conf)))
        self.confidence_memory[rule_kind] = float(min(1.0, conf + self.reward_scale))
        return conf

    def _emit(self, topic: str, **payload):
        # EXPLAIN (optional)
        try:
            if 'EXPLAIN' in globals():
                EXPLAIN.log(f"nlg.{topic}", payload)  # type: ignore[name-defined]
        except Exception:
            pass
        self._kb_push(topic, **payload)
        self._pulse(topic, **payload)
        self._holo_echo(topic, payload)
        self._tick_kairos(hint=0.25)

    def make_rule(self, inp: "np.ndarray", out: "np.ndarray"):
        t0 = time.time()
        # Collect metrics (best-effort)
        hist_in, hist_out = self._value_hist(inp), self._value_hist(out)
        inv_metrics, phys_metrics = {}, {}
        try:
            if 'InvariantScorer' in globals():
                inv_metrics = InvariantScorer().score_pair(inp, out, out)  # type: ignore[name-defined]
        except Exception:
            pass
        try:
            if 'PhysicsHeuristics' in globals():
                phys_metrics = PhysicsHeuristics().score_pair(inp, out)  # type: ignore[name-defined]
        except Exception:
            pass

        def record(rule_kind, base, conf, extras=None, ops=None, rule_obj=None):
            # dedupe exact kind+params to avoid spam
            dk = self._dedupe_key(rule_kind, (getattr(rule_obj, "payload", {}) or {}))
            if dk in self._dedupe_seen:
                self._emit("rulegen.dup", rule_kind=rule_kind)
                return
            self._dedupe_seen.add(dk)

            self._emit("rulegen.make", rule_kind=rule_kind, confidence=conf, **(extras or {}), t=time.time())
            self._emit("rulegen.nlg", text=self._nlg(rule_kind, base, conf, inv_metrics, phys_metrics, extras or {}))

            # WAL append
            try:
                os.makedirs(os.path.dirname(self._wal_path) or ".", exist_ok=True)
                with open(self._wal_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "t": time.time(),
                        "kind": rule_kind,
                        "conf": float(conf),
                        "extras": extras or {},
                        "ops": ops or [],
                    }) + "\n")
            except Exception:
                pass

            # HoloMemory add (best-effort)
            try:
                if self.holo is not None and hasattr(self.holo, "add"):
                    tag = {"subject": "rulegen", "confidence": float(conf), "rule_kind": rule_kind}
                    if ops: tag["ops"] = ops
                    self.holo.add(inp, out, tag)
            except Exception as e:
                self._safe_meta_log("holo.add_fail", site="rulegen.record", error=str(e))

            # Encoder reinforcement (optional)
            try:
                if self.encoder is not None and hasattr(self.encoder, "record_feedback"):
                    self.encoder.record_feedback(label=f"rule_{rule_kind}", memory_layer="rules", success=True)
            except Exception:
                pass

            # bump OP priors if ops present
            try:
                if ops and isinstance(ops, list) and '_update_op_rank' in globals():
                    for op, _kw in ops:
                        try:
                            _update_op_rank(op, reward=+0.5)  # type: ignore[name-defined]
                        except Exception:
                            pass
            except Exception:
                pass

            # Rulebase wiring (optional)
            try:
                rb = self.rulebase or (getattr(self.meta, "rulebase", None) if self.meta is not None else None)
                RR = globals().get("RuleRecord")
                if rb is not None and hasattr(rb, "add") and rule_obj is not None:
                    if RR is not None:
                        rb.add(RR(inp, out, rule_obj, {"source": "rulegen"}))
                    else:
                        rb.add({
                            "input_grid": inp.tolist(),
                            "output_grid": out.tolist(),
                            "rule": {"kind": rule_obj.kind, "params": rule_obj.payload},
                            "meta": {"source": "rulegen"}
                        })
                    self._emit("rulegen.rule_added", rule_kind=rule_kind)
            except Exception:
                pass

            # provenance mix pulse
            try:
                src = (extras or {}).get("source", "rulegen")
                self._src_counter[src] += 1
                if (sum(self._src_counter.values()) % 25) == 0:
                    self._emit("rulegen.source_mix", mix=dict(self._src_counter))
            except Exception:
                pass

        # identity
        if inp.shape == out.shape and np.array_equal(inp, out):
            base = 1.0
            conf = self._dynamic_confidence("identity", base, inv_metrics, phys_metrics)
            rule = self._mk_rule("identity", {"confidence": conf})
            if rule is not None:
                record("identity", base, conf, rule_obj=rule)
                self._emit("rulegen.make_rule_timing", ms=int(1000*(time.time()-t0)))
                return rule

        # symmetry
        sym = self._is_symmetry(inp, out)
        if sym is not None:
            op, params = sym; base = 0.90
            conf = self._dynamic_confidence("symmetry_map", base, inv_metrics, phys_metrics)
            payload = {"ops": [(op, params)], "confidence": conf}
            rule = self._mk_rule("symmetry_map", payload)
            if rule is not None:
                record("symmetry_map", base, conf, {"op": op}, ops=payload["ops"], rule_obj=rule)
                self._emit("rulegen.make_rule_timing", ms=int(1000*(time.time()-t0)))
                return rule

        # translation
        shift = self._best_translation(inp, out)
        if shift is not None:
            dr, dc = shift; base = 0.85
            conf = self._dynamic_confidence("component_relocate", base, inv_metrics, phys_metrics)
            payload = {"ops": [("trans", {"dr": dr, "dc": dc, "pad_val": 0})], "confidence": conf}
            rule = self._mk_rule("component_relocate", payload)
            if rule is not None:
                record("component_relocate", base, conf, {"dr": dr, "dc": dc}, ops=payload["ops"], rule_obj=rule)
                self._emit("rulegen.make_rule_timing", ms=int(1000*(time.time()-t0)))
                return rule

        # majority recolor
        if 'pad_to_same_shape' in globals():
            try:
                ii, oo = pad_to_same_shape(inp, out, -1)  # type: ignore[name-defined]
            except Exception:
                ii, oo = inp, out
        else:
            ii, oo = inp, out
        valid_out_vals = oo[oo != -1] if np is not None else []
        if np is not None and hasattr(valid_out_vals, 'size') and valid_out_vals.size > 0:
            out_cnt = Counter(int(v) for v in valid_out_vals.ravel().tolist())
            default_val = out_cnt.most_common(1)[0][0]
            uniq_in = sorted(set(int(v) for v in ii[ii != -1].ravel().tolist())) if np is not None else []
            co = defaultdict(Counter)
            for a, b in zip(ii.ravel().tolist(), oo.ravel().tolist()):
                ai, bi = int(a), int(b)
                if ai != -1 and bi != -1:
                    co[ai][bi] += 1
            mapping = {u: (int(co[u].most_common(1)[0][0]) if co[u] else int(default_val)) for u in uniq_in}
            if any(mapping.get(v, v) != v for v in uniq_in):
                base = 0.82
                conf = self._dynamic_confidence("majority_recolor", base, inv_metrics, phys_metrics)
                rule = self._mk_rule("majority_recolor", {"map": mapping, "confidence": conf})
                if rule is not None:
                    record("majority_recolor", base, conf, {"map_size": len(mapping)}, rule_obj=rule)
                    self._emit("rulegen.make_rule_timing", ms=int(1000*(time.time()-t0)))
                    return rule

        # delta
        if 'pad_to_same_shape' in globals():
            try:
                ii2, oo2 = pad_to_same_shape(inp, out, -1)  # type: ignore[name-defined]
            except Exception:
                ii2, oo2 = inp, out
        else:
            ii2, oo2 = inp, out
        try:
            delta = (oo2 - ii2).tolist()
        except Exception:
            delta = []
        overlap = set(hist_in).intersection(hist_out)
        if len(overlap) <= max(1, int(0.2 * max(len(hist_in), len(hist_out)))):
            base = 0.60
            conf = self._dynamic_confidence("delta", base, inv_metrics, phys_metrics)
            payload = {"delta": delta, "target_shape": tuple(out.shape), "confidence": conf}
            rule = self._mk_rule("delta", payload)
            if rule is not None:
                record("delta", base, conf, {"target_shape": tuple(out.shape)}, rule_obj=rule)
                self._emit("rulegen.make_rule_timing", ms=int(1000*(time.time()-t0)))
                return rule

        # object_map fallback
        uniq_in = sorted(set(np.asarray(inp).ravel().tolist()))
        out_cnt2 = Counter(np.asarray(out).ravel().tolist())
        default_val2 = out_cnt2.most_common(1)[0][0]
        mapping2 = {int(u): int(default_val2) for u in uniq_in}
        try:
            p_in = np.array(list(hist_in.values()), dtype=float); p_in /= p_in.sum() if p_in.sum() else 1.0
            p_out = np.array(list(hist_out.values()), dtype=float); p_out /= p_out.sum() if p_out.sum() else 1.0
            ent_in = -np.sum(p_in * np.log2(p_in + 1e-9))
            ent_out = -np.sum(p_out * np.log2(p_out + 1e-9))
            complexity = float(abs(ent_out - ent_in))
        except Exception:
            complexity = 0.0
        base = 0.80
        conf = self._dynamic_confidence("object_map", base, inv_metrics, phys_metrics)
        rule = self._mk_rule("object_map", {"map": mapping2, "confidence": conf, "complexity": complexity})
        if rule is not None:
            record("object_map", base, conf, {"complexity": round(complexity, 3)}, rule_obj=rule)
            self._emit("rulegen.make_rule_timing", ms=int(1000*(time.time()-t0)))
            return rule

        # If we get here, we failed to construct any Rule
        self._safe_meta_log("rulegen.no_rule_constructed")
        self._emit("rulegen.make_rule_timing", ms=int(1000*(time.time()-t0)))
        return None

    def apply(self, inp: "np.ndarray", rule, *, context: Optional[dict] = None) -> "np.ndarray":
        # Accept either Rule object with .kind/.payload or a dict {"rule":{"kind","params"}}
        if rule is None:
            return inp.copy()
        try:
            kind = getattr(rule, "kind", None)
            payload = getattr(rule, "payload", None)
            if kind is None and isinstance(rule, dict):
                kind = rule.get("rule", {}).get("kind")
                payload = rule.get("rule", {}).get("params", {})
        except Exception:
            kind, payload = None, None
        if kind is None:
            return inp.copy()

        if kind in ("xform", "symmetry_map", "component_relocate"):
            pred = self._apply_ops(inp, (payload or {}).get("ops", []))
        elif kind == "delta":
            dd = np.array((payload or {}).get("delta", []), dtype=int)
            if 'pad_to_same_shape' in globals():
                try:
                    ii, d2 = pad_to_same_shape(inp, dd, -1)  # type: ignore[name-defined]
                except Exception:
                    ii, d2 = inp, dd
            else:
                ii, d2 = inp, dd
            try:
                pred = ii + d2
            except Exception:
                pred = inp.copy()
            ts = tuple((payload or {}).get("target_shape", pred.shape))
            # Gate target-shape usage by policy (submission-safe)
            allow = True
            try:
                if '_allow_target_shape_use' in globals():
                    allow = bool(_allow_target_shape_use())  # type: ignore[name-defined]
            except Exception:
                pass
            if allow and 'center_crop' in globals():
                try:
                    pred = center_crop(pred, ts)  # type: ignore[name-defined]
                except Exception:
                    pass
        elif kind in ("object_map", "majority_recolor"):
            mp = (payload or {}).get("map", {})
            vec = np.vectorize(lambda x: int(mp.get(int(x), int(x))), otypes=[int])
            pred = vec(inp).astype(int, copy=False)
        elif kind == "identity":
            pred = inp.copy()
        else:
            pred = inp.copy()

        # HoloMemory add for applied prediction (best-effort)
        try:
            if self.holo is not None and hasattr(self.holo, "add"):
                self.holo.add(
                    inp, pred,
                    {
                        "subject": "rulegen.apply",
                        "confidence": float((payload or {}).get("confidence", 0.5)),
                        "rule_kind": kind,
                        "ops": (payload or {}).get("ops"),
                        **({"task_id": (context or {}).get("task_id")} if context else {}),
                    },
                )
        except Exception as e:
            self._safe_meta_log("holo.add_fail", site="rulegen.apply", error=str(e))

        # Optional Meta-layer recall/evaluate assist (if wired)
        try:
            meta_layer = None
            if 'runtime' in globals() and isinstance(runtime, dict):  # type: ignore[name-defined]
                meta_layer = runtime.get("meta")  # type: ignore
            if meta_layer is None:
                meta_layer = getattr(self.meta, "meta", None) or self.meta
            if meta_layer is not None and hasattr(meta_layer, "holo_recall_then_evaluate"):
                evaluator = getattr(meta_layer, "hybrid_eval", None)
                recall_hit = meta_layer.holo_recall_then_evaluate(
                    inp,
                    evaluator,
                    topk=3,
                    subject=(context or {}).get("task_id", "generic"),
                    task_id=(context or {}).get("task_id"),
                    train_index=(context or {}).get("train_index"),
                )
                if recall_hit is not None:
                    pred2, _hmeta, _sc = recall_hit
                    if pred2 is not None:
                        pred = pred2
        except Exception:
            pass

        return pred

    def pretty_print(self, rules: list):
        print("\n=== Rule Summary ===")
        for r in rules:
            try:
                kind = getattr(r, "kind", None)
                payload = getattr(r, "payload", None)
                if kind is None and isinstance(r, dict):
                    kind = r.get("rule", {}).get("kind", "unknown")
                    payload = r.get("rule", {}).get("params", {})
                extras = {k: v for k, v in (payload or {}).items() if k not in ("confidence",)}
                try:
                    conf = float((payload or {}).get('confidence', 0.0))
                except Exception:
                    conf = 0.0
                print(f" {kind} | conf={conf:.2f} | extras={extras}")
            except Exception as e:
                print(f" [malformed rule: {e}]")
        print("====================\n")
        self._emit("rulegen.pretty_print", n_rules=len(rules), t=time.time())

    def export_rule_effect(self, inp: "np.ndarray", out: "np.ndarray", prefix="rule_effect"):
        try:
            rule = self.make_rule(inp, out)
            pred = self.apply(inp, rule)
            png_path = self._in_dir(prefix, "png")
            if 'save_card_triptych' in globals():
                try:
                    save_card_triptych(inp, pred, out, png_path, title=f"{getattr(rule, 'kind', 'unknown')} conf={(getattr(rule, 'payload', {}) or {}).get('confidence', 0):.2f}")  # type: ignore[name-defined]
                except Exception:
                    pass
            js_path = self._in_dir(prefix, "json")
            with open(js_path, "w", encoding="utf-8") as f:
                rk = getattr(rule, "kind", None)
                rp = getattr(rule, "payload", None)
                if rk is None and isinstance(rule, dict):
                    rk = rule.get("rule", {}).get("kind")
                    rp = rule.get("rule", {}).get("params", {})
                json.dump({"rule_kind": rk, "payload": rp}, f, indent=2)
            kind_path = self._in_dir(prefix, "kind.txt")
            with open(kind_path, "w", encoding="utf-8") as ftxt:
                ftxt.write(str(getattr(rule, "kind", "unknown")) + "\n")
            self._emit("rulegen.export_rule_effect", rule_kind=getattr(rule, "kind", "unknown"), file=prefix, t=time.time())
            self._kb_push("export_rule_effect", rule_kind=getattr(rule, "kind", "unknown"), path=prefix)
        except Exception as e:
            self._emit("rulegen.export_rule_effect_error", error=str(e), t=time.time())

    def export_rule_confidence_hist(self, rules: list, path="rule_conf_hist.png"):
        try:
            import matplotlib.pyplot as plt
        except Exception:
            self._emit("rulegen.export_conf_hist_error", error="matplotlib_unavailable", t=time.time())
            return
        try:
            confs = []
            for r in rules:
                try:
                    payload = getattr(r, "payload", None)
                    if payload is None and isinstance(r, dict):
                        payload = r.get("rule", {}).get("params", {})
                    confs.append(float((payload or {}).get("confidence", 0.0)))
                except Exception:
                    confs.append(0.0)
            out_path = self._in_dir(path)
            plt.figure(figsize=(6, 4))
            plt.hist(confs, bins=15, alpha=0.75)
            plt.title("Rule Confidence Distribution")
            plt.xlabel("confidence"); plt.ylabel("count")
            plt.tight_layout(); plt.savefig(out_path); plt.close()
            self._emit("rulegen.export_conf_hist", n_rules=len(rules), path=out_path, t=time.time())
            self._kb_push("export_conf_hist", n_rules=len(rules), path=out_path)
        except Exception as e:
            self._emit("rulegen.export_conf_hist_error", error=str(e), t=time.time())

# ----------------------------------------------------------------
# Explanation Logger  
# ----------------------------------------------------------------
class ExplanationLogger:
    def __init__(self, path: str = "explanations.jsonl", max_events: int = 200_000):
        self.path = path
        self.max_events = int(max_events)
        self.count = 0
        self.lock = threading.Lock()

    def _compress_if_large(self, path: str):
        try:
            if not os.path.isfile(path):
                return
            size = os.path.getsize(path)
            if size < KEEL_COMPRESS_THRESHOLD:
                return
            ts = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
            os.makedirs("deployment", exist_ok=True)
            gz_out = f"deployment/explanations-{ts}.jsonl.gz"

            # prefer KEEL if the global compression helper exists
            keel = globals().get("keel_compress_file", None)
            if keel:
                keel(path, gz_out)
            else:
                with open(path, "rb") as fin, gzip.open(gz_out, "wb") as fout:
                    shutil.copyfileobj(fin, fout)
            # rotate
            with open(path, "w", encoding="utf-8") as f:
                f.write("")
            meta_log("explain.rotate", path=gz_out, bytes=size)
        except Exception as e:
            meta_log("explain.rotate_fail", error=str(e))

    def log(self, event_type: str, payload: dict):
        with self.lock:
            if self.count >= self.max_events:
                return
            try:
                rec = {"t": time.time(), "type": str(event_type)}
                if isinstance(payload, dict):
                    rec.update(payload)
                os.makedirs(os.path.dirname(self.path) or ".", exist_ok=True)
                with open(self.path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(rec) + "\n")
                self.count += 1
                # opportunistic compression
                self._compress_if_large(self.path)
            except Exception:
                pass

EXPLAIN = ExplanationLogger(path="exports/meta/explanations.jsonl")

# ------------------------------
# Explanations export (atomic, robust)
# ------------------------------

def export_explanations_summary(
    nl_path: str = "deployment/explanations_report.txt",
    csv_path: str = "deployment/explanations_event_counts.csv",
):
    src = "explanations.jsonl"
    ok_nl = False
    ok_csv = False
    n_events = None
    notes: List[str] = []

    # --- helpers ---
    def _safe_makedirs(path: str):
        d = os.path.dirname(path)
        if d:
            try:
                os.makedirs(d, exist_ok=True)
            except Exception:
                pass

    def _telemetry_pulse(ok: bool, where: str, **payload):
        # EXPLAIN bus
        try:
            if 'EXPLAIN' in globals():
                EXPLAIN.log("explain.export_success" if ok else "explain.export_error", {"where": where, **payload})  # type: ignore[name-defined]
        except Exception:
            pass
        # meta_log
        try:
            if 'meta_log' in globals():
                meta_log("explain.export", where=where, ok=ok, **payload)  # type: ignore[name-defined]
        except Exception:
            pass
        # Ultra / Kairos / Holo taps
        try:
            u = globals().get("ultra")
            k = globals().get("kairos")
            h = globals().get("holo")
            if u is not None and hasattr(u, "observe"):
                u.observe("explain_export", where=where, ok=ok, **payload)
            if k is not None and hasattr(k, "step"):
                k.step(time_step=1)
            if h is not None and hasattr(h, "add") and np is not None:
                z = np.zeros((1, 1), dtype=int)
                h.add(z, z, {"subject": "explain_export", "where": where, "ok": ok, **payload})
        except Exception:
            pass

    # --- prechecks ---
    has_src = False
    try:
        has_src = os.path.isfile(src) and os.path.getsize(src) > 0
    except Exception:
        has_src = False

    # --- Natural language report export (atomic) ---
    _safe_makedirs(nl_path)
    try:
        report_text = "No explanations generated.\n"
        reporter = None
        try:
            if 'NaturalLanguageReporter' in globals():
                reporter = NaturalLanguageReporter(src)  # type: ignore[name-defined]
        except Exception as inner_e:
            notes.append(f"reporter_unavailable:{inner_e}")
        if has_src and reporter is not None and hasattr(reporter, "summarize_session"):
            try:
                report_text = reporter.summarize_session(limit=5000) or report_text
            except Exception as inner_e:
                notes.append(f"summarize_failed:{inner_e}")
        tmp_nl = f"{nl_path}.tmp"
        with open(tmp_nl, "w", encoding="utf-8") as f:
            f.write(report_text)
        os.replace(tmp_nl, nl_path)
        ok_nl = True
        _telemetry_pulse(True, "nl", path=nl_path)
    except Exception as e:
        notes.append(f"nl_error:{e}")
        _telemetry_pulse(False, "nl", error=str(e))

    # --- CSV metrics export (atomic) ---
    _safe_makedirs(csv_path)
    try:
        if has_src:
            try:
                with open(src, "r", encoding="utf-8") as f:
                    n_events = sum(1 for _ in f)
            except Exception as cnt_e:
                notes.append(f"count_failed:{cnt_e}")
            exporter = globals().get("export_explanation_event_counts", None)
            if callable(exporter):
                tmp_csv = f"{csv_path}.tmp"
                exporter(src, tmp_csv)  # type: ignore
                os.replace(tmp_csv, csv_path)
                ok_csv = True
                _telemetry_pulse(True, "csv", path=csv_path, n_events=n_events if n_events is not None else -1)
            else:
                notes.append("csv_exporter_missing")
                _telemetry_pulse(False, "csv", error="exporter_missing")
        else:
            notes.append("no_source_jsonl")
            _telemetry_pulse(False, "csv", error="no_source_jsonl")
    except Exception as e:
        notes.append(f"csv_error:{e}")
        _telemetry_pulse(False, "csv", error=str(e))

    # --- Final unified pulse & return summary ---
    try:
        if 'meta_log' in globals():
            meta_log(
                "explain.export.complete",
                ok=bool(ok_nl and (ok_csv or (not has_src))),
                nl_path=nl_path,
                csv_path=csv_path,
                n_events=n_events,
                notes=";".join(notes),
            )  # type: ignore[name-defined]
    except Exception:
        pass
    try:
        u = globals().get("ultra"); k = globals().get("kairos"); h = globals().get("holo")
        if u is not None and hasattr(u, "observe"):
            u.observe(
                "explain_export_complete",
                ok_nl=ok_nl,
                ok_csv=ok_csv,
                has_src=has_src,
                n_events=n_events,
                nl_path=nl_path,
                csv_path=csv_path,
            )
        if k is not None and hasattr(k, "step"):
            k.step(time_step=1)
        if h is not None and hasattr(h, "add") and np is not None:
            z = np.zeros((1, 1), dtype=int)
            h.add(
                z,
                z,
                {
                    "subject": "explain_export_complete",
                    "ok_nl": ok_nl,
                    "ok_csv": ok_csv,
                    "has_src": has_src,
                    "n_events": n_events,
                    "nl_path": nl_path,
                    "csv_path": csv_path,
                },
            )
    except Exception:
        pass

    return {
        "ok": bool(ok_nl and (ok_csv or (not has_src))),
        "nl_path": nl_path,
        "csv_path": csv_path,
        "has_source": has_src,
        "n_events": n_events,
        "notes": notes,    }

# ------------------------------------------------------
# Natural Language Reporter  (compat vocab + safe emits)
# ------------------------------------------------------
class NaturalLanguageReporter:
    def __init__(self, source_path="explanations.jsonl"):
        self.source_path = source_path

    def _load(self):
        if not os.path.isfile(self.source_path):
            return []
        out = []
        with open(self.source_path, encoding="utf-8") as f:
            for line in f:
                try:
                    out.append(json.loads(line))
                except Exception:
                    pass
        return out

    def summarize_session(self, limit=4000) -> str:
        ev = self._load()[-limit:]
        lines = []

        # Compatibility map: accept both legacy and unified topics
        def pick(k):  # helper to select by type key
            return [e for e in ev if e.get("type") == k]

        picks  = pick("hybrid.pick") or pick("hybrid.rank") or pick("hybrid.rescore")
        resc   = pick("hybrid.rescore") or pick("hybrid.rank")
        invs   = pick("invariants.score") or pick("collapse.invariants")
        cls    = pick("encoder.classify")
        prn    = pick("encoder.prune")

        if picks:
            lines.append(f"Hybridized {len(picks)} predictions with physics invariants + similarity.")
        if resc:
            lines.append(f"Re-ranked {sum(int(e.get('n_candidates',0)) for e in resc)} candidates across {len(resc)} decisions.")
        if invs:
            lines.append(f"Evaluated invariants {len(invs)} times (ΔH/EPI/Binder).")
        if cls:
            lines.append(f"Observed {len(cls)} encoder classifications (telemetry).")
        if prn:
            removed = sum(int(e.get('removed', 0)) for e in prn)
            lines.append(f"Encoder pruned {removed} patterns across session.")

        for e in (picks[:5] if picks else []):
            lines.append(
                f"Pick → mode={e.get('mode')} sim={e.get('sim_score',0):.3f} "
                f"dH={e.get('dH',0):+.3f} EPI={e.get('epi',0):.3f} Bu={e.get('binder',0):+.3f} "
                f"→ blended={e.get('blended',0):.3f}"
            )

        try:
            _safe_emit("report.summary", {
                "kairos_flux": float(getattr(kairos, "last_entropy_flux", 0.0)),
                "keel_ratio":  float(getattr(kairos, "keel_ratio_avg", 1.0))
            })
        except Exception:
            pass
        return "\n".join(lines) if lines else "No hybrid decisions recorded."


def export_explanation_event_counts(jsonl_path="explanations.jsonl",
                                    csv_path="explanations_event_counts.csv"):   
    counts = {}
    prune_removed = 0
    prune_label_set = set()
    evolve_rows = 0
    evolve_success = 0
    evolve_fail = 0
    classify_winner_counts = {}

    if not os.path.isfile(jsonl_path):
        os.makedirs(os.path.dirname(csv_path) or ".", exist_ok=True)
        with open(csv_path, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f); w.writerow(["metric", "value"]); w.writerow(["no_events", 1])
        try:
            _safe_emit("report.csv_empty", {})
        except Exception:
            pass
        return

    with open(jsonl_path, encoding="utf-8") as f:
        for line in f:
            try:
                e = json.loads(line)
            except Exception:
                continue
            et = e.get("type", "unknown")
            counts[et] = counts.get(et, 0) + 1
            if et == "encoder.prune":
                prune_removed += int(e.get("removed", 0))
                for lab in (e.get("labels", []) or []):
                    prune_label_set.add(lab)
            elif et == "encoder.evolve":
                evolve_rows += int(e.get("n_rows", 0))
                if e.get("success") is True:
                    evolve_success += 1
                elif e.get("success") is False:
                    evolve_fail += 1
            elif et in ("encoder.classify",):
                wlab = e.get("winner", "Unclassified")
                classify_winner_counts[wlab] = classify_winner_counts.get(wlab, 0) + 1
            # compat for unified invariants/hybrid topics if present
            elif et in ("collapse.invariants", "hybrid.rank", "hybrid.rescore", "hybrid.pick"):
                pass  # still counted above; no extras needed

    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["metric", "value"])
        for k, v in sorted(counts.items()):
            w.writerow([f"count.{k}", int(v)])
        w.writerow(["prune.total_removed", int(prune_removed)])
        w.writerow(["prune.unique_labels", int(len(prune_label_set))])
        w.writerow(["evolve.total_rows", int(evolve_rows)])
        w.writerow(["evolve.success", int(evolve_success)])
        w.writerow(["evolve.fail", int(evolve_fail)])
        for lab, n in sorted(classify_winner_counts.items(), key=lambda kv: -kv[1])[:20]:
            w.writerow([f"classify.winner.{lab}", int(n)])
    try:
        _safe_emit("report.csv_exported", {
            "kairos_flux": float(getattr(kairos, "last_entropy_flux", 0.0)),
            "keel_ratio":  float(getattr(kairos, "keel_ratio_avg", 1.0)),
            "path": csv_path
        })
    except Exception:
        pass
    _compress_if_large(csv_path)

# ----------------------
# Build runtime (unified) — FIXED
# ----------------------

def build_runtime(run_id: str = "na", phase: str = "train") -> Dict[str, Any]:
    import os, json, time, csv, hashlib, inspect

# --- local helpers -------------------------------------------------------
    health: Dict[str, Any] = {"warnings": [], "errors": [], "fallbacks": []}
    runtime: Dict[str, Any] = {"health": health, "run_id": run_id, "phase": phase}

    def _emit(topic: str, payload: Dict[str, Any]) -> None:
        try:
            fn = globals().get("_safe_emit", None)
            if callable(fn):
                fn(topic, payload)
                return
            # Fallback: route to meta_log if available (spread kwargs)
            meta_log_fn = globals().get("meta_log", None)
            if callable(meta_log_fn):
                meta_log_fn(topic, **payload)
        except Exception:
            # Never break bootstrap due to telemetry
            pass

    def _warn(msg: str, **kv) -> None:
        health["warnings"].append(msg)
        _emit("runtime.warn", {"msg": msg, **kv})

    def _safe_setattr(obj, name, value) -> None:
        if obj is None:
            return
        try:
            setattr(obj, name, value)
        except Exception:
            pass


    # --- route EXPLAIN into meta_log (guarded) -------------------------------
    try:
        EXPLAIN = globals().get("EXPLAIN", None)
        set_meta_log_event = globals().get("set_meta_log_event", None)
        if EXPLAIN is not None and hasattr(EXPLAIN, "log") and callable(set_meta_log_event):
            set_meta_log_event(EXPLAIN.log)  # type: ignore[arg-type]
            _emit("runtime.explain_routed", {"ok": True, "has_EXPLAIN": True, "has_setter": True})
        else:
            # Expected if EXPLAIN plumbing is not compiled-in
            _emit("runtime.expected_missing", {
                "component": "EXPLAIN_route",
                "has_EXPLAIN": bool(EXPLAIN),
                "has_setter": bool(callable(set_meta_log_event)),
            })
    except Exception as e:
        _warn("EXPLAIN routing error", error=str(e))

    # --- Kairos (single authority) ------------------------------------------
    try:
        kairos = globals().get("kairos", None)  # reuse singleton if present
        if kairos is None:
            KairosCls = (globals().get("KairosPulseManager")
                         or globals().get("KairosTemporalEngine")
                         or globals().get("Kairos"))
            if KairosCls:
                try:
                    kairos = KairosCls(run_id=run_id, phase=phase)
                except Exception:
                    kairos = KairosCls()
        globals()["kairos"] = kairos  # legacy alias / singleton
    except Exception as e:
        kairos = None
        _warn("Kairos missing", error=str(e))
    runtime["kairos"] = kairos

    # --- KB / Rulebase -------------------------------------------------------
    kb = None
    try:
        KBCls = globals().get("SymbolicKB")
        kb = KBCls() if KBCls else None
    except Exception as e:
        _warn("SymbolicKB missing", error=str(e))
    runtime["kb"] = kb

    rulebase = None
    try:
        RBCls = globals().get("RuntimeRulebase") or globals().get("Rulebase")
        rulebase = RBCls() if RBCls else None
        if kb and not getattr(kb, "rulebase", None):
            try:
                kb.rulebase = rulebase  # best-effort link
            except Exception:
                pass
    except Exception as e:
        _warn("Rulebase missing", error=str(e))
    runtime["rulebase"] = rulebase

    
    # Runtime adaptation (if an older Rule class is already loaded)
    try:   
        _RuleType = Rule  
    except Exception:
        pass
    else:    
        if hasattr(_RuleType, "payload") and not hasattr(_RuleType, "params"):
            try:            
                def _get_params(self): return getattr(self, "payload")
                def _set_params(self, v): setattr(self, "payload", _as_dict(v))
                setattr(_RuleType, "params", property(_get_params, _set_params))
            except Exception:           
                pass

    # --- Meta / Holo / Curiosity --------------------------------------------
    meta = None
    try:
        MetaCls = globals().get("MetaLayer") or globals().get("Meta")
        meta = MetaCls(kb=kb, rulebase=rulebase, run_id=run_id) if MetaCls else None
    except Exception as e:
        _warn("Meta missing", error=str(e))
    runtime["meta"] = meta

    holo = None
    try:
        HoloCls = globals().get("HoloMemory") or globals().get("Holo")
        holo = HoloCls(run_id=run_id) if HoloCls else None
    except Exception as e:
        _warn("Holo missing", error=str(e))
    runtime["holo"] = holo

    curiosity = None
    try:
        CurCls = globals().get("CuriosityEngine") or globals().get("Curiosity")
        curiosity = CurCls(meta=meta) if CurCls else None
    except Exception as e:
        _warn("Curiosity missing", error=str(e))
    runtime["curiosity"] = curiosity

    # --- Similarity / Blender / Encoder -------------------------------------
    sim = None
    try:
        SimCls = globals().get("HybridSimilarity") or globals().get("Similarity")
        sim = SimCls(meta=meta) if SimCls else None
    except Exception as e:
        _warn("HybridSimilarity missing", error=str(e))
    runtime["sim"] = sim

    blender = None
    try:
        BlenderCls = globals().get("HybridConfidenceBlender") or globals().get("ConfidenceBlender")
        blender = BlenderCls() if BlenderCls else None
    except Exception as e:
        _warn("HybridConfidenceBlender missing", error=str(e))
    runtime["blender"] = blender

    encoder = None
    try:
        EncCls = globals().get("Encoder") or globals().get("SymbolicEncoder")
        if EncCls:
            encoder = EncCls()
        else:
            # Encoder is optional; do not raise warnings — log as expected absence
            _emit("runtime.expected_missing", {"component": "Encoder"})
    except Exception as e:
        # Encoder is optional; log as expected absence with reason
        _emit("runtime.expected_missing", {"component": "Encoder", "reason": str(e)})
        encoder = None
    runtime["encoder"] = encoder

    # --- Sandbox -------------------------------------------------------------
    sandbox = None
    try:
        SBXCls = globals().get("SandboxExplorer") or globals().get("Sandbox")
        if SBXCls:
            mdepth = globals().get("SANDBOX_MAX_DEPTH", 4)
            sandbox = SBXCls(kb=kb, max_depth=mdepth, meta=None, run_id=run_id)
    except Exception as e:
        _warn("Sandbox missing", error=str(e))
    runtime["sandbox"] = sandbox

    # --- ULTRA observer ------------------------------------------------------
    ULTRA = None
    try:
        UltraCls = globals().get("ARCSymbolicUltra") or globals().get("Ultra")
        if UltraCls:
            try:
                ULTRA = UltraCls(meta=meta, kb=kb, sandbox=sandbox, sim=sim,
                                 blender=blender, holo=holo, curiosity=curiosity,
                                 encoder=encoder, kairos=kairos)
            except Exception:
                # permissive no-kwargs constructor path
                ULTRA = UltraCls()
        else:
            _warn("ULTRA missing")
    except Exception as e:
        _warn("ULTRA construction failed", error=str(e))
        ULTRA = None
    runtime["ultra"] = ULTRA
    globals()["ultra"] = ULTRA  # legacy alias

    try:
        if ULTRA and hasattr(ULTRA, "set_phase"):
            ULTRA.set_phase(phase)
        _emit("ultra.ready", {"has_ultra": bool(ULTRA), "phase": phase})
        if ULTRA and hasattr(ULTRA, "observe"):
            ULTRA.observe("runtime.build", {"phase": phase, "run_id": run_id})
    except Exception:
        pass

    # --- Cross-link context where supported ---------------------------------
    _safe_call(rulebase, "set_context", meta=meta, kb=kb, sandbox=sandbox, sim=sim, holo=holo, ultra=ULTRA)
    _safe_call(sim,      "set_context", meta=meta, kb=kb, sandbox=sandbox, rulebase=rulebase, holo=holo, ultra=ULTRA)
    _safe_call(sandbox,  "set_context", meta=meta, kb=kb, rulebase=rulebase, sim=sim, holo=holo, ultra=ULTRA)
    _safe_call(meta,     "set_context", kb=kb, rulebase=rulebase, sandbox=sandbox, sim=sim, holo=holo, ultra=ULTRA)
    _safe_call(holo,     "set_context", meta=meta, kb=kb)
    _safe_call(curiosity,"set_context", meta=meta, kb=kb, sandbox=sandbox)

    # Verify rulebase exists and context has been set (best-effort)
    rulebase_exists = rulebase is not None
    runtime["rulebase_exists"] = rulebase_exists
    _emit("runtime.rulebase_context_ok", {"exists": rulebase_exists})

    # --- Build/attach solver -------------------------------------------------
    solver = None
    fatal_detail: Dict[str, Any] = {}
    attach_path = None

    try:
        build_solver = globals().get("build_solver", None)
        make_solver  = globals().get("make_solver", None)

        if callable(build_solver):
            sig = inspect.signature(build_solver)
            kw = {
                "ULTRA": ULTRA, "meta": meta, "kb": kb, "sandbox": sandbox, "sim": sim,
                "blender": blender, "holo": holo, "kairos": kairos, "run_id": run_id, "phase": phase
            }
            kw = {k: v for k, v in kw.items() if k in sig.parameters}
            solver = build_solver(**kw)  # type: ignore[misc]
            fatal_detail["branch"] = "build_solver"
        elif callable(make_solver):
            solver = make_solver(ULTRA)
            fatal_detail["branch"] = "make_solver"
        elif ULTRA is not None and (hasattr(ULTRA, "predict") or hasattr(ULTRA, "solve")):
            class _UltraSolverAdapter:
                def __init__(self, ultra, fallback_rulebase):
                    self.ultra = ultra
                    # pass-through attrs for hybrid attach and context users
                    for name in ("meta", "kb", "sandbox", "sim", "blender", "holo", "curiosity", "encoder", "kairos", "rulebase"):
                        val = getattr(ultra, name, None)
                        if val is None and name == "rulebase":
                            val = fallback_rulebase
                        _safe_setattr(self, name, val)

                def predict(self, x):  # minimal adapter
                    if hasattr(self.ultra, "predict"):
                        return self.ultra.predict(x)
                    return self.ultra.solve(x)

                def solve_task(self, task):
                    return self.predict(task)

            solver = _UltraSolverAdapter(ULTRA, rulebase)
            fatal_detail["branch"] = "_UltraSolverAdapter"
        else:
            fatal_detail = {"branch": "solver_missing", "hint": "No solver and ULTRA lacks predict/solve"}

    except Exception as e:
        solver = None
        fatal_detail = {"branch": "constructor_error", "error": str(e)}

    runtime["solver"] = solver

    # Attach hybrid to solver (helper or soft)
    try:
        if solver is not None:
            helper = globals().get("attach_hybrid_to_solver", None)
            if callable(helper):
                helper(solver, sim=sim, blender=blender)  # type: ignore[misc]
                attach_path = "helper"
            else:
                if sim is not None:
                    _safe_setattr(solver, "sim", sim)
                if blender is not None:
                    _safe_setattr(solver, "blender", blender)
                attach_path = "soft"
            has_hybrid = bool(getattr(solver, "sim", None)) and bool(getattr(solver, "blender", None))
            _safe_setattr(solver, "has_hybrid", has_hybrid)
            _emit("runtime.hybrid_attach", {"ok": True, "path": attach_path, "has_sim": bool(sim), "has_blender": bool(blender), "has_hybrid": has_hybrid})
    except Exception as e:
        _warn("hybrid attach failed", error=str(e))

    # --- Backfill critical attrs on solver (hard guarantee for warm-start) ---
    try:
        if solver is not None:
            if not hasattr(solver, "rulebase") or getattr(solver, "rulebase", None) is None:
                _safe_setattr(solver, "rulebase", rulebase)
            if not hasattr(solver, "kb") or getattr(solver, "kb", None) is None:
                _safe_setattr(solver, "kb", kb)
    except Exception:
        pass

    # --- Link-check pulse ----------------------------------------------------
    peers_ok = {
        "sandbox_meta": (getattr(sandbox, "meta", None) is meta),
        "sim_meta":     (getattr(sim, "meta", None) is meta),
        "meta_holo":    (getattr(meta, "holo", None) is holo) if meta else False,
        "curiosity_meta": (getattr(curiosity, "meta", None) is meta) if curiosity else False,
    }
    runtime["peers_ok"] = peers_ok
    _emit("runtime.peer_check", peers_ok)

    # --- Bus-lock verification (EXPLAIN/meta_log/Kairos) ---------------------
    try:
        EXPLAIN = globals().get("EXPLAIN", None)
        meta_log_fn = globals().get("meta_log", None)
        bus_eq = {}

        # Kairos singleton agreement across components that expose it
        for name, comp in (("meta", meta), ("sandbox", sandbox), ("sim", sim), ("solver", solver)):
            if comp is None:
                bus_eq[f"{name}.kairos"] = None
            else:
                bus_eq[f"{name}.kairos"] = (getattr(comp, "kairos", None) is kairos)

        # We cannot reliably inspect EXPLAIN/meta_log inside components; record presence & ids
        bus_lock = {
            "kairos_id": id(kairos) if kairos else None,
            "EXPLAIN_id": id(EXPLAIN) if EXPLAIN is not None else None,
            "meta_log_id": id(meta_log_fn) if callable(meta_log_fn) else None,
            "eq": bus_eq,
        }
        # Locked if all non-None eq checks are True
        non_none = [v for v in bus_eq.values() if v is not None]
        bus_lock["locked"] = all(non_none) if non_none else True
        runtime["bus_lock"] = bus_lock
        _emit("runtime.bus_lock", bus_lock)
    except Exception as e:
        _warn("bus_lock_verification_failed", error=str(e))

    # --- Consolidated attach summary ----------------------------------------
    try:
        _emit("runtime.attach_summary", {
            "has": {k: bool(runtime.get(k)) for k in ["kb","rulebase","sandbox","sim","meta","holo","encoder","ultra","solver","curiosity","blender","kairos"]},
            "hybrid": {"path": attach_path, "has_sim": bool(sim), "has_blender": bool(blender), "has_hybrid": bool(getattr(solver, "has_hybrid", False))}
        })
    except Exception:
        pass

    # --- Snapshot & hashes ---------------------------------------------------
    try:
        os.makedirs("deployment", exist_ok=True)

        symbols = ["SymbolicKB", "RuntimeRulebase", "Rulebase", "SandboxExplorer",
                   "HybridSimilarity", "HybridConfidenceBlender", "Encoder", "ARCSymbolicUltra"]
        rows = []
        for sym in symbols:
            obj = globals().get(sym, None)
            if obj is None:
                continue
            try:
                src = inspect.getsource(obj)
                sha = hashlib.sha1(src.encode("utf-8")).hexdigest()
                rows.append((sym, sha))
            except Exception:
                rows.append((sym, "na"))

        snapshot = {
            "run_id": run_id,
            "phase": phase,
            "has": {k: bool(runtime.get(k)) for k in ["kb", "rulebase", "sandbox", "sim", "meta", "holo", "encoder", "ultra", "solver", "curiosity", "blender", "kairos"]},
            "peers_ok": peers_ok,
            "health": health,
            "attached_hybrid": {"path": attach_path, "has_sim": bool(sim), "has_blender": bool(blender), "has_hybrid": bool(getattr(solver, "has_hybrid", False))},
        }
        _atomic_write("deployment/runtime_snapshot.json", json.dumps(snapshot, indent=2).encode("utf-8"))

        with open("deployment/runtime_hash.csv", "w", newline="") as f:
            w = csv.writer(f)
            w.writerow(["symbol", "sha1"])
            w.writerows(rows)

    except Exception as e:
        _warn("snapshot_write_failed", error=str(e))

    # --- Fatal handling ------------------------------------------------------
    if solver is None:
        runtime["fatal"] = "solver_missing"
        runtime["fatal_detail"] = fatal_detail or {"branch": "unknown"}
        _emit("runtime.fatal", {"reason": "solver_missing", **(fatal_detail or {})})

    # Ensure keys always exist for downstream orchestration
    for k in ("kb", "rulebase", "sandbox", "sim", "meta", "holo", "encoder", "ultra", "solver", "curiosity", "blender", "kairos", "peers_ok", "bus_lock", "rulebase_exists"):
        runtime.setdefault(k, runtime.get(k, None))

    return runtime

# --- Lazy/idempotent Kairos singleton (R-01) ---
try:
    kairos  # type: ignore
except NameError:
    kairos = KairosPulseManager()
# ===========================================
# ARCSymbolicUltra  (Full Rewrite: contract-safe, hybrid-wired, policy-driven)
# ===========================================

# ---------- Compact, lazy-safe logging helpers ----------
def _safe_type(x):
    try:
        if np is not None and isinstance(x, np.ndarray):
            return {"type": "ndarray", "shape": list(x.shape), "dtype": str(x.dtype)}
        if isinstance(x, (int, float, str, bool)) or x is None:
            return x
        if isinstance(x, (list, tuple)):
            return f"{type(x).__name__}[{len(x)}]"
        if isinstance(x, dict):
            return f"dict[{len(x)}]"
        return type(x).__name__
    except Exception:
        return "<err>"


# ---------- Guarded: Solver toggles (extend if missing) ----------
try:
    SolverToggles
except NameError:
    @dataclass
    class SolverToggles:
        # Exports / telemetry
        VIS_EXPORTS: bool = True
        CSV_EXPORTS: bool = True
        JSON_EXPORTS: bool = True
        DIAG_EXPORTS: bool = True
        HEARTBEAT: bool = True
        LOG_DECISIONS: bool = True

        # Hooks / physics / explainability
        EXPLAIN_HOOKS: bool = True
        PHYSICS_HOOKS: bool = True
        PHYSICS_PULSE: bool = True

        # Exploration & safety
        CURIOUS_TAIL: bool = True
        SANDBOX_ENABLE: bool = True
        SANDBOX_CIRCUIT: bool = True
        DRY_RUN: bool = False

        # Confidence
        CALIBRATE_CONF: bool = True
        CONF_POLICY_ENABLE: bool = False

        # Head-to-head (RHCM vs classic)
        HEAD_TO_HEAD: bool = True

        # Perceptual KEEL metrics modulator
        KEEL_PERCEPT: bool = True

        # NEW: Sandbox policy knobs
        SBX_MAX_DEPTH: int = 3
        SBX_ERR_WINDOW: int = 60          # seconds window
        SBX_ERR_TRIP: int = 8             # errors in window to trip
        SBX_TRIP_SECS: float = 5.0        # trip cooldown

        # NEW: Head-to-head policy knobs
        H2H_MIN_INTERVAL: float = 0.25    # seconds between attempts
        H2H_MAX_LOSS_STREAK: int = 20
        H2H_LOSS_DECAY: int = 1           # decay on success

        # NEW: auto trainer attach
        AUTO_ATTACH_TRAINER: bool = False

        # NEW: Retry knobs
        RETRY_BASE_BOUNDS: Tuple[int, int] = (0, 2)
        RETRY_MAX: int = 6

        # NEW: ML join timeout (s)
        ML_JOIN_TIMEOUT_S: float = 1.0

        # Confidence contract frequency (predictions)
        CONF_CONTRACT_EVERY: int = 16

try:
    SOLVER_TOGGLES  # type: ignore
except Exception:
    SOLVER_TOGGLES = SolverToggles()

# ---------- Export config + helpers (guarded) ----------
try:
    ExportConfig
except NameError:
    @dataclass
    class ExportConfig:
        run_id: str = "na"
        solver_version: str = "ultra-1"
        export_root: str = "exports"
        compress: Optional[str] = None
        parallel: bool = False
        keel_snapshots: bool = True
        confidence_history: str = "confidence_history.json"  # NEW


    def _safe_holo_snapshot(label: str):
        try:
            h = globals().get("holo", None)
            if h is not None and hasattr(h, "snapshot"):
                h.snapshot(label)
        except Exception:
            pass

    def _write_meta_sidecar(csv_path: str, run_id: str, solver_version: str, kairos=None):
        try:
            import socket
            meta = {
                "run_id": run_id,
                "timestamp": _now_iso(),
                "solver_version": solver_version,
                "host": socket.gethostname(),
            }
            try:
                if kairos is not None and hasattr(kairos, "seed"):
                    meta["kairos_seed"] = getattr(kairos, "seed")
            except Exception:
                pass
            side = csv_path + ".meta.json"
            with open(side, "w", encoding="utf-8") as f:
                json.dump(meta, f, indent=2)
            try:
                meta_log("csv.meta_written", fn=side)  # noqa: F821
            except Exception:
                pass
        except Exception as e:
            try:
                meta_log("csv.error", fn=csv_path, err=str(e))  # noqa: F821
            except Exception:
                pass
            try:
                logger.exception(f"[meta_sidecar] {e}")  # noqa: F821
            except Exception:
                pass

    def _validate_nonempty(path: str):
        try:
            ok = os.path.isfile(path) and os.path.getsize(path) > 0
            if not ok:
                try: meta_log("csv.validation_failed", fn=path)  # noqa: F821
                except Exception: pass
            return ok
        except Exception:
            return False

    def _compress_path(path: str, method: Optional[str]) -> Optional[str]:
        if not method:
            return None
        try:
            if method == "gz":
                import gzip
                gz_path = path + ".gz"
                with open(path, "rb") as fin, gzip.open(gz_path, "wb") as fout:
                    fout.write(fin.read())
                try: meta_log("csv.compressed", src=path, dst=gz_path, method="gz")  # noqa: F821
                except Exception: pass
                return gz_path
            elif method == "zip":
                import zipfile
                zip_path = path + ".zip"
                with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as z:
                    z.write(path, arcname=os.path.basename(path))
                try: meta_log("csv.compressed", src=path, dst=zip_path, method="zip")  # noqa: F821
                except Exception: pass
                return zip_path
        except Exception as e:
            try: meta_log("csv.error", fn=path, err=str(e))  # noqa: F821
            except Exception: pass
            try: logger.exception(f"[compress] {e}")  # noqa: F821
            except Exception: pass
        return None



# ---------- Local grid utils ----------
def _grid(a):
    if np is None:  # ultra-safe fallback
        return a
    return np.array(a, dtype=int)

def _sanitize_grid(a):
    if np is None:
        return a
    x = _grid(a)
    return x if getattr(x, "ndim", 2) == 2 else x.reshape((1, -1))

def _apply_single(grid: np.ndarray, op: Tuple[str, Dict[str, Any]]) -> np.ndarray:
    # delegate to sandbox_apply_ops if available; otherwise safe local fallbacks
    try:
        fn = globals().get("sandbox_apply_ops", None)
        if callable(fn):
            return fn(grid, [op])  # noqa: F821
    except Exception:
        pass
    name, params = op
    try:
        if name == "rot":     return np.rot90(grid, k=int(params.get("k", 1)))
        if name == "flip_lr": return np.fliplr(grid)
        if name == "flip_ud": return np.flipud(grid)
    except Exception:
        pass
    return grid.copy()


# ---------- Proxy-safe confidence primitives ----------
def _inv_composite(a, b) -> float:
    try:
        pa = np.bincount(a.ravel(), minlength=10).astype(float); pa /= (pa.sum() or 1.0)
        pb = np.bincount(b.ravel(), minlength=10).astype(float); pb /= (pb.sum() or 1.0)
        return float(1.0 - 0.5 * np.abs(pa - pb).sum())
    except Exception:
        return 0.0

# ---------- Optional: confidence policy + calibrator ----------
class _ConfidencePolicy:
    def __init__(self):
        self.base = 0.72
        self.by_shape: Dict[Tuple[int,int], float] = {}

    def threshold(self, x: np.ndarray) -> float:
        shp = tuple(x.shape)
        return float(self.by_shape.get(shp, self.base))

class _ConfCalibrator:
    def __init__(self, beta=0.05):
        self.beta = float(beta)
        self.mu = 0.0
        self.n  = 0

    def update(self, conf: float):
        self.n += 1
        self.mu = (1 - self.beta) * self.mu + self.beta * float(conf)

    def apply(self, conf: float) -> float:
        # Simple bias correction toward running mean
        return float(0.5 * conf + 0.5 * self.mu) if self.n > 10 else float(conf)


# ===========================================
# Solver (Prediction-first; Hybrid reranking; Sandbox is confidence-gated)
# ===========================================


class ARCSymbolicUltra:
    def __init__(self,
                 sandbox_depth: int = 3,
                 retry_base: int = 2,
                 enable_thread: bool = True,
                 run_id: str = "na",
                 phase: str = "train",
                 seed: int = 1337,
                 toggles: "SolverToggles" = "SOLVER_TOGGLES",  # resolved at runtime
                 export: Optional["ExportConfig"] = None,      # resolved at runtime
                 auto_attach_trainer: Optional[bool] = None,
                 **kwargs):
        # -------------------------
        # Safe logger helper
        # -------------------------
        def _safe_log(topic: str, **kw):
            try:
                if "meta_log" in globals() and callable(globals()["meta_log"]):
                    globals()["meta_log"](topic, **kw)
            except Exception:
                pass
        self._log = _safe_log

        # -------------------------
        # Basic fields / defaults
        # -------------------------
        self.kb = getattr(self, "kb", None)
        self.rulebase = getattr(self, "rulebase", None)
        self.sandbox = getattr(self, "sandbox", None)
        self.meta = getattr(self, "meta", None)
        self.holo = getattr(self, "holo", None)
        self.sim = getattr(self, "sim", None)
        self.blender = getattr(self, "blender", None)
        self.encoder = getattr(self, "encoder", None)
        self.ultra = getattr(self, "ultra", None)
        self.kairos = getattr(self, "kairos", None)
        self.compression = getattr(self, "compression", None)
        self.ml = getattr(self, "ml", None)
        self.export = export

        # -------------------------
        # Seed & toggles
        # -------------------------
        import random
        random.seed(seed)
        try:
            np = globals().get("np")
            if np is not None:
                np.random.seed(seed)
        except Exception:
            pass

        # Resolve toggles default at runtime
        try:
            self.toggles = toggles if toggles is not None and toggles != "SOLVER_TOGGLES" else globals().get("SOLVER_TOGGLES")
        except Exception:
            self.toggles = toggles

        # Export config default
        try:
            ExportConfigCls = globals().get("ExportConfig")  # type: ignore
            if self.export is None and ExportConfigCls:
                self.export = ExportConfigCls(run_id=run_id, solver_version="ultra-1")
        except Exception:
            # keep whatever was passed in
            pass

        if auto_attach_trainer is None:
            try:
                auto_attach_trainer = bool(getattr(self.toggles, "AUTO_ATTACH_TRAINER", False))
            except Exception:
                auto_attach_trainer = False

        # -------------------------
        # Optional peers from builder (permissive ctor)
        # -------------------------
        for k in ("kb", "rulebase", "sandbox", "meta", "holo", "sim", "blender", "encoder", "ultra", "kairos", "compression", "ml"):
            try:
                if k in kwargs and getattr(self, k, None) is None:
                    setattr(self, k, kwargs[k])
            except Exception:
                pass

        # -------------------------
        # Core graph (prefer existing globals; do not duplicate)
        # -------------------------

        # KB / Rulebase
        try:
            if self.kb is None and "SymbolicKB" in globals():
                self.kb = globals()["SymbolicKB"]()  # type: ignore
        except Exception:
            self.kb = getattr(self, "kb", None)

        if self.rulebase is None:
            try:
                if "GlobalRulebase" in globals() and self.kb is not None:
                    self.rulebase = globals()["GlobalRulebase"](self.kb)  # type: ignore
                    try:
                        self.kb.rulebase = self.rulebase
                    except Exception:
                        pass
            except Exception:
                self.rulebase = None

        # Hybrid similarity
        try:
            if self.sim is None and "HybridSimilarity" in globals():
                # meta and sandbox may not be ready yet; wire them after creation
                self.sim = globals()["HybridSimilarity"](
                    meta=None,
                    kb=self.kb,
                    sandbox=None,
                    ultra=getattr(self, "ultra", None),
                    kairos=getattr(self, "kairos", None),
                    holo=getattr(self, "holo", None),
                    encoder=getattr(getattr(self, "ml", None), "encoder", self.encoder)
                )  # type: ignore
        except Exception:
            self.sim = getattr(self, "sim", None)

        # Meta
        try:
            if self.meta is None and "MetaLayer" in globals():
                self.meta = globals()["MetaLayer"](self.sim, self.kb)  # type: ignore
        except Exception:
            self.meta = getattr(self, "meta", None)

        # Back-link meta into sim
        try:
            if self.sim is not None:
                self.sim.meta = self.meta
        except Exception:
            pass

        # Holo
        try:
            if self.holo is None and "HoloMemory" in globals():
                self.holo = globals()["HoloMemory"]()  # type: ignore
        except Exception:
            self.holo = getattr(self, "holo", None)

        try:
            # Bridge meta <-> holo
            if self.meta is not None and hasattr(self.meta, "attach_holomemory"):
                self.meta.attach_holomemory(self.holo)
        except Exception:
            pass

        # Kairos / compression (optional names to preserve your existing pipeline)
        try:
            if self.kairos is None and "KairosTemporalEngine" in globals():
                self.kairos = globals()["KairosTemporalEngine"]()  # type: ignore
        except Exception:
            self.kairos = getattr(self, "kairos", None)

        try:
            if self.compression is None and "FractalCompressor" in globals():
                self.compression = globals()["FractalCompressor"]()  # type: ignore
        except Exception:
            self.compression = getattr(self, "compression", None)

        try:
            if self.kairos and hasattr(self.kairos, "bind_compression"):
                self.kairos.bind_compression(self.compression)
            if self.kairos and self.holo is not None:
                self.kairos.holo = self.holo
        except Exception:
            pass

        # Sandbox (respect policy depth)
        try:
            depth = int(getattr(self.toggles, "SBX_MAX_DEPTH", sandbox_depth))
        except Exception:
            depth = int(sandbox_depth)
        try:
            if self.sandbox is None and "SandboxExplorer" in globals():
                self.sandbox = globals()["SandboxExplorer"](kb=self.kb, meta=self.meta, max_depth=depth)  # type: ignore
        except Exception:
            self.sandbox = getattr(self, "sandbox", None)

        # ML controller (symbolic)
        try:
            if self.ml is None and "SymbolicMLController" in globals():
                self.ml = globals()["SymbolicMLController"](  # type: ignore
                    meta=self.meta, holo=self.holo, sandbox=self.sandbox, kb=self.kb, enable_threads=True
                )
        except Exception:
            self.ml = getattr(self, "ml", None)

        # Blender (Hybrid Confidence)
        try:
            if self.blender is None and "HybridConfidenceBlender" in globals():
                self.blender = globals()["HybridConfidenceBlender"](logger=globals().get("EXPLAIN", None))  # type: ignore
        except Exception:
            self.blender = getattr(self, "blender", None)

        # After sim/meta/holo/sandbox/ml are established, re-wire sim peers
        try:
            if self.sim is not None:
                self.sim.kb = self.kb
                self.sim.sandbox = self.sandbox
                self.sim.ultra = getattr(self, "ultra", None)
                self.sim.kairos = self.kairos
                self.sim.holo = self.holo
                enc = getattr(getattr(self, "ml", None), "encoder", self.encoder)
                if enc is not None:
                    self.sim.encoder = enc
        except Exception:
            pass

        # Link KB -> hybrid sim if available
        try:
            if self.kb is not None and self.sim is not None:
                self.kb.hybrid = self.sim
        except Exception:
            pass

        # Ultra agent (optional)
        try:
            if self.ultra is None and "SymbolicUltraAgent" in globals():
                self.ultra = globals()["SymbolicUltraAgent"](enable=True, run_id=run_id, phase=phase)  # type: ignore
        except Exception:
            self.ultra = getattr(self, "ultra", None)

        # -------------------------
        # Hybrid auto-attach (if available)
        # -------------------------
        try:
            attach_hybrid = globals().get("attach_hybrid_to_solver")
            if callable(attach_hybrid):
                attach_hybrid(self)
        except Exception:
            pass


        # -------------------------
        # Cross-linking (post-conditions + logs)
        # -------------------------
        try:
            if self.meta:
                if self.holo:        self.meta.holo        = self.holo
                if self.kairos:      self.meta.kairos      = self.kairos
                if self.compression: self.meta.compression = self.compression
                if self.sandbox:     self.meta.sandbox     = self.sandbox
                if self.ml:          self.meta.ml          = self.ml
            if self.holo:
                self.holo.meta   = self.meta
                self.holo.sim    = self.sim
                self.holo.kairos = self.kairos
            if self.sandbox:
                self.sandbox.meta   = self.meta
                self.sandbox.kairos = self.kairos
                self.sandbox.holo   = self.holo
            if self.kairos:
                self.kairos.meta     = self.meta
                self.kairos.rulebase = self.rulebase
            if self.compression:
                self.compression.meta = self.meta
                self.compression.holo = self.holo
            if self.ml:
                self.ml.kairos      = self.kairos
                self.ml.compression = self.compression
        except Exception as e:
            self._log("solver.crosslink_error", err=str(e))

        # -------------------------
        # Canonical evaluator for Meta/Holo bridge (monolith-safe)
        # -------------------------
        try:
            if self.meta is not None:
                def _hybrid_eval(a, b):
                    # Use blender+sim if available; fall back to solver’s proxy score
                    if self.blender and self.sim:
                        bm = self.blender.blended_score(self.sim, a, b) or {}
                        s  = float(bm.get("blended", 0.0))
                        return s, {"score": s, **{k: bm.get(k) for k in ("entropy_delta","epi_gap","binder_delta","stable")}}
                    # fallback: reuse solver’s score_confidence
                    m = self.score_confidence(a, b)
                    s = float(m.get("final", 0.0))
                    return s, {"score": s, **m}
                self.meta.hybrid_eval = _hybrid_eval  # exposed for holo bridges
        except Exception:
            pass

        # -------------------------
        # Hybrid auto-attach (guaranteed reranking path)
        # -------------------------
        try:
            if "attach_hybrid_to_solver" in globals() and callable(globals()["attach_hybrid_to_solver"]):
                globals()["attach_hybrid_to_solver"](self, sim=self.sim, blender=self.blender)  # type: ignore
            else:
                # soft attach (idempotent)
                if self.blender is not None:
                    setattr(self, "blender", self.blender)
                if self.sim is not None:
                    setattr(self, "sim", self.sim)
            self._log("solver.hybrid_attached", has_sim=bool(self.sim), has_blender=bool(self.blender))
        except Exception as e:
            self._log("solver.hybrid_attach_fail", err=str(e))

        # -------------------------
        # Optional explanation hooks / physics hooks
        # -------------------------
        try:
            if getattr(self.toggles, "EXPLAIN_HOOKS", False) and "install_explanation_hooks" in globals():
                globals()["install_explanation_hooks"](  # type: ignore
                    encoder=getattr(self, "encoder", None),
                    meta=self.meta,
                    solver=self,
                    sandbox=self.sandbox
                )
        except Exception:
            pass
        try:
            if (getattr(self.toggles, "EXPLAIN_HOOKS", False) or getattr(self.toggles, "PHYSICS_HOOKS", False)) \
               and "attach_invariants_and_explanations" in globals():
                globals()["attach_invariants_and_explanations"](self)  # type: ignore
        except Exception:
            pass

        # Emit consolidated hook status (visibility)
        try:
            self._emit_hook_status()
        except Exception:
            pass

        # -------------------------
        # Optional trainer attach (guarded by toggle)
        # -------------------------
        self.trainer = getattr(self, "trainer", None)
        if bool(auto_attach_trainer):
            try:
                if "attach_unified_trainer" in globals() and callable(globals()["attach_unified_trainer"]):
                    globals()["attach_unified_trainer"](self)  # sets self.trainer if available
                    self._log("trainer.attached_by_solver", ok=bool(getattr(self, "trainer", None) is not None))
            except Exception as e:
                self._log("trainer.attach_in_solver_failed", err=str(e))
        else:
            self._log("trainer.attach_skipped", reason="toggle_off")

        # -------------------------
        # Runtime vars
        # -------------------------
        self.retry_base   = int(retry_base)
        self._conf_ema    = 0.75
        self._conf_beta   = 0.10
        self._recent_conf = deque(maxlen=128)
        self._stop_flag   = False
        self._bg          = None
        self._lock        = threading.RLock()

        # Cached helpers
        self._inv_scorer  = None
        self._phys_heur   = None
        # Guarded policy/calibrator
        try:
            self._policy = globals().get("_ConfidencePolicy", lambda: None)()
        except Exception:
            self._policy = None
        try:
            self._calib  = globals().get("_ConfCalibrator", lambda **k: None)(beta=0.05)
        except Exception:
            self._calib = None

        # Sandbox circuit breaker
        self._sbx_err_budget = {"window": deque(maxlen=100), "tripped_until": 0.0}

        # Head-to-Head budget
        self._h2h_stats = {"wins": 0, "tries": 0, "last_try": 0.0, "loss_streak": 0}

        # Degradation state (emit once)
        self._degradation_emitted = False

        # BADASS UPGRADE: Holo add de-dup LRU to avoid repeated identical commits
        self._recent_holo_adds = deque(maxlen=512)

        # -------------------------
        # Background thread (skip if DRY_RUN)
        # -------------------------
        if enable_thread and not getattr(self.toggles, "DRY_RUN", False):
            try:
                self._bg = threading.Thread(target=self._bg_loop, name="Solver-BG", daemon=True)
                self._bg.start()
            except Exception:
                pass

        

    # -------------------------
    # Capabilities (orchestrator introspection)
    # -------------------------
    def capabilities(self) -> Dict[str, bool]:
        return {
            "predict": True,
            "kb": bool(self.kb),
            "rulebase": bool(self.rulebase),
            "sandbox": bool(self.sandbox),
            "hybrid": bool(self.sim) and bool(self.blender),
            "ml": bool(self.ml),
            "holo": bool(self.holo),
            "kairos": bool(self.kairos),
        }

    # -------------------------
    # Degradation matrix diagnostics
    # -------------------------
    def _emit_degradation_once(self):
        if self._degradation_emitted:
            return
        deg = {
            "blender": bool(self.blender and self.sim),
            "invariants": bool("InvariantScorer" in globals()),
            "physics": bool("PhysicsHeuristics" in globals()),
            "sandbox_ops": bool(callable(globals().get("sandbox_apply_ops", None))),
        }
        try: meta_log("solver.degradation", **deg)  # noqa: F821
        except Exception: pass
        self._degradation_emitted = True

    def _emit_hook_status(self):
        try:
            status = {
                "has_explain": bool("EXPLAIN" in globals()),
                "has_invariants": bool("InvariantScorer" in globals()),
                "has_physics": bool("PhysicsHeuristics" in globals()),
                "has_sim": bool(self.sim),
                "has_blender": bool(self.blender),
                "has_sandbox": bool(self.sandbox),
            }
            meta_log("solver.hooks_status", **status)  # noqa: F821
        except Exception:
            pass

    # -------------------------
    # Stop lifecycle
    # -------------------------
    def stop(self):
        self._stop_flag = True
        if self._bg and self._bg.is_alive():
            try: self._bg.join(timeout=2.0)
            except Exception: pass
        # ML lifecycle (save/stop/join) — R-325
        try:
            ml = getattr(self, "ml", None)
            if ml is not None:
                try:
                    if hasattr(ml, "save_state"):
                        ml.save_state()
                except Exception:
                    pass
                try:
                    if hasattr(ml, "stop"):
                        ml.stop()
                except Exception:
                    pass
                try:
                    th = getattr(ml, "thread", None)
                    if th and hasattr(th, "join"):
                        th.join(timeout=float(getattr(self.toggles, "ML_JOIN_TIMEOUT_S", 1.0)))
                except Exception:
                    pass
        except Exception:
            pass
        try:
            if self.sandbox and hasattr(self.sandbox, "finalize_exports"):
                self.sandbox.finalize_exports()
        except Exception:
            pass
        # BADASS UPGRADE: final housekeeping/export flush
        try:
            if "periodic_housekeeping_and_exports" in globals():
                periodic_housekeeping_and_exports(epoch_idx=999999)
        except Exception:
            pass
        try: meta_log("solver.stop")  # noqa: F821
        except Exception: pass

    # -------------------------
    # Confidence scoring surface (Hybrid > Invariants > Proxies)
    # -------------------------
    def score_confidence(self, x: np.ndarray, y_hat: np.ndarray) -> Dict[str, float]:
        out = {"final": 0.0, "blended": 0.0, "p_conf": 0.0,
               "epi": 0.0, "inv": 0.0, "phys": 0.0,
               "entropy_delta": None, "epi_gap": None, "binder_delta": None, "stable": None}
        # Hybrid primary
        try:
            if self.blender and self.sim:
                bm = self.blender.blended_score(self.sim, x, y_hat)  # expected rich dict
                out["blended"]       = float(bm.get("blended", 0.0))
                out["p_conf"]        = float(bm.get("p_conf", out["blended"]))
                out["entropy_delta"] = bm.get("entropy_delta")
                out["epi_gap"]       = bm.get("epi_gap")
                out["binder_delta"]  = bm.get("binder_delta")
                out["stable"]        = bm.get("stable")
                out["final"]         = float(0.7 * out["blended"] + 0.15 * (1.0 if out["stable"] else 0.0) + 0.15 * (1.0 - abs(out["entropy_delta"] or 0.0)))
                if "epi" in bm:  out["epi"]  = float(bm["epi"])
                if "inv" in bm:  out["inv"]  = float(bm["inv"])
                if "phys" in bm: out["phys"] = float(bm["phys"])
            else:
                raise RuntimeError("no_blender_path")
        except Exception:
            # Invariants + Physics secondary
            try:
                if "InvariantScorer" in globals():
                    scorer = self._inv_scorer or InvariantScorer(logger=globals().get("EXPLAIN", None))
                    self._inv_scorer = scorer
                    m = scorer.score_pair(x, y_hat, y_true=None) or {}
                    inv = float(m.get("score", 0.0))
                else:
                    inv = _inv_composite(x, y_hat)
            except Exception:
                inv = _inv_composite(x, y_hat)

            try:
                if "PhysicsHeuristics" in globals():
                    phys_h = self._phys_heur or PhysicsHeuristics(logger=globals().get("EXPLAIN", None))
                    self._phys_heur = phys_h
                    m2 = phys_h.score_pair(x, y_hat) or {}
                    phys = 1.0 if bool(m2.get("ok", False)) else max(0.0, 1.0 - float(m2.get("mass_delta", 1.0)))
                else:
                    phys = _physics_plausibility(x, y_hat)
            except Exception:
                phys = _physics_plausibility(x, y_hat)

            epi = _epi(y_hat, x)
            final = float(0.5 * epi + 0.3 * inv + 0.2 * phys)
            out.update({"final": final, "blended": final, "p_conf": final, "epi": epi, "inv": inv, "phys": phys})

        # Optional online calibration
        try:
            if getattr(self.toggles, "CALIBRATE_CONF", True) and self._calib is not None:
                out["final"] = float(self._calib.apply(out["final"]))
        except Exception:
            pass
        return out

    # -------------------------
    # Retry modulation (Kairos/Keel + optional perceptual metrics)
    # -------------------------
    def _modulated_retries(self, baseline_pred: Optional[np.ndarray] = None) -> int:
        # bounds from toggles
        try:
            base_lo, base_hi = tuple(getattr(self.toggles, "RETRY_BASE_BOUNDS", (0, 2)))
        except Exception:
            base_lo, base_hi = (0, 2)
        rmax = int(getattr(self.toggles, "RETRY_MAX", 6))

        conf_term = max(0.0, 1.0 - float(self._conf_ema))
        kairos = 0.0
        keel_ratio = 1.0
        try:
            sbx = getattr(self, "sandbox", None)
            if sbx and getattr(sbx, "kairos_flux_history", None):
                if np is not None:
                    kairos = float(np.mean(sbx.kairos_flux_history))
            if sbx and getattr(sbx, "keel_ratio_history", None):
                if np is not None:
                    keel_ratio = float(np.mean(sbx.keel_ratio_history))
        except Exception:
            pass

        percept_factor = 1.0
        if getattr(self.toggles, "KEEL_PERCEPT", True):
            if baseline_pred is not None and "keel_metrics" in globals():
                try:
                    km = keel_metrics(baseline_pred)  # noqa: F821
                    psnr = km.get("psnr"); ssim = km.get("ssim")
                    if isinstance(ssim, (int, float)):
                        percept_factor *= (1.0 + min(0.3, max(0.0, 0.5 - float(ssim))))
                    if isinstance(psnr, (int, float)):
                        percept_factor *= (1.0 - min(0.2, max(0.0, (float(psnr) - 30.0) * 0.01)))
                except Exception:
                    pass

        # circuit-aware + loss-streak-aware modulation
        loss_streak = int(self._h2h_stats.get("loss_streak", 0))
        circuit_penalty = 0.7 if (time.time() < float(self._sbx_err_budget.get("tripped_until", 0.0))) else 1.0

        flux_factor = 1.0 + min(0.5, 0.01 * kairos)
        keel_factor = max(0.7, min(1.3, 0.9 + 0.05 * (keel_ratio - 1.0)))
        loss_factor = max(0.7, 1.0 - 0.02 * min(15, loss_streak))
        base = max(base_lo, min(base_hi, self.retry_base))
        n = int(round(base * (1.0 + 2.0*conf_term) * flux_factor * keel_factor * percept_factor * loss_factor * circuit_penalty))
        retries = max(base, min(rmax, n))

        # self-tuning retry_base (soft; bounded)
        try:
            target = 3 if self._conf_ema < 0.6 else 2
            self.retry_base = int(max(1, min(4, 0.8*self.retry_base + 0.2*target)))
        except Exception:
            pass

        try: meta_log("solver.retry_matrix", conf_ema=float(self._conf_ema), retries=int(retries), base=int(base))  # noqa: F821
        except Exception: pass
        return int(retries)

    def _update_conf_ema(self, conf: float):
        self._conf_ema = (1.0 - self._conf_beta) * self._conf_ema + self._conf_beta * float(conf)
        self._recent_conf.append(float(conf))
        try:
            if getattr(self.toggles, "CALIBRATE_CONF", True) and self._calib is not None:
                self._calib.update(float(conf))
        except Exception:
            pass
        # Confidence history writer (closes null avg_confidence gap)
        try:
            path = getattr(self.export, "confidence_history", "confidence_history.json") if self.export else "confidence_history.json"
            hist = []
            if os.path.exists(path):
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        hist = json.load(f)
                except Exception:
                    hist = []
            hist.append({
                "ts": time.time(),
                "pass_ix": 1,
                "avg_confidence": round(float(self._conf_ema), 6)
            })
            tmp = f"{path}.tmp"
            with open(tmp, "w", encoding="utf-8") as f:
                json.dump(hist, f, indent=2)
            os.replace(tmp, path)
        except Exception:
            pass

    # -------------------------
    # Sandbox circuit breaker helpers (policy-driven)
    # -------------------------
    def _sbx_record_error(self):
        if not self.toggles.SANDBOX_CIRCUIT:
            return
        now = time.time()
        self._sbx_err_budget["window"].append(now)
        # drop old entries (>SBX_ERR_WINDOW)
        win = int(getattr(self.toggles, "SBX_ERR_WINDOW", 60))
        trip_n = int(getattr(self.toggles, "SBX_ERR_TRIP", 8))
        trip_secs = float(getattr(self.toggles, "SBX_TRIP_SECS", 5.0))
        while self._sbx_err_budget["window"] and now - self._sbx_err_budget["window"][0] > win:
            self._sbx_err_budget["window"].popleft()
        if len(self._sbx_err_budget["window"]) >= trip_n:
            self._sbx_err_budget["tripped_until"] = now + trip_secs
            try: meta_log("sandbox.circuit_trip", count=len(self._sbx_err_budget["window"]))  # noqa: F821
            except Exception: pass

    def _sbx_allowed(self) -> bool:
        if not (self.toggles.SANDBOX_ENABLE):
            return False
        if not self.toggles.SANDBOX_CIRCUIT:
            return True
        return time.time() >= float(self._sbx_err_budget.get("tripped_until", 0.0))

    # -------------------------
    # KB recall (with tiny provenance bump + ML tap)
    # -------------------------
    def _kb_recall(self, x: np.ndarray, top_k: int = 3) -> Optional[Tuple[np.ndarray, float, Any, Optional[Any]]]:
        try:
            if not (self.kb and hasattr(self.kb, "recall_xforms")):
                return None
            sig = compute_invariants(x) if "compute_invariants" in globals() else None  # noqa: F821
            gid = getattr(sig, "glyph_id", None)
            cands = self.kb.recall_xforms(gid, x.shape, top_k=top_k) if gid is not None else self.kb.recall_xforms(None, x.shape, top_k=top_k)
            if not cands:
                return None

            # Optional glyph-gate any candidate list if a gate fn exists
            if gid is not None and "glyph_constrained_candidates" in globals():
                try:
                    cands = glyph_constrained_candidates(cands, gid)  # noqa: F821
                except Exception:
                    pass

            best_pred = None
            best_score = -1e9
            best_rec = None
            best_ops = None

            seen = set()
            for rec in cands:
                ops = rec.rule.params.get("ops", []) if hasattr(rec, "rule") else []
                try:
                    pred = apply_ops(x, ops)
                except Exception:
                    continue
                # de-dup candidate outputs by hash
                try:
                    h = hash(pred.tobytes())
                except Exception:
                    h = hash(str(pred))
                if h in seen:
                    continue
                seen.add(h)
                sim = 0.0
                try:
                    if self.blender and self.sim:
                        bm = self.blender.blended_score(self.sim, x, pred)
                        sim = float(bm.get("blended", 0.0))
                    else:
                        sim = _epi(pred, x)
                except Exception:
                    sim = _epi(pred, x)
                prov = 0.05
                s = sim + prov
                if s > best_score:
                    best_score = s
                    best_pred = pred
                    best_rec = rec
                    best_ops = ops

            if best_pred is None:
                return None

            try:
                if self.ml:
                    self.ml.record_feedback(label="kb_recall", memory_layer="kb", success=True,
                                            weight=float(min(1.5, max(0.3, best_score))),
                                            meta={"ops_len": len(best_ops or []), "glyph": gid})
            except Exception:
                pass
            return best_pred, float(best_score), best_rec, gid
        except Exception:
            return None

    # -------------------------
    # Head-to-Head (RHCM vs Classic) attempt before Sandbox (guarded)
    # -------------------------
    def _try_head_to_head(self, x: np.ndarray, current_conf: float, conf_threshold: float) -> Optional[Tuple[np.ndarray, Dict[str, Any]]]:
        if not self.toggles.HEAD_TO_HEAD or "solve_with_head_to_head" not in globals():
            return None
        try:
            now = time.time()
            if now - self._h2h_stats["last_try"] < float(getattr(self.toggles, "H2H_MIN_INTERVAL", 0.25)):
                return None
            if (self._h2h_stats["tries"] - self._h2h_stats["wins"]) > int(getattr(self.toggles, "H2H_MAX_LOSS_STREAK", 20)):
                return None
            self._h2h_stats["last_try"] = now

            def _eval_fn(y_hat):
                m = self.score_confidence(x, y_hat)
                return m.get("final", 0.0)

            res = solve_with_head_to_head(x, evaluate_fn=_eval_fn)  # noqa: F821
            if isinstance(res, dict) and "best" in res:
                y_best = res.get("best")
                if np is not None and isinstance(y_best, np.ndarray):
                    cm = self.score_confidence(x, y_best)
                    self._h2h_stats["tries"] += 1
                    if cm["final"] > max(current_conf, conf_threshold):
                        self._h2h_stats["wins"] += 1
                        # decay loss streak on success
                        self._h2h_stats["loss_streak"] = max(0, int(self._h2h_stats["loss_streak"]) - int(getattr(self.toggles, "H2H_LOSS_DECAY", 1)))
                        try:
                            meta_log("solver.head_to_head_accept",
                                     blended=cm.get("blended"), p_conf=cm.get("p_conf"),
                                     epi_gap=cm.get("epi_gap"), entropy_delta=cm.get("entropy_delta"))  # noqa: F821
                        except Exception:
                            pass
                        return y_best, {"source": "head2head", "confidence": cm["final"],
                                        "epi": cm["epi"], "inv": cm["inv"], "phys": cm["phys"]}
                    else:
                        self._h2h_stats["loss_streak"] += 1
        except Exception:
            pass
        return None

    # -------------------------
    # Unified sandbox attempt (discovery → apply → score → optional commit)
    # -------------------------
    def _sandbox_attempt(self,
                         x: np.ndarray,
                         seed_or_target: Optional[np.ndarray],
                         task_id: str,
                         commit_gate: float) -> Optional[Tuple[np.ndarray, Dict[str, Any]]]:
        if not (self._sbx_allowed() and self.sandbox):
            return None
        chain = None
        try:
            tgt_hint = seed_or_target if seed_or_target is not None else x
            chain = self.sandbox.discover_chain(x, tgt_hint, task_id=task_id)
        except Exception:
            chain = None
            self._sbx_record_error()
        if not chain:
            return None
        try:
            pred = apply_ops(x, chain)
        except Exception:
            return None
        cm = self.score_confidence(x, pred)
        # commit discipline (stricter)
        if cm["final"] >= max(0.85, commit_gate):
            try:
                commit_xform(x, pred, chain, self, meta_extra={"discovery": "solver.sandbox_attempt", "task_id": task_id}, confidence=cm["final"])
            except Exception:
                pass
        return pred, {"source": "sandbox", "confidence": cm["final"], "epi": cm["epi"], "inv": cm["inv"], "phys": cm["phys"]}

    # -------------------------
    # Main rule application (prediction mind)
    # -------------------------
    def apply_rules(self, x: np.ndarray, conf_threshold: Optional[float] = None) -> Tuple[np.ndarray, Dict[str, Any]]:
        x = _sanitize_grid(x)
        # BADASS UPGRADE: EMA-based hysteresis modulation of threshold + guard policy
        base_thr = (self._policy.threshold(x) if (getattr(self.toggles, "CONF_POLICY_ENABLE", False) and hasattr(self._policy, "threshold")) else (conf_threshold if conf_threshold is not None else 0.72))
        hyster = 0.05 if self._conf_ema < 0.6 else (-0.03 if self._conf_ema > 0.85 else 0.0)
        thr = float(max(0.10, min(0.95, base_thr + hyster)))

        # 1) Try KB/rules path
        kb = self._kb_recall(x, top_k=3)
        if kb is not None:
            pred0, kb_score, _rec, glyph_id = kb
            conf0 = self.score_confidence(x, pred0)
            if self.toggles.LOG_DECISIONS:
                try: meta_log("solver.kb_path", conf=conf0["final"], glyph=glyph_id)  # noqa: F821
                except Exception: pass
            if conf0["final"] >= thr:
                self._update_conf_ema(conf0["final"])
                try: set_grid_telemetry(prev=x, cur=pred0)
                except Exception: pass
                # Reinforce Holo on accept (KB)
                self._maybe_holo_add(x, pred0, {"subject":"solver.kb", "confidence":float(conf0["final"]), "rule_kind":"kb_path"}, min_conf=thr)
                # Shape-aware threshold tuning on success
                self._on_success_threshold_tune(x.shape, conf0["final"], thr)
                return pred0, {"source": "kb", "confidence": conf0["final"], "epi": conf0["epi"], "inv": conf0["inv"], "phys": conf0["phys"]}

            # 2) Head-to-Head before Sandbox (guarded)
            h2h = self._try_head_to_head(x, conf0["final"], thr)
            if h2h is not None:
                pred_h, info_h = h2h
                self._update_conf_ema(info_h["confidence"])
                try: set_grid_telemetry(prev=x, cur=pred_h)
                except Exception: pass
                self._maybe_holo_add(x, pred_h, {"subject":"solver.h2h", "confidence":float(info_h["confidence"]), "rule_kind":"head2head"}, min_conf=thr)
                self._on_success_threshold_tune(x.shape, info_h["confidence"], thr)
                return pred_h, info_h

            pred_seed = pred0
        else:
            pred_seed = None

        # 2.5) Holo recall-then-evaluate (prefer validated recalled pred if strong)
        try:
            m = getattr(self, "meta", None)
            if m is not None and hasattr(m, "holo_recall_then_evaluate") and hasattr(m, "hybrid_eval"):
                hit = m.holo_recall_then_evaluate(
                    x, m.hybrid_eval, topk=3, subject="solver.apply", task_id="solver", train_index=None
                )
                if hit is not None:
                    pred_r, _hmeta, sc = hit
                    self._update_conf_ema(float(sc))
                    try: set_grid_telemetry(prev=x, cur=pred_r)
                    except Exception: pass
                    self._maybe_holo_add(x, pred_r, {"subject":"solver.holo_recall", "confidence":float(sc), "rule_kind":"holo_recall"}, min_conf=thr)
                    self._on_success_threshold_tune(x.shape, float(sc), thr)
                    return pred_r, {"source": "holo_recall", "confidence": float(sc), "epi": None, "inv": None, "phys": None}
        except Exception:
            pass

        # 3) Sandbox only if KB absent or confidence low (unified attempt)
        sbx = self._sandbox_attempt(x, pred_seed, task_id="solver.apply_rules", commit_gate=thr)
        if sbx is not None:
            pred1, info1 = sbx
            if self.toggles.LOG_DECISIONS:
                try: meta_log("solver.sandbox_accept", conf=info1["confidence"])  # noqa: F821
                except Exception: pass
            self._update_conf_ema(info1["confidence"])
            try: set_grid_telemetry(prev=x, cur=pred1)
            except Exception: pass
            self._maybe_holo_add(x, pred1, {"subject":"solver.sandbox", "confidence":float(info1["confidence"]), "rule_kind":"sandbox"}, min_conf=thr)
            self._on_success_threshold_tune(x.shape, info1["confidence"], thr)
            return pred1, info1

        # 4) Fallback to identity (low confidence)
        pred_fallback = x.copy()
        conf = self.score_confidence(x, pred_fallback)
        if self.toggles.LOG_DECISIONS:
            try: meta_log("solver.fallback_identity", conf=conf["final"])  # noqa: F821
            except Exception: pass
        self._update_conf_ema(conf["final"])
        try: set_grid_telemetry(prev=x, cur=pred_fallback)
        except Exception: pass
        # optional very-low weight reinforcement; gate high to usually skip
        self._maybe_holo_add(x, pred_fallback, {"subject":"solver.fallback", "confidence":float(conf["final"]), "rule_kind":"fallback"}, min_conf=max(0.90, thr))
        return pred_fallback, {"source": "none", "confidence": conf["final"], "epi": conf["epi"], "inv": conf["inv"], "phys": conf["phys"]}

    # -------------------------
    # Hybrid-aware candidate reranker
    # -------------------------
    def choose_best_prediction(self, inp: np.ndarray, candidates: List[np.ndarray], ref: Optional[np.ndarray] = None):
        if not candidates:
            return inp.copy(), {"fallback": True, "blended": 0.0}
        # de-dup by content
        uniq = []
        seen = set()
        for c in candidates:
            try:
                h = hash(c.tobytes())
            except Exception:
                h = hash(str(c))
            if h not in seen:
                seen.add(h); uniq.append(c)
        if self.sim and self.blender:
            best = None
            best_s = -1e9
            best_meta = {}
            for c in uniq:
                bm = self.blender.blended_score(self.sim, inp, c)
                s = float(bm.get("blended", 0.0))
                if s > best_s:
                    best_s = s; best = c; best_meta = bm
            try: meta_log("hybrid.pick", blended=float(max(best_s, 0.0)), has_ref=bool(ref is not None))  # noqa: F821
            except Exception: pass
            return (best if best is not None else uniq[0]), {"blended": float(max(best_s, 0.0)), **best_meta}
        elif self.sim:
            scored = []
            for c in uniq:
                try:
                    m = self.sim.composite(inp, c)
                    scored.append((c, m.get("score", 0.0), m))
                except Exception:
                    scored.append((c, _epi(inp, c), {}))
            scored.sort(key=lambda t: -t[1])
            return scored[0][0], {"blended": float(scored[0][1])}
        return uniq[0], {"blended": 0.0}

    # -------------------------
    # Candidate generator (simple; extend as needed)
    # -------------------------
    def _generate_candidates(self, x: np.ndarray) -> List[np.ndarray]:
        cands = []
        try:
            kb = self._kb_recall(x, top_k=3)
            if kb is not None:
                cands.append(kb[0])
        except Exception:
            pass
        if self._sbx_allowed() and self.sandbox:
            try:
                chain = self.sandbox.discover_chain(x, x, task_id="solver.generate_candidates")
                if chain:
                    cands.append(apply_ops(x, chain))
            except Exception:
                self._sbx_record_error()
        # ensure at least one
        if not cands:
            cands.append(x.copy())
        return cands

    # -------------------------
    # (3) Sandbox/solver loop integration (recall-first, add-on-success)
    # -------------------------
    def solve_or_search(self, current_grid: np.ndarray, evaluate_fn, accept_fn, subject="solver"):        
        # 1) Holo recall-first
        try:
            if self.holo is not None and hasattr(self.holo, "get"):
                hits = self.holo.get(current_grid, topk=1)
                if hits:
                    pred, meta_hit, dist = hits[0]
                    meta_log("holo.recall_hit", dist=dist, conf=meta_hit.get("confidence", 0.0),
                             subject=subject, shape=tuple(current_grid.shape))  # noqa: F821
                    ok, score, gold = evaluate_fn(pred)
                    if ok:
                        accept_fn(pred)
                        # add-on-success (reinforce)
                        try:
                            self.holo.add(current_grid, pred if gold is None else gold,
                                          {"subject":subject, "confidence":max(0.85, meta_hit.get("confidence", 0.6)),
                                           "rule_kind":"recall_success"})
                        except Exception as e:
                            meta_log("holo.add_fail", site="solver.recall_success", error=str(e))  # noqa: F821
                        return pred, score
        except Exception as e:
            meta_log("holo.recall_fail", error=str(e), subject=subject)  # noqa: F821

        # 2) Fall back to normal search/generation pipeline
        cands = self._generate_candidates(current_grid)
        best = None
        best_score = -1e9
        for pred in cands:
            ok, score, gold = evaluate_fn(pred)
            if score > best_score:
                best, best_score = pred, score
            if ok:
                accept_fn(pred)
                # add-on-success
                try:
                    if self.holo is not None and hasattr(self.holo, "add"):
                        self.holo.add(current_grid, pred if gold is None else gold,
                                      {"subject":subject, "confidence":0.9, "rule_kind":"search_success"})
                except Exception as e:
                    meta_log("holo.add_fail", site="solver.success", error=str(e))  # noqa: F821
                return pred, score
        return best, best_score

    # -------------------------
    # Single-sample prediction (confidence-gated sandbox retries)
    # -------------------------
    def predict_one(self, sample: Dict[str, Any], conf_threshold: Optional[float] = None):
        with self._lock:
            x = _sanitize_grid(sample.get("input"))
            # BADASS UPGRADE: EMA hysteresis applied via apply_rules; here pass-thru threshold with policy guard
            thr = (self._policy.threshold(x) if (getattr(self.toggles, "CONF_POLICY_ENABLE", False) and hasattr(self._policy, "threshold")) else (conf_threshold if conf_threshold is not None else 0.72))

            # --- Holo recall-fast (before generation) ---
            try:
                if self.holo is not None and hasattr(self.holo, "get"):
                    hits = self.holo.get(x, topk=1)
                    if hits:
                        pred_hit, meta_hit, dist = hits[0]
                        cm = self.score_confidence(x, pred_hit)
                        if cm.get("final", 0.0) >= thr:
                            try:
                                self._maybe_holo_add(x, pred_hit, {"subject": "solver.holo_recall", "confidence": float(cm["final"]), "rule_kind": "recall_accept"}, min_conf=thr)
                            except Exception:
                                pass
                            try: set_grid_telemetry(prev=x, cur=pred_hit)
                            except Exception: pass
                            self._update_conf_ema(float(cm["final"]))
                            self._pred_count += 1; self._accept_count += 1
                            # Cadence: housekeeping every 50 preds
                            self._maybe_housekeeping()
                            return pred_hit, {"source": "holo_recall", "confidence": float(cm["final"])}
            except Exception:
                pass

            pred, info = self.apply_rules(x, conf_threshold=thr)

            # Sandbox retries via unified attempt (if needed)
            if info["confidence"] < thr and self._sbx_allowed() and self.sandbox and not self.toggles.DRY_RUN:
                retries = self._modulated_retries(baseline_pred=pred)
                improved = False
                for _ in range(max(0, int(retries))):
                    sbx = self._sandbox_attempt(x, pred, task_id=sample.get("task_id", "na"), commit_gate=thr)
                    if sbx is None:
                        continue
                    pred2, info2 = sbx
                    if info2["confidence"] > info["confidence"]:
                        # improved prediction accepted
                        try: set_grid_telemetry(prev=pred, cur=pred2)
                        except Exception: pass
                        pred, info = pred2, info2
                        improved = True
                        try:
                            if self.ml and hasattr(self.ml, "ingest_sandbox_outcome"):
                                self.ml.ingest_sandbox_outcome(kind="solver_retry",
                                                               success=True, inp=x, out=pred2, chain=None,
                                                               score=float(info2["confidence"]), task_id=sample.get("task_id"))
                        except Exception:
                            pass
                        # choose whether to continue trying to improve further or break; break on first improvement
                        break
                if not improved and retries > 0:
                    self._sbx_record_error()

            # Curiosity ping on very low confidence (gated)
            try:
                if self.toggles.CURIOUS_TAIL and info.get("confidence", 1.0) < 0.5 and getattr(self, "curiosity", None) and hasattr(self.curiosity, "explore"):
                    gi, go = (np.zeros((1, 1), int), np.ones((1, 1), int)) if np is not None else ([[0]], [[1]])
                    self.curiosity.explore(gi, go, budget=1, task_id=sample.get("task_id"))
            except Exception:
                pass

            # ML feedback (rich)
            try:
                if self.ml:
                    self.ml.record_feedback(label="predict_one",
                                            memory_layer="solver",
                                            success=True,
                                            weight=float(min(1.5, max(0.2, info.get("confidence", 0.66)))) ,
                                            meta={"confidence": float(info.get("confidence", 0.66)),
                                                  "retries": int(retries) if 'retries' in locals() else 0,
                                                  "source": info.get("source", "kb")})
            except Exception:
                pass

            if self.toggles.LOG_DECISIONS:
                try: meta_log("solver.predict_one", conf=float(info.get("confidence", 0.0)), src=info.get("source", "kb"))  # noqa: F821
                except Exception: pass

            # final telemetry update for heartbeat
            try: set_grid_telemetry(prev=x, cur=pred)
            except Exception: pass

            # --- Post-solve learn-on-success (commit to Holo) ---
            try:
                conf_final = float(info.get("confidence", 0.0))
                if conf_final >= max(0.80, thr):
                    self._maybe_holo_add(x, pred, {"subject": "solver", "confidence": conf_final, "rule_kind": info.get("source", "solver")}, min_conf=max(0.80, thr))
            except Exception:
                pass

            self._pred_count += 1
            if info.get("confidence", 0.0) >= thr:
                self._accept_count += 1
            self._maybe_housekeeping()

            return pred, {"source": info.get("source", "kb"), "confidence": float(info.get("confidence", 0.66))}

    # -------------------------
    # Minimal solver contract (predict shim)
    # -------------------------
    def predict(self, sample: Dict[str, Any], conf_threshold: Optional[float] = None):
        """Canonical solver API expected by trainer/orchestrator."""
        return self.predict_one(sample, conf_threshold=conf_threshold)

    # -------------------------
    # Task solve (toggle-aware CSV export + KEEL compress + snapshots)
    # -------------------------
    def solve_task(self, task: Dict[str, Any]):
        tid = str(task.get("id", "unknown"))

        # Install a target geometry (if provided) for context-sensitive explorers
        installed_ctx = False
        tr = task.get("train") or []
        if tr and isinstance(tr, list):
            y0 = tr[0].get("output")
            if y0 is not None and np is not None:
                try:
                    _set_geom_target(np.array(y0, dtype=int))  # noqa: F821
                    installed_ctx = True
                except Exception:
                    try: _clear_geom_target()  # noqa: F821
                    except Exception: pass

        outs, rows = [], []
        for i, t in enumerate(task.get("test", [])):
            pred, meta = self.predict_one({"input": t.get("input"), "task_id": f"{tid}:{i}"})
            outs.append(pred)
            rows.append({
                "task_id": tid,
                "test_index": i,
                "prediction": pred.tolist() if (np is not None and hasattr(pred, "tolist")) else pred,
                "rule_kind": meta.get("source", "kb"),
                "sim_score": meta.get("confidence", 0.0),
                "correct": ""
            })

        if installed_ctx:
            try: _clear_geom_target()  # noqa: F821
            except Exception: pass

        # Diagnostics visuals (optional)
        try:
            if self.toggles.VIS_EXPORTS and "export_geometry_physics_visuals" in globals():
                export_geometry_physics_visuals(self, task, out_dir=os.path.join(self.export.export_root, "diagnostics"))
        except Exception:
            pass

        # CSV export (toggle-aware) + KEEL compression enforced
        if self.toggles.CSV_EXPORTS and rows:
            try:
                out_dir = os.path.join(self.export.export_root, "eval")
                os.makedirs(out_dir, exist_ok=True)
                csv_path = os.path.join(out_dir, f"{tid}.csv")

                if self.export.keel_snapshots and self.toggles.CSV_EXPORTS:
                    _safe_holo_snapshot(f"csv_pre_{os.path.basename(csv_path)}")

                if "_write_csv_eval_predictions" in globals():
                    _write_csv_eval_predictions(rows, csv_path, compress="keel")  # <— enforce KEEL sidecar
                else:
                    with open(csv_path, "w", newline="", encoding="utf-8") as f:
                        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
                        writer.writeheader(); writer.writerows(rows)

                _write_meta_sidecar(csv_path, self.export.run_id, self.export.solver_version, getattr(self, "kairos", None))
                ok = _validate_nonempty(csv_path)
                if ok and self.export.compress:
                    _compress_path(csv_path, self.export.compress)

                if self.export.keel_snapshots and self.toggles.CSV_EXPORTS:
                    _safe_holo_snapshot(f"csv_post_{os.path.basename(csv_path)}")

                if self.toggles.LOG_DECISIONS:
                    try: meta_log("solver.csv_exported", path=csv_path, n=len(rows))  # noqa: F821
                    except Exception: pass
            except Exception as e:
                try:
                    meta_log("csv.error", fn=tid, err=str(e))  # noqa: F821
                except Exception:
                    pass
                try:
                    logger.exception(f"[solve_task.csv] {e}")  # noqa: F821
                except Exception:
                    pass

        try: meta_log("solver.solve_task_done", task_id=tid, n=len(rows))  # noqa: F821
        except Exception: pass
       
        self._maybe_emit_diag()
        
        try:
            if "periodic_housekeeping_and_exports" in globals():
                periodic_housekeeping_and_exports(epoch_idx=max(1, len(rows)//max(1, len(task.get("test", []))) ) + 1)
        except Exception:
            pass

        return {"prediction": outs[0] if len(outs) == 1 else outs, "rows": rows}

    # -------------------------
    # Miss repair: learn from gold (kept + hybrid-aware) — class-level
    # -------------------------
    def learn_from_misses(self, misses: List[Dict[str, Any]]) -> int:        
        def _to_grid(z):
            try:
                if "_sanitize_grid" in globals() and callable(globals()["_sanitize_grid"]):
                    return globals()["_sanitize_grid"](z)
            except Exception:
                pass
            try:
                a = np.array(z, dtype=int) if np is not None else z
                if np is None:
                    return a
                if a.ndim != 2:
                    a = a.reshape((-1, a.shape[-1])) if a.ndim > 2 else a.reshape((1, -1))
                return a
            except Exception:
                return None

        accepted = 0
        sbx = getattr(self, "sandbox", None)
        rb  = getattr(self, "rulebase", None)
        tr  = getattr(self, "trainer", None)
        enc = getattr(self, "encoder", None)
        ml  = getattr(self, "ml", None)

        for m in (misses or []):
            tid = str(m.get("task_id", "na"))
            x = _to_grid(m.get("input"))
            y = _to_grid(m.get("gold"))
            if x is None or y is None:
                try: meta_log("miss.learn.skip", task_id=tid, reason="sanitize_failed")  # noqa: F821
                except Exception: pass
                continue

            chain = None
            if callable(getattr(self, "_sbx_allowed", None)) and self._sbx_allowed() and sbx:
                try:
                    chain = sbx.discover_chain(x, y, task_id=f"miss:{tid}")
                except Exception as e:
                    chain = None
                    if callable(getattr(self, "_sbx_record_error", None)):
                        try: self._sbx_record_error()
                        except Exception: pass
                    try: meta_log("miss.learn.sbx_error", task_id=tid, err=str(e))  # noqa: F821
                    except Exception: pass

            if chain:
                try:
                    commit_xform(x, y, chain, self, meta_extra={"discovery": "miss.repair"}, confidence=0.85)
                    accepted += 1
                except Exception as e:
                    try: meta_log("miss.learn.commit_error", task_id=tid, err=str(e))  # noqa: F821
                    except Exception: pass
                try:
                    if ml and hasattr(ml, "ingest_sandbox_outcome"):
                        ml.ingest_sandbox_outcome(kind="miss_repair",
                                                  success=True, inp=x, out=y,
                                                  chain=chain, score=0.9, task_id=tid)
                except Exception:
                    pass
            else:                
                try:
                    cands = self._generate_candidates(x)
                    best, meta = self.choose_best_prediction(x, cands, ref=y)
                    if rb and hasattr(rb, "try_learn_from_pair"):
                        ok = bool(rb.try_learn_from_pair(x, y))
                        accepted += int(ok)
                        try: meta_log("miss.learn.rulebase", task_id=tid, ok=bool(ok), blended=meta.get("blended", 0.0))  # noqa: F821
                        except Exception: pass
                except Exception as e:
                    try: meta_log("miss.learn.rulebase_error", task_id=tid, err=str(e))  # noqa: F821
                    except Exception: pass

            try:
                if tr and hasattr(tr, "nudge_pair"):
                    tr.nudge_pair(x, y, source="miss_repair")
            except Exception:
                pass

            try:
                if enc and hasattr(enc, "record_feedback"):
                    enc.record_feedback("miss_repair", "training", True, inp=x, out=y, confidence=0.9)
            except Exception:
                pass

        try:
            meta_log("solver.learn_from_misses", accepted=int(accepted), total=int(len(misses or [])))  # noqa: F821
        except Exception:
            pass
        return accepted

    # -------------------------
    # Convert eval rows → miss structures (shared helper)
    # -------------------------
    def rows_to_misses(self, rows: List[Dict[str, Any]], gold_lookup: Dict[Tuple[str,int], np.ndarray]) -> List[Dict[str, Any]]:
        misses = []
        for r in (rows or []):
            tid, idx = r["task_id"], r["test_index"]
            y = gold_lookup.get((tid, idx))
            if y is None:
                continue
            pr = np.array(r["prediction"], dtype=int) if np is not None else r["prediction"]
            y  = np.array(y, dtype=int) if np is not None else y
            try:
                eq = (pr.shape == y.shape and np.array_equal(pr, y)) if np is not None else (pr == y)
            except Exception:
                eq = False
            if not eq:
                misses.append({
                    "task_id": f"{tid}:{idx}",
                    "input": r.get("input_snapshot") or None,
                    "gold": y.tolist() if (np is not None and hasattr(y, "tolist")) else y,
                    "pred": pr.tolist() if (np is not None and hasattr(pr, "tolist")) else pr,
                })
        return misses

    # -------------------------
    # Background opportunistic loop (light, with backoff, de-dup)
    # -------------------------
    def _bg_loop(self):
        err_streak = 0
        while not self._stop_flag:
            try:
                time.sleep(0.05 if err_streak < 3 else 0.10)
                if self.toggles.DRY_RUN:
                    continue
                if not self._sbx_allowed():
                    continue
                if not (self.kb and hasattr(self.kb, "idx_by_glyph_shape") and self.kb.idx_by_glyph_shape):
                    continue
                (glyph, shp), idxs = random.choice(list(self.kb.idx_by_glyph_shape.items()))
                recs = [self.kb.records[i] for i in idxs[:1]]
                if not recs:
                    continue
                rec = recs[0]
                x = rec.input_grid; y = rec.output_grid
                if x is None or y is None or self.sandbox is None:
                    continue
                chain = None
                try:
                    chain = self.sandbox.discover_chain(x, y, task_id=f"bg:{glyph}")
                except Exception:
                    chain = None
                    self._sbx_record_error()
                if chain:
                    try:
                        commit_xform(x, y, chain, self, meta_extra={"discovery": "bg"}, confidence=0.8)
                    except Exception:
                        pass
                    try:
                        if self.ml:
                            self.ml.ingest_sandbox_outcome(kind="background",
                                                           success=True, inp=x, out=y,
                                                           chain=chain, score=0.8, task_id=f"bg:{glyph}")
                    except Exception:
                        pass
                err_streak = 0
            except Exception:
                err_streak = min(10, err_streak + 1)
                continue

    # -------------------------
    # Periodic solver diagnostics export (compact)
    # -------------------------
    def _maybe_emit_diag(self):
        if not self.toggles.DIAG_EXPORTS:
            return
        try:
            diag = {
                "t": time.time(),
                "conf_ema": float(self._conf_ema),
                "recent_conf_mean": float(np.mean(self._recent_conf)) if (np is not None and self._recent_conf) else None,
                "retry_base": int(self.retry_base),
                "h2h": dict(self._h2h_stats),
                "sbx_trip_until": float(self._sbx_err_budget.get("tripped_until", 0.0)),
                "toggles": asdict(self.toggles),
            }
            out_dir = os.path.join(self.export.export_root, "diagnostics")
            os.makedirs(out_dir, exist_ok=True)
            path = os.path.join(out_dir, "solver_diag.jsonl")
            with open(path, "a", encoding="utf-8") as f:
                f.write(json.dumps(diag) + "\n")
            if self.export.keel_snapshots and self.toggles.CSV_EXPORTS:
                _safe_holo_snapshot("diag_append")
        except Exception:
            pass

    # -------------------------
    # holo add dedup + threshold tuning + cadence
    # -------------------------
    def _maybe_holo_add(self, inp: np.ndarray, out: np.ndarray, meta: Dict[str, Any], min_conf: float = 0.8):
        try:
            if self.holo is None or not hasattr(self.holo, "add"):
                return
            conf = float(meta.get("confidence", 0.0))
            if conf < float(min_conf):
                return
            # de-dup by pair hash
            try:
                h = hash((_sha1_grid(inp), _sha1_grid(out))) if "_sha1_grid" in globals() else hash((str(inp.shape), str(out.shape), str(inp.tobytes()[:32]), str(out.tobytes()[:32])))
            except Exception:
                h = hash((str(inp), str(out)))
            if h in self._recent_holo_adds:
                return
            self._recent_holo_adds.append(h)
            self.holo.add(inp, out, meta)
        except Exception:
            pass

    def _on_success_threshold_tune(self, shape: Tuple[int, ...], conf: float, thr: float):        
        try:
            shp = tuple(int(s) for s in (shape or ()))
            # guard policy object
            base_val = getattr(self._policy, "base", 0.72) if self._policy is not None else 0.72
            cur = float(getattr(getattr(self._policy, "by_shape", {}), "get", lambda *_: base_val)(shp))
            target = float(max(min(conf, 0.95), 0.55))
            new = 0.9*cur + 0.1*target
            if self._policy is not None and hasattr(self._policy, "by_shape"):
                self._policy.by_shape[shp] = float(max(0.10, min(0.95, new)))
        except Exception:
            pass

    def _maybe_housekeeping(self):        
        try:
            cnt = int(self._pred_count)
            if (cnt % 50) == 0 and "periodic_housekeeping_and_exports" in globals():
                periodic_housekeeping_and_exports(epoch_idx=cnt // 50)
        except Exception:
            pass

# ================================================================
# Explain + Physics Hooks (idempotent, sampling, safe adapters)
# ================================================================
# global sentinel to avoid double-wraps on reloads
try:
    _HOOKS_INSTALLED
except NameError:
    _HOOKS_INSTALLED = False

# simple evolve logging throttle
LOG_SAMPLE_RATE = 0.10   # 10%
MAX_EVOLVE_LOG = 500     # cap per call

def _apply_ops_adapter(grid, ops):
    """Try local apply_ops, else sandbox_apply_ops; be gentle with signature mismatches."""
    fn = globals().get("apply_ops") or globals().get("sandbox_apply_ops")
    if fn is None:
        return grid
    try:
        return fn(grid, ops)
    except TypeError:
        try:
            return fn(grid, [("identity", {})])  # last-resort no-op if supported
        except Exception:
            return grid

def install_explanation_hooks(encoder=None, meta=None, solver=None, sandbox=None):
    global _HOOKS_INSTALLED
    if _HOOKS_INSTALLED:
        return
    # Encoder hooks
    try:
        if encoder and hasattr(encoder, "classify") and not hasattr(encoder, "_orig_classify_explain"):
            encoder._orig_classify_explain = encoder.classify
            def _wrap_classify(row_dict):
                try:
                    label = encoder._orig_classify_explain(row_dict)
                    EXPLAIN.log("encoder.classify", {"winner": label})  # noqa: F821
                    _meta_log("encoder.classify", winner=label)
                    if meta and hasattr(meta, "ml"):
                        meta.ml.record_feedback(label="encoder_classify", success=True)
                    return label
                except Exception as e:
                    try: EXPLAIN.log("encoder.classify_error", {"error": str(e)})  # noqa: F821
                    except Exception: pass
                    return "Unclassified"
            encoder.classify = _wrap_classify

        if encoder and hasattr(encoder, "evolve") and not hasattr(encoder, "_orig_evolve_explain"):
            encoder._orig_evolve_explain = encoder.evolve
            def _wrap_evolve(df_like, true_labels):
                preds = encoder._orig_evolve_explain(df_like, true_labels)
                cnt = 0
                for p, t in zip(preds, true_labels):
                    if random.random() <= LOG_SAMPLE_RATE:
                        try: EXPLAIN.log("encoder.evolve", {"true": t, "pred": p, "ok": p == t})  # noqa: F821
                        except Exception: pass
                        _meta_log("encoder.evolve", true=t, pred=p, ok=p == t)
                        cnt += 1
                        if cnt >= MAX_EVOLVE_LOG:
                            break
                return preds
            encoder.evolve = _wrap_evolve
    except Exception:
        pass

    # Meta hook
    try:
        if meta and hasattr(meta, "observe") and not hasattr(meta, "_orig_observe_explain"):
            meta._orig_observe_explain = meta.observe
            def _wrap_observe(*a, **k):
                out = meta._orig_observe_explain(*a, **k)
                try: EXPLAIN.log("meta.observe", {"event": "boost"})  # noqa: F821
                except Exception: pass
                _meta_log("meta.observe", event="boost")
                return out
            meta.observe = _wrap_observe
    except Exception:
        pass

    # Solver learn_from_misses
    try:
        if solver and hasattr(solver, "learn_from_misses") and not hasattr(solver, "_orig_lfm_explain"):
            solver._orig_lfm_explain = solver.learn_from_misses
            def _wrap_lfm(misses):
                try: EXPLAIN.log("solver.replay", {"count": len(misses)})  # noqa: F821
                except Exception: pass
                _meta_log("solver.replay", count=len(misses))
                return solver._orig_lfm_explain(misses)
            solver.learn_from_misses = _wrap_lfm
    except Exception:
        pass

    # Sandbox physics/invariants
    try:
        if sandbox:
            if not hasattr(sandbox, "invariantscorer"):
                sandbox.invariantscorer = InvariantScorer(logger=EXPLAIN)  # noqa: F821
            if not hasattr(sandbox, "physics_heuristics"):
                sandbox.physics_heuristics = PhysicsHeuristics(logger=EXPLAIN)  # noqa: F821
            scorer, phys = sandbox.invariantscorer, sandbox.physics_heuristics
            if hasattr(sandbox, "discover_chain") and not hasattr(sandbox, "_orig_discover_chain_explain"):
                sandbox._orig_discover_chain_explain = sandbox.discover_chain
                def _wrap_discover_chain(inp, gold, *a, **kw):
                    chain = sandbox._orig_discover_chain_explain(inp, gold, *a, **kw)
                    try:
                        y_hat = _apply_ops_adapter(np.array(inp, dtype=int), chain)
                        scorer.score_pair(inp, y_hat, y_true=np.array(gold, dtype=int))
                        phys.score_pair(inp, y_hat)
                    except Exception as e:
                        try: EXPLAIN.log("sandbox.chain_error", {"error": str(e)})  # noqa: F821
                        except Exception: pass
                        _meta_log("sandbox.chain_error", error=str(e))
                    return chain
                sandbox.discover_chain = _wrap_discover_chain
    except Exception:
        pass

    _HOOKS_INSTALLED = True

# ================================================================
# Visual Exports (grayscale kept + color palette + diff panel)
# ================================================================
def _normalize_palette(grid: np.ndarray) -> np.ndarray:
    g = _ensure_int_ndarray(grid).astype(float)
    if g.size == 0:
        return np.zeros((1, 1), dtype=np.uint8)
    g -= g.min()
    m = g.max()
    if m > 0:
        g *= (255.0 / m)
    return g.astype(np.uint8)

def _label_bar(text: str, width: int, height: int = 28):
    if Image is None:
        _meta_log("visual.labelbar_failed", reason="PIL not available")
        return None
    bar = Image.new("RGB", (width, height), (15, 15, 15))
    try:
        from PIL import ImageFont  # type: ignore
        font = ImageFont.load_default()
    except Exception:
        font = None
    d = ImageDraw.Draw(bar)
    d.text((6, 6), text, fill=(230, 230, 230), font=font)
    return bar

# fixed 10-color palette (ARC-like)
_ARC_PALETTE = [
    (0,0,0),        # 0
    (255,0,0),      # 1
    (0,255,0),      # 2
    (0,0,255),      # 3
    (255,255,0),    # 4
    (255,0,255),    # 5
    (0,255,255),    # 6
    (255,128,0),    # 7
    (128,0,255),    # 8
    (128,128,128),  # 9
]

def grid_to_rgb(grid: np.ndarray):
    if Image is None:
        _meta_log("visual.rgb_failed", reason="PIL not available")
        return None
    g = _ensure_int_ndarray(grid)
    h, w = g.shape
    img = Image.new("RGB", (w, h))
    px = img.load()
    for i in range(h):
        for j in range(w):
            v = int(g[i, j]) % len(_ARC_PALETTE)
            px[j, i] = _ARC_PALETTE[v]
    return img

def save_grid_png(grid: np.ndarray, path: str, scale: int = 20):    
    if Image is None:
        _meta_log("visual.grid_png_failed", path=path, reason="PIL not available")
        return
    arr = _normalize_palette(_ensure_int_ndarray(grid))
    img = Image.fromarray(arr, mode="L")
    if scale != 1:
        img = img.resize((img.width * scale, img.height * scale), resample=Image.NEAREST)
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    img.save(path)
    _meta_log("visual.grid_png", path=path, scale=scale, shape=arr.shape)

def save_grid_png_color(grid: np.ndarray, path: str, scale: int = 20, title: str = ""):
   
    if Image is None:
        _meta_log("visual.grid_png_color_failed", path=path, reason="PIL not available")
        return
    im = grid_to_rgb(grid)
    if im is None:
        return
    if scale != 1:
        im = im.resize((im.width * scale, im.height * scale), resample=Image.NEAREST)
    if title:
        bar = _label_bar(title, im.width)
        if bar:
            canvas = Image.new("RGB", (im.width, im.height + bar.height), (0, 0, 0))
            canvas.paste(bar, (0, 0)); canvas.paste(im, (0, bar.height))
            im = canvas
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    im.save(path)
    _meta_log("visual.grid_png_color", path=path, scale=scale, shape=_ensure_int_ndarray(grid).shape)

def save_card_triptych(inp: np.ndarray, pred: np.ndarray, truth: "Optional[np.ndarray]",
                       out_path: str, title: str = "", show_diff: bool = True):
   
    if Image is None:
        _meta_log("visual.triptych_failed", path=out_path, reason="PIL not available")
        return
    scale = 16

    def _panel(label, grid):
        im = grid_to_rgb(grid) or Image.new("RGB", (1,1))
        im = im.resize((im.width * scale, im.height * scale), resample=Image.NEAREST)
        bar = _label_bar(label, im.width)
        if bar:
            canvas = Image.new("RGB", (im.width, im.height + bar.height), (0, 0, 0))
            canvas.paste(bar, (0, 0)); canvas.paste(im, (0, bar.height))
            return canvas
        return im

    blocks = [("INPUT", _ensure_int_ndarray(inp)), ("PRED", _ensure_int_ndarray(pred))]
    if truth is not None:
        t = _ensure_int_ndarray(truth)
        blocks.append(("TRUTH", t))
        if show_diff:
            # DIFF: highlight mismatches; keep pred color where mismatch
            p = _ensure_int_ndarray(pred)
            mask = (p != t).astype(int)
            # build a colored diff: mismatches keep pred color; matches set to dark
            diff = np.where(mask == 1, p, 0)
            blocks.append(("DIFF", diff))

    imgs = [_panel(lbl, g) for lbl, g in blocks]
    total_w = sum(im.width for im in imgs)
    max_h = max(im.height for im in imgs) if imgs else 0
    canvas = Image.new("RGB", (total_w, max_h + 30), (0, 0, 0))
    x = 0
    for im in imgs:
        canvas.paste(im, (x, 30)); x += im.width
    if title:
        bar = _label_bar(title, canvas.width, 30)
        if bar:
            canvas.paste(bar, (0, 0))

    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    canvas.save(out_path)
    _meta_log("visual.triptych", path=out_path, title=title, blocks=len(blocks))


# -----------------------------
# Kaggle Main
# -----------------------------
# =============================================================
# 1) KEEL-only compression gateway
# =============================================================
class KEELOnly:   
    def __init__(self):
        # Support either byte-level API or (blob, meta) API
        self.available = ("keel_compress_bytes" in globals()) or ("keel_encode" in globals())

    def compress_bytes(self, raw: bytes) -> Optional[bytes]:
        try:
            if "keel_compress_bytes" in globals():
                b1, _ = keel_compress_bytes(raw, q_ll=3.0, deblock=True)  # type: ignore
                return b1
            if "keel_encode" in globals():
                # encode likely expects structured arrays, not arbitrary bytes; treat as unavailable for raw
                _meta("keel.partial_api", mode="encode_only")
                return None
            _meta("keel.unavailable")
            return None
        except Exception as e:
            _log_exc(f"[KEELOnly.compress_bytes] {e}")
            return None

    def compress_path(self, path: str) -> Optional[str]:
        try:
            if not os.path.isfile(path):
                return None
            with open(path, "rb") as f:
                raw = f.read()
            out = self.compress_bytes(raw)
            if out is None:
                _meta("keel.unavailable.path", path=path)
                return None
            out_path = path + ".keel"
            tmp = out_path + ".tmp"
            with open(tmp, "wb") as fo:
                fo.write(out)
            os.replace(tmp, out_path)
            try:
                src_sz = os.stat(path).st_size
                dst_sz = os.stat(out_path).st_size
                _meta("keel.path", src=path, dst=out_path, ratio=float(src_sz / max(1, dst_sz)))
            except Exception:
                _meta("keel.path", src=path, dst=out_path)
            return out_path
        except Exception as e:
            _log_exc(f"[KEELOnly.compress_path] {e}")
            return None

KEEL = KEELOnly()

def compress_file_keel_only(path: str) -> Optional[str]:
    return KEEL.compress_path(path)

# =============================================================
# 2) Data product manifest (atomic)
# =============================================================
@dataclass
class ManifestItem:
    path: str
    bytes: int
    keel_ratio: Optional[float] = None
    sha1: Optional[str] = None
    ts: float = field(default_factory=time.time)

class DataProductManifest:
    def __init__(self, out_path: str = "exports/manifest.json"):
        self.out_path = out_path
        os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
        self._lock = threading.Lock()

    def _sha1(self, path: str) -> Optional[str]:
        try:
            with open(path, "rb") as f:
                h = hashlib.sha1()
                for chunk in iter(lambda: f.read(1<<20), b""):
                    h.update(chunk)
            return h.hexdigest()
        except Exception:
            return None

    def add(self, path: str, keel_sidecar: Optional[str] = None):
        try:
            st = os.stat(path)
            ratio = None
            if keel_sidecar and os.path.isfile(keel_sidecar):
                try:
                    src_sz = os.stat(path).st_size
                    dst_sz = os.stat(keel_sidecar).st_size
                    ratio = float(src_sz / max(1, dst_sz))
                except Exception:
                    ratio = None
            rec = ManifestItem(path=path, bytes=st.st_size, keel_ratio=ratio, sha1=self._sha1(path))
            tmp = self.out_path + ".tmp"
            with self._lock:
                data = []
                if os.path.isfile(self.out_path):
                    try:
                        with open(self.out_path, "r", encoding="utf-8") as f:
                            data = json.load(f)
                    except Exception:
                        data = []
                data.append(rec.__dict__)
                with open(tmp, "w", encoding="utf-8") as f:
                    json.dump(data, f, indent=2)
                os.replace(tmp, self.out_path)
            _meta("manifest.add", path=path, keel=keel_sidecar, bytes=st.st_size, ratio=ratio)
        except Exception as e:
            _log_exc(f"[Manifest.add] {e}")

MANIFEST = DataProductManifest()

# =============================================================
# 3) Backpressure-aware export scheduler
# =============================================================
class ExportScheduler:
    def __init__(self, max_queue: int = 1024):
        self.q: "queue.Queue[Tuple[Callable, tuple, dict]]" = queue.Queue(maxsize=max_queue)
        self._stop = False
        self._fuse_until = 0.0
        self._fused = False
        self._th = threading.Thread(target=self._loop, daemon=True)
        self._th.start()

    def submit(self, fn: Callable, *a, **k) -> bool:
        now = time.time()
        if now < self._fuse_until:
            self._fused = True
            _meta("export.fused", until=self._fuse_until)
            return False
        try:
            self.q.put_nowait((fn, a, k))
            return True
        except queue.Full:
            self._fuse_until = now + 1.0
            self._fused = True
            _meta("export.backpressure", fuse_until=self._fuse_until)
            return False

    def stop(self, join: bool = True):
        self._stop = True
        if join:
            try: self._th.join(timeout=2.0)
            except Exception: pass

    def _loop(self):
        err = 0
        while not self._stop:
            try:
                fn, a, k = self.q.get(timeout=0.1)
            except queue.Empty:
                continue
            try:
                fn(*a, **k)
                if self._fused and time.time() >= self._fuse_until:
                    _meta("export.fuse_close")
                    self._fused = False
                err = 0
            except Exception as e:
                err += 1
                _log_exc(f"[ExportScheduler] {e}")
                if err >= 3:
                    self._fuse_until = time.time() + min(3.0, 0.5*err)
                    self._fused = True
                    _meta("export.fuse_trip", fuse_until=self._fuse_until, err_streak=err)

EXPORTS = ExportScheduler()

# =============================================================
# 4) Memory/shape skew watchdog
# =============================================================
class ShapeSkewWatchdog:
    def __init__(self, warn_ratio: float = 0.6):
        self.warn_ratio = float(warn_ratio)

    def check(self, shape_hist: Dict[Tuple[int, ...], int]):
        try:
            if not shape_hist:
                return
            total = sum(shape_hist.values())
            (shape, cnt) = max(shape_hist.items(), key=lambda kv: kv[1])
            ratio = float(cnt)/max(1, total)
            if ratio >= self.warn_ratio:
                _meta("watch.skew", shape=str(shape), ratio=ratio, total=total)
        except Exception:
            pass

SHAPE_WATCH = ShapeSkewWatchdog()

# =============================================================
# 5) Phase-timing SLA & alerts
# =============================================================
@dataclass
class SLA:
    phase: str
    max_sec: float

class PhaseSLA:
    def __init__(self, slas: List[SLA]):
        self.slas = {s.phase: s.max_sec for s in slas}
        self._accum = {}  # phase -> elapsed_sec

    def record(self, phase: str, elapsed_sec: float):
        try:
            self._accum[phase] = self._accum.get(phase, 0.0) + float(elapsed_sec)
            lim = self.slas.get(phase)
            if lim is None:
                return
            val = self._accum[phase]
            if val > 0.9 * lim:
                _meta("sla.near_breach", phase=phase, elapsed=val, limit=lim)
            if val > lim:
                _meta("sla.breach", phase=phase, elapsed=val, limit=lim)
        except Exception:
            pass

PHASE_SLA = PhaseSLA([
    SLA("decode", 0.15), SLA("encode", 0.15), SLA("sandbox", 2.0), SLA("training", 5.0)
])

# =============================================================
# 6) Adaptive heartbeat cadence v2 (multi-signal)
# =============================================================

def pulse_sleep_adv(flux: float, keel_ratio: float, rolling_acc: Optional[float],
                    idle_default: float = 60.0, min_secs: float = 0.2, max_secs: float = 90.0) -> float:
    try:
        flux = max(0.0, min(100.0, float(flux or 0.0)))
        keel_ratio = float(keel_ratio or 1.0)
        acc = 0.5 if rolling_acc is None else float(rolling_acc)
        acc_adj = 1.15 if acc < 0.5 else (0.9 if acc > 0.8 else 1.0)
        base = idle_default * (1.0 - 0.006 * flux) * (0.95 + 0.05 * max(0.0, keel_ratio - 1.0)) * acc_adj
        dt = max(min_secs, min(max_secs, base))
        return float(dt)
    except Exception:
        return idle_default

# =============================================================
# 7) Miss/repair ledger (event-sourced JSONL)
# =============================================================
class Ledger:
    def __init__(self, path: str = "exports/ledger/miss_repair.jsonl"):
        self.path = path
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        self._lock = threading.Lock()

    def append(self, kind: str, task_id: str, ops_len: int, conf_before: float, conf_after: float,
               keel_ratio: Optional[float] = None, extra: Optional[Dict[str, Any]] = None):
        rec = {
            "ts": time.time(), "kind": kind, "task_id": task_id, "ops_len": int(ops_len),
            "conf_before": float(conf_before), "conf_after": float(conf_after),
            "keel_ratio": keel_ratio, **(extra or {})
        }
        line = json.dumps(rec) + "\n"
        try:
            with self._lock:
                with open(self.path, "a", encoding="utf-8") as f:
                    f.write(line)
            _meta("ledger.append", **{k:v for k,v in rec.items() if k!="extra"})
        except Exception as e:
            _log_exc(f"[Ledger.append] {e}")

LEDGER = Ledger()

# =============================================================
# 8) Circuit breaker FSM (sandbox safety)
# =============================================================
class CircuitBreaker:
    """OPEN -> (probation) HALF_OPEN -> CLOSED cycle with cooldowns."""
    CLOSED = "closed"
    OPEN = "open"
    HALF = "half_open"

    def __init__(self, error_budget: int = 8, open_sec: float = 5.0, probation_attempts: int = 3):
        self.error_budget = int(error_budget)
        self.open_sec = float(open_sec)
        self.probation_attempts = int(probation_attempts)
        self.state = self.CLOSED
        self._errors: List[float] = []
        self._opened_at = 0.0
        self._probes_left = 0

    def allow(self) -> bool:
        now = time.time()
        if self.state == self.OPEN:
            if now - self._opened_at >= self.open_sec:
                self.state = self.HALF
                self._probes_left = self.probation_attempts
                _meta("cb.half_open")
            else:
                return False
        if self.state == self.HALF:
            if self._probes_left <= 0:
                return False
            self._probes_left -= 1
            return True
        return True

    def record_error(self):
        now = time.time()
        self._errors.append(now)
        self._errors = [t for t in self._errors if now - t <= 60.0]
        if len(self._errors) >= self.error_budget:
            self.state = self.OPEN
            self._opened_at = now
            _meta("cb.trip", count=len(self._errors))

    def record_success(self):
        if self.state == self.HALF:
            self.state = self.CLOSED
            self._errors.clear()
            _meta("cb.close")

CB_SANDBOX = CircuitBreaker()

# =============================================================
# 9) Keystone cache for KB recall
# =============================================================
class KeystoneCache:
    def __init__(self, max_items: int = 2048):
        self.max_items = int(max_items)
        self._data: Dict[Tuple, Tuple[Any, float]] = {}

    def _prune(self):
        if len(self._data) <= self.max_items:
            return
        items = sorted(self._data.items(), key=lambda kv: kv[1][1])
        for k,_ in items[: len(self._data) - self.max_items]:
            self._data.pop(k, None)

    def get_or_put(self, key: Tuple, fn: Callable[[], Any]):
        now = time.time()
        if key in self._data:
            val, _ts = self._data[key]
            self._data[key] = (val, now)
            return val
        val = fn()
        self._data[key] = (val, now)
        self._prune()
        return val

KEYSTONE = KeystoneCache()

# =============================================================
# 10) Forensics pack exporter (KEEL-compressed)
# =============================================================
class ForensicsPack:
    def __init__(self, out_dir: str = "exports/forensics"):
        self.out_dir = out_dir
        os.makedirs(out_dir, exist_ok=True)

    def _zip_dir_to_bytes(self, dir_path: str) -> bytes:
        buf = io.BytesIO()
        with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as z:
            for root, _, files in os.walk(dir_path):
                for fn in files:
                    fp = os.path.join(root, fn)
                    arc = os.path.relpath(fp, start=dir_path)
                    z.write(fp, arc)
        return buf.getvalue()

    def export(self, case_id: str, files: List[str], meta: Optional[Dict[str, Any]] = None) -> Optional[str]:
        try:
            case_dir = os.path.join(self.out_dir, case_id)
            os.makedirs(case_dir, exist_ok=True)
            copied = []
            for src in files:
                if not src or not os.path.isfile(src):
                    continue
                dst = os.path.join(case_dir, os.path.basename(src))
                with open(src, "rb") as fi, open(dst, "wb") as fo:
                    fo.write(fi.read())
                copied.append(dst)
            side = {"case_id": case_id, "host": socket.gethostname(), "ts": _now_iso(), **(meta or {})}
            with open(os.path.join(case_dir, "meta.json"), "w", encoding="utf-8") as f:
                json.dump(side, f, indent=2)
            raw_zip = self._zip_dir_to_bytes(case_dir)
            b1 = KEEL.compress_bytes(raw_zip)
            if b1:
                outp = os.path.join(self.out_dir, f"{case_id}.forensics.keel")
                tmp = outp + ".tmp"
                with open(tmp, "wb") as f:
                    f.write(b1)
                os.replace(tmp, outp)
                _meta("forensics.pack", case=case_id, files=len(copied), out=outp)
                return outp
            # clean fallback if KEEL unavailable
            outp_zip = os.path.join(self.out_dir, f"{case_id}.forensics.zip")
            with open(outp_zip, "wb") as f:
                f.write(raw_zip)
            _meta("forensics.no_keel", case=case_id, out=outp_zip)
            return outp_zip
        except Exception as e:
            _log_exc(f"[ForensicsPack.export] {e}")
            return None

FORENSICS = ForensicsPack()

# =============================================================
# Integration helpers (call from your existing modules)
# =============================================================

def keel_export_csv(csv_path: str) -> Optional[str]:
    """Compress a CSV with KEEL and append to manifest."""
    out = compress_file_keel_only(csv_path)
    if out:
        MANIFEST.add(csv_path, keel_sidecar=out)
    return out


def schedule_keel_export(path: str):
    EXPORTS.submit(keel_export_csv, path)


def kb_recall_cached(glyph, shape, x_hash: int, compute_fn: Callable[[], Any]):
    key = ("kb", glyph, tuple(shape) if isinstance(shape, (list, tuple)) else shape, int(x_hash))
    return KEYSTONE.get_or_put(key, compute_fn)


def cb_sandbox_allow() -> bool:
    return CB_SANDBOX.allow()


def cb_sandbox_error():
    CB_SANDBOX.record_error()


def cb_sandbox_success():
    CB_SANDBOX.record_success()


def ledger_miss_repair(kind: str, task_id: str, ops_len: int, conf_before: float, conf_after: float, extra: Optional[Dict[str, Any]] = None):
    LEDGER.append(kind, task_id, ops_len, conf_before, conf_after, extra=extra)


def skew_watch(shape_hist: Dict[Tuple[int, ...], int]):
    SHAPE_WATCH.check(shape_hist)


def heartbeat_sleep(flux: float, keel_ratio: float, rolling_acc: Optional[float]) -> float:
    return pulse_sleep_adv(flux, keel_ratio, rolling_acc)

# ---------- Compatibility shim between orchestrator and open-ended loader ----------

# 1) Normalize root const names
try:
    # If the loader used ARCDb_ROOT_DEFAULT, map it to ARCDB_ROOT_DEFAULT
    if "ARCDb_ROOT_DEFAULT" in globals() and "ARCDB_ROOT_DEFAULT" not in globals():
        ARCDB_ROOT_DEFAULT = globals()["ARCDb_ROOT_DEFAULT"]  # noqa: F821
except Exception:
    pass

def build_splits():    
    # Pick roots from globals published by kaggle_main, else use defaults
    r2025 = globals().get("DATA_ROOT_2025", globals().get("ARC2025_ROOT_DEFAULT", "/kaggle/input/arc-prize-2025"))
    rdb   = globals().get("DATA_ROOT_ARCDB", globals().get("ARCDB_ROOT_DEFAULT", "/kaggle/input/arc-database/arc_data"))

    # Call canonical open-ended builder
    splits_open = build_splits_open(root_2025=r2025, root_arcdb=rdb)  # noqa: F821

    # Remap to orchestrator’s expected keys
    out = {
        # training (take first two sessions by convention)
        "train2025_pre1": getattr(splits_open, "train_2025_s1", []) or [],
        "train2025_pre2": getattr(splits_open, "train_2025_s2", []) or [],

        # mocks (use ARC-DB eval halves as mock)
        "mock2025_pre1": getattr(splits_open, "test_arcdb_eval_pre1", []) or [],
        "mock2025_pre2": getattr(splits_open, "test_arcdb_eval_pre2", []) or [],

        # eval lookup the orchestrator probes for
        "eval_lookup": getattr(splits_open, "eval_2025_lookup", {}) or {},

        # submission set
        "submission_test_all": getattr(splits_open, "submission_test_2025_all", []) or [],
    }
    # Optionally keep original for other modules
    out["_OPEN_SPLITS"] = splits_open
    return out

def describe_splits(splits):   
    s_open = None
    if isinstance(splits, dict):
        s_open = splits.get("_OPEN_SPLITS")
    if s_open is not None:
        try:
            describe_splits_open(s_open)  # noqa: F821
            return
        except Exception:
            pass

    # Minimal fallback print using orchestrator keys
    def n(x): return len(x) if isinstance(x, list) else (0 if x is None else -1)
    print("[SPLITS: legacy view]")
    print(f"train2025_pre1={n(splits.get('train2025_pre1'))} | train2025_pre2={n(splits.get('train2025_pre2'))}")
    print(f"mock2025_pre1={n(splits.get('mock2025_pre1'))} | mock2025_pre2={n(splits.get('mock2025_pre2'))}")
    print(f"submission_test_all={n(splits.get('submission_test_all'))}")
    print(f"eval_lookup={'yes' if splits.get('eval_lookup') else 'no'}")


# ==========================================================
# Unified kaggle_main (aligned to DATA LOADER STACK)
# ==========================================================
try:
    OrchestratorToggles  # type: ignore
except NameError:
    @dataclass
    class OrchestratorToggles:
        LOOPS_BEFORE_MOCK: int = 2
        EVAL_RETEST_PASSES: int = 3
        ENABLE_WARMUP: bool = True
        ENABLE_REINFORCE: bool = True
        AUDIT_ENABLED: bool = True
        TUNER_TARGET_CONF: float = 0.72
        ENABLE_COMPRESSION_SWEEP: bool = True
        ENABLE_GALLERY_INDEX: bool = True
        ENABLE_CONFIDENCE_GOVERNOR: bool = True
        ENABLE_CRASH_BUNDLE: bool = True

# ---------- manifest & gallery ----------
class ArtifactManifest:
    def __init__(self, root: str = "exports/meta/artifact_manifest.json"):
        self.path = root
        self.items: List[Dict[str, Any]] = []
        self._seen = set()

    def add(self, file_path: str, *, phase: str = "", pass_ix: Optional[int] = None, exclude_from_compression: bool = False):
        if not file_path or not os.path.exists(file_path):
            return
        key = (file_path, pass_ix, phase)
        if key in self._seen:
            return
        self._seen.add(key)
        st = os.stat(file_path)
        self.items.append({
            "path": file_path,
            "size": st.st_size,
            "sha256": _sha256(file_path),
            "phase": phase,
            "pass_ix": pass_ix,
            "exclude_from_compression": bool(exclude_from_compression),
            "ts": _now_ts(),
        })

    def mark_compressed(self, file_path: str, ratio: Optional[float]):
        for it in self.items:
            if it["path"] == file_path:
                it["compressed_ratio"] = ratio
                it["compressed_ts"] = _now_ts()

    def save(self):
        _write_json(self.path, {"items": self.items, "ts": _now_ts()})

class GalleryIndex:
    def __init__(self, out_path: str = "exports/vis/index.html"):
        self.out_path = out_path
        self.rows: List[Tuple[str, str]] = []

    def add(self, label: str, file_path: str):
        if os.path.exists(file_path):
            self.rows.append((label, file_path))

    def save(self):
        _ensure_dir(os.path.dirname(self.out_path))
        lines = [
            "<!doctype html><meta charset='utf-8'><title>ARC Visual Gallery</title>",
            "<style>body{font-family:system-ui,Segoe UI,Arial;margin:24px} .card{margin:12px 0;padding:12px;border:1px solid #eee;border-radius:12px;box-shadow:0 1px 4px rgba(0,0,0,0.06)}</style>",
            "<h1>ARC Visual Gallery</h1>",
        ]
        for label, path in self.rows:
            rel = path
            lines.append(f"<div class='card'><h3>{label}</h3><img src='{rel}' style='max-width:100%;height:auto'/><p><a href='{rel}' download>download</a></p></div>")
        with open(self.out_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

# ---------- compression policy ----------

def compression_posthook(summary: Dict[str, Any], dirs: List[str], extra_exclude: List[str]) -> Dict[str, Any]:    
    exclude_patterns = [r"(^|/|\\)submission\.", r"\.keel$"] + list(extra_exclude or [])
    def _excluded(p: str) -> bool:
        return any(re.search(rx, p) for rx in exclude_patterns)

    compressed, total_ratio = 0, 0.0
    for d in dirs or []:
        if not d or not os.path.isdir(d):
            continue
        for root, _, files in os.walk(d):
            for fn in files:
                fp = os.path.join(root, fn)
                if _excluded(fp):
                    continue
                out = compress_file_keel_only(fp)
                if out and os.path.exists(out):
                    compressed += 1
                    try:
                        r = os.stat(fp).st_size / max(1, os.stat(out).st_size)
                        total_ratio += r
                        MANIFEST.add(fp, keel_sidecar=out)
                    except Exception:
                        pass
                else:
                    _meta("keel.skip_or_unavailable", path=fp)
    summary.setdefault("compression_stats", {})
    if compressed:
        summary["compression_stats"]["files_compressed"] = compressed
        summary["compression_stats"]["avg_ratio"] = total_ratio / compressed if compressed else 1.0
    return summary

# ---------- submission proof ----------

def _write_submission_proof(submission_path: str, out_path: str = "exports/meta/submission_proof.json"):
    proof = {
        "path": submission_path,
        "exists": os.path.exists(submission_path),
        "sha256": _sha256(submission_path) if os.path.exists(submission_path) else None,
        "size": (os.stat(submission_path).st_size if os.path.exists(submission_path) else None),
        "ts": _now_ts(),
        "exclusions_confirmed": True
    }
    _write_json(out_path, proof)
    return proof

# ---------- crash bundle (KEEL preferred; .zip fallback) ----------

def _emit_crash_bundle(e: BaseException):
    try:
        _ensure_dir("exports/crash")
        # Collect a minimal set of artifacts
        staging = "exports/crash/_staging"
        _ensure_dir(staging)
        try:
            for p in [
                "exports/meta/explanations.jsonl",
                "exports/meta/run_summary.json",
                "exports/meta/confidence_history.json",
                "exports/meta/governor_actions.json",
            ]:
                if os.path.exists(p):
                    dst = os.path.join(staging, os.path.basename(p))
                    with open(p, "rb") as fi, open(dst, "wb") as fo:
                        fo.write(fi.read())
        except Exception:
            pass
        tb_txt = f"Traceback:\n{traceback.format_exc()}\nError: {repr(e)}\n"
        with open(os.path.join(staging, "traceback.txt"), "w", encoding="utf-8") as f:
            f.write(tb_txt)

        # Zip to bytes
        buf = io.BytesIO()
        with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as z:
            for root, _, files in os.walk(staging):
                for fn in files:
                    fp = os.path.join(root, fn)
                    arc = os.path.relpath(fp, start=staging)
                    z.write(fp, arc)
        raw_zip = buf.getvalue()

        # KEEL compress preferred
        b1 = KEEL.compress_bytes(raw_zip)
        if b1:
            outp = "exports/crash/crash_bundle.forensics.keel"
            tmp = outp + ".tmp"
            with open(tmp, "wb") as f:
                f.write(b1)
            os.replace(tmp, outp)
            _meta("crash.bundle", out=outp)
            return outp

        # Fallback clean .zip if KEEL unavailable
        outp_zip = "exports/crash/crash_bundle.zip"
        with open(outp_zip, "wb") as f:
            f.write(raw_zip)
        _meta("forensics.no_keel", out=outp_zip)
        return outp_zip
    except Exception:
        return None

# ---------- kaggle_main ----------

def kaggle_main(
    data_root_2025: Optional[str] = None,
    data_root_arcdb: Optional[str] = None,
    attempts: int = 2,
    emit_csv: bool = True,
    export_cards_flag: bool = True,
    expand_with_eval: bool = True,
    seed: int = 1337,
    loops_before_mock: Optional[int] = None,
    toggles: OrchestratorToggles = OrchestratorToggles(),
    phase_plan: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    
    t0 = time.time()
    _ensure_dir("exports/csv"); _ensure_dir("exports/json")
    _ensure_dir("exports/vis"); _ensure_dir("exports/holo")
    _ensure_dir("exports/meta"); _ensure_dir("logs")

    # Seed & run id
    _safe_call("set_global_seed", seed)
    run_id = _safe_call("make_run_id") or f"run_{int(_now_ts())}"
    phase  = (phase_plan or {}).get("phase", "train")

    
    # Build runtime (respects single Kairos + upstream components)
    runtime = _safe_call("build_runtime", run_id, phase)

    # Robust solver extraction + fallbacks 
    solver = None
    try:
        solver = runtime.get("solver") if isinstance(runtime, dict) else getattr(runtime, "solver", None)
    except Exception:
        solver = None

    if solver is None:
        ARC = globals().get("ARCSymbolicUltra")
        if callable(ARC):
            try:
                solver = ARC(run_id=run_id, phase=phase)
            except Exception:
                solver = None

    if solver is None and callable(globals().get("build_solver", None)):
        try: solver = build_solver()  # type: ignore
        except Exception: solver = None

    if solver is None and callable(globals().get("make_solver", None)):
        try: solver = make_solver()   # type: ignore
        except Exception: solver = None

    if solver is None:
        ULTRA = globals().get("ULTRA") or globals().get("ultra")
        if ULTRA is not None and hasattr(ULTRA, "predict"):
            solver = ULTRA

    # Back-propagate solver so downstream reads are safe
    if solver is not None:
        try:
            if isinstance(runtime, dict): runtime["solver"] = solver
            else: setattr(runtime, "solver", solver)
        except Exception: pass
        globals()["solver"] = solver
    else:
        msg = ("kaggle_main: solver is None. "
               "Define ARCSymbolicUltra or build_solver()/make_solver(), "
               "or ensure ULTRA has predict().")
        _meta("orchestrator.solver_missing", msg=msg, run_id=run_id, phase=phase)
        raise RuntimeError(msg)

    # Safe to attach trainer/threads now (sandbox concurrent threads via ensure_threads)
    _safe_call("attach_unified_trainer", solver)
    _safe_call("ensure_threads", solver)

    # Splits — prefer no-arg builder (reads global roots); fallback positional
    splits = None
    if callable(_g("build_splits")):
        try:
            splits = _safe_call("build_splits")
            _safe_call("describe_splits", splits)
        except Exception:
            try:
                splits = _g("build_splits")(DATA_ROOT_2025, DATA_ROOT_ARCDB)  # type: ignore
                _safe_call("describe_splits", splits)
            except Exception as e:
                _meta("splits.call_failed", err=str(e))
                splits = None
    if splits is None:
        _safe_call("ensure_arc_dataset_ready")
        _safe_call("_load_year_2025", DATA_ROOT_2025)
        _safe_call("_build_arc_chal_sol")
        splits = _g("SPLITS") or {}

    # Warm start
    if toggles.ENABLE_WARMUP:
        _safe_call("preload_rules_from_csv", solver)

    # Telemetry banner
    _meta("run_start", run_id=run_id, seed=seed, ts=_now_ts(), phase=phase)

    manifest = ArtifactManifest()
    gallery  = GalleryIndex()

    # Phase plan / loops
    loops = loops_before_mock if isinstance(loops_before_mock, int) else toggles.LOOPS_BEFORE_MOCK

    def _block(name: str):
        if isinstance(splits, dict): return splits.get(name) or []
        return getattr(splits, name, []) if splits is not None else []

    train_blocks = [
        ("train2025_pre1", _block("train2025_pre1")),
        ("train2025_pre2", _block("train2025_pre2")),
    ]
    mock_blocks = [
        ("mock2025_pre1", _block("mock2025_pre1")),
        ("mock2025_pre2", _block("mock2025_pre2")),
    ]
    eval_lookup = _block("eval_lookup") or _g("EVAL_LOOKUP")
    submission_set = _block("submission_test_all") or _g("SUBMISSION_TEST_ALL")

    # Confidence governor trace
    conf_hist: List[Dict[str, Any]] = []
    governor_actions: List[Dict[str, Any]] = []

    # Local helper: emit artifacts per pass
    def _emit_eval_artifacts(pass_ix: int, rows, report, phase_label: str):
        if emit_csv and rows is not None:
            csv_path = os.path.join("exports/csv", f"eval_pass{pass_ix}.csv")
            _safe_call("write_eval_rows_csv", rows, csv_path)
            manifest.add(csv_path, phase=phase_label, pass_ix=pass_ix)
            schedule_keel_export(csv_path)
        res = _safe_call("emit_resonance_map", pass_ix)
        if res and isinstance(res, dict):
            if "png" in res and os.path.exists(res["png"]):
                gallery.add(f"Resonance pass{pass_ix}", res["png"])
                manifest.add(res["png"], phase=phase_label, pass_ix=pass_ix)
            if "json" in res and os.path.exists(res["json"]):
                manifest.add(res["json"], phase=phase_label, pass_ix=pass_ix)
                schedule_keel_export(res["json"])  # compress JSON maps too
        snap = _safe_call("holo_snapshot", pass_ix)
        if isinstance(snap, str) and os.path.exists(snap):
            manifest.add(snap, phase=phase_label, pass_ix=pass_ix)
        pca = _safe_call("emit_holo_pca", pass_ix)
        if isinstance(pca, str) and os.path.exists(pca):
            gallery.add(f"Holo PCA pass{pass_ix}", pca)
            manifest.add(pca, phase=phase_label, pass_ix=pass_ix)

    # Retest-until-resolved with miss-replay
    def _retest_until_resolved(eval_tasks, eval_lookup_local, attempts_local: int, start_pass_ix: int = 1, phase_label: str = "eval"):
        pass_ix = start_pass_ix
        while pass_ix <= toggles.EVAL_RETEST_PASSES:
            out = _safe_call(
                "_evaluate_on_eval_set",
                eval_tasks,
                eval_lookup_local,
                attempts_local,
                csv_predictions=f"eval_pass{pass_ix}.csv",
                export_dir="exports",
                phase_tag=phase_label,
                pass_ix=pass_ix
            ) or ([], {}, 0, 0, [])
            rows, report, correct, total, misses = out
            conf = (report or {}).get("avg_confidence")
            conf_hist.append({"pass_ix": pass_ix, "avg_confidence": conf, "ts": _now_ts()})
            _emit_eval_artifacts(pass_ix, rows, report, phase_label)

            # Miss replay gate
            miss_path = os.path.join("exports/json", f"misses_pass{pass_ix}.json")
            if misses or os.path.exists(miss_path):
                replay_log = _safe_call("_loop_learn_misses_json", miss_path, solver=solver) or {}
                replay_out = os.path.join("exports/meta", f"miss_replay_pass{pass_ix}.json")
                _write_json(replay_out, replay_log)
                manifest.add(replay_out, phase=phase_label, pass_ix=pass_ix)
                schedule_keel_export(replay_out)

            if not misses:
                break

            # Confidence governor: raise attempts if low
            if toggles.ENABLE_CONFIDENCE_GOVERNOR and isinstance(conf, (float, int)) and conf < toggles.TUNER_TARGET_CONF:
                attempts_local += 1
                governor_actions.append({"pass_ix": pass_ix, "action": "raise_attempts", "new_attempts": attempts_local, "conf": conf, "ts": _now_ts()})

            pass_ix += 1

    # ---------- super-cycles ----------
    summary: Dict[str, Any] = {"run_id": run_id, "ts": _now_ts(), "strategy": "unified(kaggle_main)"}

    # Shape skew guard (if upstream exposes histogram via _safe_call)
    shape_hist = _safe_call("get_shape_histogram") or {}
    try: skew_watch(shape_hist)
    except Exception: pass

    for cycle in range(1, int(loops) + 1):
        _meta("orchestrator.cycle_begin", cycle=cycle, ts=_now_ts())

        for label, block in train_blocks:
            if not block:
                continue

            # Pre-blind eval
            _retest_until_resolved(block, eval_lookup, attempts, start_pass_ix=1, phase_label=f"{label}_preblind")

            # Train (replay + trainer)
            t_train0 = time.time()
            _safe_call("train_on_tasks", solver, block, attempts=attempts)
            PHASE_SLA.record("training", time.time() - t_train0)
            _safe_call("auto_retry_tuner_update", target_conf=toggles.TUNER_TARGET_CONF)

            # Post-blind eval
            _retest_until_resolved(block, eval_lookup, attempts, start_pass_ix=1, phase_label=f"{label}_postblind")

            # Rulebase hygiene & audit
            _safe_call("strip_rule_and_task_ids")
            if toggles.AUDIT_ENABLED:
                _safe_call("symbolic_audit_export", out_dir="exports/audit")

            # Optional compression sweep during run (sidecars only, via KEEL)
            if toggles.ENABLE_COMPRESSION_SWEEP:
                summary = compression_posthook(summary, dirs=["exports","logs"], extra_exclude=[r"/?private", r"\.ipynb_checkpoints"])

        _meta("orchestrator.cycle_end", cycle=cycle, ts=_now_ts())

        # Save gallery index every cycle
        if toggles.ENABLE_GALLERY_INDEX:
            gallery.save()
        manifest.save()

    # ---------- mock phases ----------
    for label, block in mock_blocks:
        if not block:
            continue
        _retest_until_resolved(block, eval_lookup, attempts, start_pass_ix=1, phase_label=f"{label}_mock")
        if toggles.ENABLE_GALLERY_INDEX:
            gallery.save()
        manifest.save()

    # ---------- submission ----------
    submission_path = _safe_call("emit_submission_json", submission_set) or "submission.json"
    if export_cards_flag:
        _safe_call("export_solution_cards", out_dir="exports/cards")
    summary["submission"] = {"path": submission_path, "exists": os.path.exists(submission_path)}
    proof = _write_submission_proof(submission_path)
    manifest.add(submission_path, phase="submission", pass_ix=None, exclude_from_compression=True)
    manifest.save()

    # ---------- end-of-run compression (never touch submission.*) ----------
    summary = compression_posthook(summary, dirs=["exports","logs","meta"], extra_exclude=[r"/?private", r"(^|/|\\)submission\\..*"])
    _write_json("exports/meta/run_summary.json", summary)

    # Emit confidence & governor logs
    _write_json("exports/meta/confidence_history.json", conf_hist)
    _write_json("exports/meta/governor_actions.json", governor_actions)

    # Final gallery index
    if toggles.ENABLE_GALLERY_INDEX:
        gallery.save()

    # Final explanations
    _meta("run_end", run_id=run_id, duration_s=round(time.time() - t0, 2), ts=_now_ts())

    # Graceful shutdown of export scheduler
    EXPORTS.stop(join=True)

    return summary


# ==========================================================
# Bottom driver (script/notebook-safe)
# ==========================================================
if __name__ == "__main__":
    DATA_ROOT_2025 = os.getenv("DATA_ROOT_2025") or None
    DATA_ROOT_ARCDB = os.getenv("DATA_ROOT_ARCDB") or None
    ATTEMPTS        = int(os.getenv("ATTEMPTS", "2"))
    SEED            = int(os.getenv("SEED", "1337"))
    ENABLE_WARMUP   = os.getenv("ENABLE_WARMUP", "1") != "0"
    ENABLE_REINFORCE= os.getenv("ENABLE_REINFORCE", "1") != "0"
    ENABLE_SUBMIT   = os.getenv("ENABLE_SUBMIT", "0") == "1"

    TUNER_TARGET_CONF = float(os.getenv("TUNER_TARGET_CONF", "0.72"))
    LOOPS_BEFORE_MOCK = int(os.getenv("LOOPS_BEFORE_MOCK", "2"))

    toggles = OrchestratorToggles(
        LOOPS_BEFORE_MOCK=LOOPS_BEFORE_MOCK,
        ENABLE_WARMUP=ENABLE_WARMUP,
        ENABLE_REINFORCE=ENABLE_REINFORCE,
        TUNER_TARGET_CONF=TUNER_TARGET_CONF,
    )

    t0 = time.time()
    try:
        summary = kaggle_main(
            data_root_2025=DATA_ROOT_2025,
            data_root_arcdb=DATA_ROOT_ARCDB,
            attempts=ATTEMPTS,
            emit_csv=True,
            export_cards_flag=True,
            expand_with_eval=True,
            seed=SEED,
            toggles=toggles,
        )

        print("\n✅ Orchestration complete")
        print("────────────────────────────────────────────────────────")
        print(f"  • Runtime: {round(time.time() - t0, 2)} s")
        comp = summary.get("compression_stats", {})
        print(f"  • Compressed exports: {comp.get('files_compressed', 0)} files")
        print(f"  • Avg ratio: {comp.get('avg_ratio', 1.0)}")
        print("────────────────────────────────────────────────────────\n")

    except Exception as e:
        print(f"\n❌  FATAL ERROR: {e}")
        traceback.print_exc()
        try:
            _emit_crash_bundle(e)
        finally:
            # Ensure export scheduler stops even on fatal error
            try: EXPORTS.stop(join=True)
            except Exception: pass
            if (os.getenv("RUN_MODE") in ("script","kaggle")):
                sys.exit(1)
            else:
                raise

# ------------------------------
# Public helper: attach to solver object (idempotent)
# ------------------------------

def attach_invariants_and_explanations(solver):
    try:
        if solver is None:
            return
        # Avoid double-install
        if getattr(solver, "_explain_hooks_installed", False):
            return
        enc = getattr(getattr(solver, "ml", None), "encoder", None)
        meta = getattr(solver, "meta", None)
        sandbox = getattr(solver, "sandbox", None)
        if 'install_explanation_hooks' in globals():
            install_explanation_hooks(encoder=enc, meta=meta, solver=solver, sandbox=sandbox)  # type: ignore[name-defined]
        if 'EXPLAIN' in globals():
            try:
                EXPLAIN.log("system.attach", {"encoder": bool(enc), "meta": bool(meta), "sandbox": bool(sandbox), "ok": True})  # type: ignore[name-defined]
            except Exception:
                pass
        # Broadcast attachment
        try:
            if getattr(solver, "ultra", None):
                solver.ultra.observe("system_attach", encoder=bool(enc), meta=bool(meta), sandbox=bool(sandbox))
            if getattr(solver, "kairos", None):
                solver.kairos.step(time_step=1)
            if getattr(solver, "holo", None):
                h = solver.holo
                if hasattr(h, "add") and np is not None:
                    z = np.zeros((1, 1), dtype=int)
                    h.add(z, z, {"subject": "system_attach", "encoder": bool(enc), "meta": bool(meta)})
        except Exception:
            pass
        setattr(solver, "_explain_hooks_installed", True)
    except Exception as e:
        try:
            if 'EXPLAIN' in globals():
                EXPLAIN.log("system.attach_error", {"error": str(e)})  # type: ignore[name-defined]
        except Exception:
            pass
